{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "06_02_qa_train_colab.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kurikinton105/GDL_code_colab/blob/main/06_02_qa_train_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OjIHlOABC386"
      },
      "source": [
        "**ドライブのマウントを行い、ディレクトリを移動する**\n",
        "\n",
        "データをdata/qa/に配置する"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y0b8BpM7BUKY",
        "outputId": "57e61feb-cd01-41bf-b16d-f8d746c58d69"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8aRF_WP7Bd9j",
        "outputId": "8c0fe660-4055-4948-c17d-110e25185c28"
      },
      "source": [
        "cd /content/drive/MyDrive/Colab Notebooks/GDL"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/GDL\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-SHgglaBJqk"
      },
      "source": [
        "# Generating question answer pairs from text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "W4dT8PFlBJqk"
      },
      "source": [
        "import importlib\n",
        "import os\n",
        "\n",
        "from tensorflow.keras.layers import Input, Embedding, GRU, Bidirectional, Dense, Lambda\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import keras.backend as K\n",
        "from tensorflow.keras.utils import plot_model\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "import pickle as pkl\n",
        "\n",
        "from utils.write import training_data, test_data, collapse_documents, expand_answers, _read_data, glove\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HIenAsqNBJqk"
      },
      "source": [
        "# run params\n",
        "SECTION = 'write'\n",
        "RUN_ID = '0001'\n",
        "DATA_NAME = 'qa'\n",
        "RUN_FOLDER = 'run/{}/'.format(SECTION)\n",
        "RUN_FOLDER += '_'.join([RUN_ID, DATA_NAME])\n",
        "\n",
        "if not os.path.exists(RUN_FOLDER):\n",
        "    os.mkdir(RUN_FOLDER)\n",
        "    os.mkdir(os.path.join(RUN_FOLDER, 'viz'))\n",
        "    os.mkdir(os.path.join(RUN_FOLDER, 'images'))\n",
        "    os.mkdir(os.path.join(RUN_FOLDER, 'weights'))\n",
        "\n",
        "mode =  'build' #'load' #"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78FTOaltBJqk"
      },
      "source": [
        "# data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8dilzNuBJqk"
      },
      "source": [
        "\n",
        "training_data_gen = training_data()\n",
        "# training_data_gen = [next(training_data_gen)]\n",
        "test_data_gen = test_data()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bp65nuhIBJql"
      },
      "source": [
        "t = next(training_data_gen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WcWUNE1ABJql",
        "outputId": "3e32dfd6-be99-42e9-8b44-6355bfc9c874"
      },
      "source": [
        "idx = 0\n",
        "\n",
        "print('document_tokens\\n', t['document_tokens'][idx])\n",
        "print('\\n')\n",
        "print('question_input_tokens\\n', t['question_input_tokens'][idx])\n",
        "print('\\n')\n",
        "print('answer_masks\\n', t['answer_masks'][idx])\n",
        "print('\\n')\n",
        "print('answer_labels\\n', t['answer_labels'][idx])\n",
        "print('\\n')\n",
        "print('question_output_tokens\\n', t['question_output_tokens'][idx])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "document_tokens\n",
            " [   1 4633    1   66  285    1    1 5115 1420   60    4 1136 1952    8\n",
            "  852  316  287   10    4  122   16  183    8   89 1090   60   72  558\n",
            "    8  361    8    1    4  283  122    6 6413 7338 4861   11 7656   16\n",
            "   27 1240   26  852  115    8 3356    5 2387    5   16  183    6   29\n",
            "   42  471   10  218  359    5   28   19 5115    5   39   17 3281    8\n",
            "    4 2694  935  852 7860    9 1136    1   26   72  163   10    1    5\n",
            " 6312    6   29   42  471   11 1112    1    5  100   42  767  241 5180\n",
            "    6   28   10  892    7    4 7391 2621    5    4   99   93  357    9\n",
            "    4 2709   13 1280  423    9 4488 1095   11 1952   98  130    1 1136\n",
            "    8  852    6    4  482   95 4051    1   30  410 3001 2426  458   10\n",
            "    4  123    6    4   86  359  732 1419  251   37    8    1  146 1136\n",
            "   47  382 2944    5   22  125 1175   55   37   10  994 4221    4 3773\n",
            "    6  676    8 3475    4 3773   24    1 1136   55   31 2948    8  161\n",
            "  295   29    1 1175    5   28   19 1332    1    1    5    4  428   12\n",
            " 1667    1    6   35]\n",
            "\n",
            "\n",
            "question_input_tokens\n",
            " [   2   39  614   30 3582  188    0    0    0    0    0    0    0    0\n",
            "    0    0    0]\n",
            "\n",
            "\n",
            "answer_masks\n",
            " [[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "\n",
            "\n",
            "answer_labels\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1\n",
            " 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "\n",
            "\n",
            "question_output_tokens\n",
            " [  39  614   30 3582  188    3    0    0    0    0    0    0    0    0\n",
            "    0    0    0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8p60_r-1BJql",
        "outputId": "7da8f670-a7d1-496d-ecad-1c3e94064d1c"
      },
      "source": [
        "# GloVe\n",
        "\n",
        "VOCAB_SIZE = glove.shape[0]\n",
        "EMBEDDING_DIMENS = glove.shape[1]\n",
        "\n",
        "print('GLOVE')\n",
        "print('VOCAB_SIZE: ', VOCAB_SIZE)\n",
        "print('EMBEDDING_DIMENS: ', EMBEDDING_DIMENS)\n",
        "\n",
        "GRU_UNITS = 100\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GLOVE\n",
            "VOCAB_SIZE:  9984\n",
            "EMBEDDING_DIMENS:  100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2w9rZPW3BJql"
      },
      "source": [
        "# parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SpIO9J0YBJql"
      },
      "source": [
        "MAX_DOC_SIZE = None\n",
        "MAX_ANSWER_SIZE = None\n",
        "MAX_Q_SIZE = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1gLW11ABJql"
      },
      "source": [
        "# architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 634
        },
        "id": "gb93qFWqBJql",
        "outputId": "50487029-db9f-4cab-911a-3a14ba7bb3c6"
      },
      "source": [
        "document_tokens = Input(shape=(MAX_DOC_SIZE,), name=\"document_tokens\")\n",
        "\n",
        "embedding = Embedding(input_dim = VOCAB_SIZE, output_dim = EMBEDDING_DIMENS, weights=[glove], mask_zero = True, name = 'embedding')\n",
        "document_emb = embedding(document_tokens)\n",
        "\n",
        "answer_outputs = Bidirectional(GRU(GRU_UNITS, return_sequences=True), name = 'answer_outputs')(document_emb)\n",
        "answer_tags = Dense(2, activation = 'softmax', name = 'answer_tags')(answer_outputs)\n",
        "\n",
        "encoder_input_mask = Input(shape=(MAX_ANSWER_SIZE, MAX_DOC_SIZE), name=\"encoder_input_mask\")\n",
        "encoder_inputs = Lambda(lambda x: K.batch_dot(x[0], x[1]), name=\"encoder_inputs\")([encoder_input_mask, answer_outputs])\n",
        "encoder_cell = GRU(2 * GRU_UNITS, name = 'encoder_cell')(encoder_inputs)\n",
        "\n",
        "decoder_inputs = Input(shape=(MAX_Q_SIZE,), name=\"decoder_inputs\")\n",
        "decoder_emb = embedding(decoder_inputs)\n",
        "decoder_emb.trainable = False\n",
        "decoder_cell = GRU(2 * GRU_UNITS, return_sequences = True, name = 'decoder_cell')\n",
        "decoder_states = decoder_cell(decoder_emb, initial_state = [encoder_cell])\n",
        "\n",
        "decoder_projection = Dense(VOCAB_SIZE, name = 'decoder_projection', activation = 'softmax', use_bias = False)\n",
        "decoder_outputs = decoder_projection(decoder_states)\n",
        "\n",
        "total_model = Model([document_tokens, decoder_inputs, encoder_input_mask], [answer_tags, decoder_outputs])\n",
        "plot_model(total_model, to_file='model.png',show_shapes=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NNieItYBBJql",
        "outputId": "0f54f84f-c155-4f97-9e4c-ef135c761027"
      },
      "source": [
        "decoder_emb.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([None, None, 100])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wR-rA7HtBJql"
      },
      "source": [
        "answer_model = Model(document_tokens, [answer_tags])\n",
        "decoder_initial_state_model = Model([document_tokens, encoder_input_mask], [encoder_cell])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AzfVT0ZxBJql",
        "outputId": "7c8c2393-428d-42fc-f823-bb94ae84df89"
      },
      "source": [
        "total_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "document_tokens (InputLayer)    [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, None, 100)    998400      document_tokens[0][0]            \n",
            "                                                                 decoder_inputs[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "answer_outputs (Bidirectional)  (None, None, 200)    121200      embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "encoder_input_mask (InputLayer) [(None, None, None)] 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "decoder_inputs (InputLayer)     [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "encoder_inputs (Lambda)         (None, None, 200)    0           encoder_input_mask[0][0]         \n",
            "                                                                 answer_outputs[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "encoder_cell (GRU)              (None, 200)          241200      encoder_inputs[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "decoder_cell (GRU)              (None, None, 200)    181200      embedding[1][0]                  \n",
            "                                                                 encoder_cell[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "answer_tags (Dense)             (None, None, 2)      402         answer_outputs[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "decoder_projection (Dense)      (None, None, 9984)   1996800     decoder_cell[0][0]               \n",
            "==================================================================================================\n",
            "Total params: 3,539,202\n",
            "Trainable params: 3,539,202\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zroR3T1XBJql"
      },
      "source": [
        "#### INFERENCE MODEL ####\n",
        "\n",
        "decoder_inputs_dynamic = Input(shape=(1,), name=\"decoder_inputs_dynamic\")\n",
        "decoder_emb_dynamic = embedding(decoder_inputs_dynamic)\n",
        "decoder_init_state_dynamic = Input(shape=(2 * GRU_UNITS,), name = 'decoder_init_state_dynamic') #the embedding of the previous word\n",
        "decoder_states_dynamic = decoder_cell(decoder_emb_dynamic, initial_state = [decoder_init_state_dynamic])\n",
        "decoder_outputs_dynamic = decoder_projection(decoder_states_dynamic)\n",
        "\n",
        "question_model = Model([decoder_inputs_dynamic, decoder_init_state_dynamic], [decoder_outputs_dynamic, decoder_states_dynamic])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ebM4N26kBJql"
      },
      "source": [
        "#### COMPILE TRAINING MODEL ####\n",
        "\n",
        "opti = Adam(lr=0.001)\n",
        "total_model.compile(loss=['sparse_categorical_crossentropy', 'sparse_categorical_crossentropy']\n",
        "                    , optimizer=opti\n",
        "                    , loss_weights = [1,1]) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utHSSDnBBJql"
      },
      "source": [
        "# train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxEXTO3rBJql"
      },
      "source": [
        "training_loss_history = []\n",
        "test_loss_history = []\n",
        "\n",
        "EPOCHS = 3\n",
        "start_epoch = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0geFg4oBJql",
        "outputId": "33b42f7a-c2bf-4793-93da-d11b66cf8ce1"
      },
      "source": [
        "for epoch in range(start_epoch, start_epoch + EPOCHS + 1):\n",
        "    print(\"Epoch {0}\".format(epoch))\n",
        "    \n",
        "    for i, batch in enumerate(training_data()):\n",
        "        \n",
        "        val_batch = next(test_data_gen, None)\n",
        "        \n",
        "        if val_batch is None:\n",
        "            test_data_gen = test_data()\n",
        "            val_batch = next(test_data_gen, None)\n",
        "            \n",
        "        training_loss = total_model.train_on_batch(\n",
        "            [batch['document_tokens'], batch['question_input_tokens'], batch['answer_masks']]\n",
        "            , [np.expand_dims(batch['answer_labels'], axis = -1), np.expand_dims(batch['question_output_tokens'], axis = -1)]\n",
        "        )\n",
        "        \n",
        "        test_loss = total_model.test_on_batch(\n",
        "            [val_batch['document_tokens'], val_batch['question_input_tokens'], val_batch['answer_masks']]\n",
        "            , [np.expand_dims(val_batch['answer_labels'], axis = -1), np.expand_dims(val_batch['question_output_tokens'], axis = -1)]\n",
        "        )\n",
        "        \n",
        "        training_loss_history.append(training_loss)\n",
        "        test_loss_history.append(test_loss)\n",
        "        \n",
        "        print(\"{}: Train Loss: {} | Test Loss: {}\".format(i, training_loss, test_loss))\n",
        "        \n",
        "        total_model.save_weights(os.path.join(RUN_FOLDER, 'weights/weights_{}.h5'.format(epoch)))\n",
        "        "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "0: Train Loss: [2.4305832386016846, 0.31344175338745117, 2.1171412467956543] | Test Loss: [4.013306617736816, 0.20902468264102936, 3.8042819499969482]\n",
            "1: Train Loss: [2.293238878250122, 0.29070138931274414, 2.002537488937378] | Test Loss: [4.005984783172607, 0.20907878875732422, 3.796905994415283]\n",
            "2: Train Loss: [2.450779676437378, 0.4104864299297333, 2.0402932167053223] | Test Loss: [4.008505344390869, 0.21342089772224426, 3.7950844764709473]\n",
            "3: Train Loss: [2.324796438217163, 0.251020222902298, 2.0737762451171875] | Test Loss: [4.009485244750977, 0.2158113718032837, 3.7936739921569824]\n",
            "4: Train Loss: [2.6023433208465576, 0.2926391065120697, 2.309704303741455] | Test Loss: [4.006556510925293, 0.21609294414520264, 3.79046368598938]\n",
            "5: Train Loss: [1.3628567457199097, 0.32056018710136414, 1.0422966480255127] | Test Loss: [4.003810405731201, 0.2151477038860321, 3.7886626720428467]\n",
            "6: Train Loss: [2.640296459197998, 0.3432840406894684, 2.2970123291015625] | Test Loss: [4.010798931121826, 0.2155236452817917, 3.7952752113342285]\n",
            "7: Train Loss: [1.3132048845291138, 0.37799811363220215, 0.9352067708969116] | Test Loss: [4.021650791168213, 0.218469500541687, 3.8031814098358154]\n",
            "8: Train Loss: [2.5304129123687744, 0.39585283398628235, 2.1345601081848145] | Test Loss: [4.0329270362854, 0.22349584102630615, 3.8094310760498047]\n",
            "9: Train Loss: [2.2208473682403564, 0.2683454155921936, 1.952501893043518] | Test Loss: [4.031713962554932, 0.22268472611904144, 3.8090293407440186]\n",
            "10: Train Loss: [2.300612211227417, 0.322214812040329, 1.9783973693847656] | Test Loss: [4.028976917266846, 0.22030998766422272, 3.808666706085205]\n",
            "11: Train Loss: [2.5219852924346924, 0.3262820541858673, 2.1957032680511475] | Test Loss: [4.031978607177734, 0.21724753081798553, 3.8147311210632324]\n",
            "12: Train Loss: [2.055992841720581, 0.4107004404067993, 1.6452924013137817] | Test Loss: [4.034896373748779, 0.21715886890888214, 3.817737579345703]\n",
            "13: Train Loss: [2.682974100112915, 0.29576626420021057, 2.3872077465057373] | Test Loss: [4.038924217224121, 0.2154276967048645, 3.8234965801239014]\n",
            "14: Train Loss: [1.6542081832885742, 0.32920002937316895, 1.3250081539154053] | Test Loss: [4.049992084503174, 0.21476785838603973, 3.835224151611328]\n",
            "15: Train Loss: [2.7307891845703125, 0.2676562964916229, 2.463132858276367] | Test Loss: [4.0591254234313965, 0.2132570445537567, 3.8458685874938965]\n",
            "16: Train Loss: [2.5939037799835205, 0.25677791237831116, 2.337125778198242] | Test Loss: [4.06998348236084, 0.21084104478359222, 3.859142303466797]\n",
            "17: Train Loss: [2.0936279296875, 0.4150811731815338, 1.6785467863082886] | Test Loss: [4.079986095428467, 0.21251565217971802, 3.8674705028533936]\n",
            "18: Train Loss: [2.3633337020874023, 0.30726057291030884, 2.0560731887817383] | Test Loss: [4.092528820037842, 0.21484383940696716, 3.877685070037842]\n",
            "19: Train Loss: [2.254835844039917, 0.26691675186157227, 1.9879190921783447] | Test Loss: [4.100820064544678, 0.21555034816265106, 3.8852696418762207]\n",
            "20: Train Loss: [2.5753836631774902, 0.3303758502006531, 2.2450077533721924] | Test Loss: [4.107046127319336, 0.21639283001422882, 3.890653133392334]\n",
            "21: Train Loss: [2.3696577548980713, 0.27289944887161255, 2.0967583656311035] | Test Loss: [4.110919952392578, 0.2163107693195343, 3.894608974456787]\n",
            "22: Train Loss: [2.4045469760894775, 0.34766894578933716, 2.056878089904785] | Test Loss: [4.10896110534668, 0.21723826229572296, 3.8917229175567627]\n",
            "23: Train Loss: [2.436936378479004, 0.2957192659378052, 2.141216993331909] | Test Loss: [4.1051740646362305, 0.21729794144630432, 3.887876033782959]\n",
            "24: Train Loss: [2.1906564235687256, 0.28449955582618713, 1.9061567783355713] | Test Loss: [4.08856725692749, 0.21637223660945892, 3.87219500541687]\n",
            "25: Train Loss: [2.291992425918579, 0.32273826003074646, 1.9692541360855103] | Test Loss: [4.075467586517334, 0.2172337919473648, 3.858233690261841]\n",
            "26: Train Loss: [2.0411550998687744, 0.338560551404953, 1.702594518661499] | Test Loss: [4.063335418701172, 0.21903160214424133, 3.844303846359253]\n",
            "27: Train Loss: [2.7566909790039062, 0.3391215205192566, 2.417569398880005] | Test Loss: [4.057872295379639, 0.22184020280838013, 3.8360321521759033]\n",
            "28: Train Loss: [1.9845174551010132, 0.249452605843544, 1.73506498336792] | Test Loss: [4.051633834838867, 0.22097313404083252, 3.830660581588745]\n",
            "29: Train Loss: [2.706268787384033, 0.3110657334327698, 2.395203113555908] | Test Loss: [4.04973030090332, 0.21937604248523712, 3.8303542137145996]\n",
            "30: Train Loss: [2.797877073287964, 0.28924936056137085, 2.5086276531219482] | Test Loss: [4.057063102722168, 0.21716277301311493, 3.839900255203247]\n",
            "31: Train Loss: [2.376232624053955, 0.2614043056964874, 2.11482834815979] | Test Loss: [4.065098762512207, 0.21436461806297302, 3.850733995437622]\n",
            "32: Train Loss: [2.5731983184814453, 0.38867858052253723, 2.1845197677612305] | Test Loss: [4.078772068023682, 0.21506865322589874, 3.8637032508850098]\n",
            "33: Train Loss: [2.291731834411621, 0.32890090346336365, 1.962830901145935] | Test Loss: [4.089678764343262, 0.21635961532592773, 3.873318910598755]\n",
            "34: Train Loss: [2.421968460083008, 0.29114729166030884, 2.1308212280273438] | Test Loss: [4.1030049324035645, 0.2174399048089981, 3.8855648040771484]\n",
            "35: Train Loss: [2.2563273906707764, 0.3163404166698456, 1.939987063407898] | Test Loss: [4.112993240356445, 0.21957692503929138, 3.893416166305542]\n",
            "36: Train Loss: [2.666221857070923, 0.2792368531227112, 2.3869850635528564] | Test Loss: [4.125122547149658, 0.22078531980514526, 3.9043374061584473]\n",
            "37: Train Loss: [2.6679816246032715, 0.31127750873565674, 2.356703996658325] | Test Loss: [4.144896507263184, 0.22098056972026825, 3.9239158630371094]\n",
            "38: Train Loss: [2.511237144470215, 0.3438304662704468, 2.1674065589904785] | Test Loss: [4.167870044708252, 0.2232041358947754, 3.9446659088134766]\n",
            "39: Train Loss: [2.294205904006958, 0.2653375566005707, 2.0288684368133545] | Test Loss: [4.18709135055542, 0.22296354174613953, 3.964128017425537]\n",
            "40: Train Loss: [2.147913932800293, 0.2634780704975128, 1.884435772895813] | Test Loss: [4.196659088134766, 0.22028684616088867, 3.976372480392456]\n",
            "41: Train Loss: [2.6722676753997803, 0.2675533592700958, 2.404714345932007] | Test Loss: [4.206520080566406, 0.2161761075258255, 3.9903438091278076]\n",
            "42: Train Loss: [2.6050219535827637, 0.3740495443344116, 2.2309722900390625] | Test Loss: [4.220548152923584, 0.21372854709625244, 4.006819725036621]\n",
            "43: Train Loss: [2.660553455352783, 0.4213639497756958, 2.239189624786377] | Test Loss: [4.240521430969238, 0.21501734852790833, 4.025504112243652]\n",
            "44: Train Loss: [2.554075241088867, 0.3440106511116028, 2.210064649581909] | Test Loss: [4.257373332977295, 0.21812273561954498, 4.039250373840332]\n",
            "45: Train Loss: [2.42307186126709, 0.3010421097278595, 2.1220297813415527] | Test Loss: [4.263485908508301, 0.2197941094636917, 4.043691635131836]\n",
            "46: Train Loss: [2.3363847732543945, 0.3031030595302582, 2.0332818031311035] | Test Loss: [4.265240669250488, 0.22062210738658905, 4.044618606567383]\n",
            "47: Train Loss: [2.2437989711761475, 0.3158794939517975, 1.9279193878173828] | Test Loss: [4.270175457000732, 0.22078374028205872, 4.049391746520996]\n",
            "48: Train Loss: [2.06266450881958, 0.33777916431427, 1.72488534450531] | Test Loss: [4.27322244644165, 0.21942050755023956, 4.053802013397217]\n",
            "49: Train Loss: [2.31817626953125, 0.29546916484832764, 2.022706985473633] | Test Loss: [4.271153926849365, 0.21673288941383362, 4.0544209480285645]\n",
            "50: Train Loss: [2.2694573402404785, 0.28183627128601074, 1.9876211881637573] | Test Loss: [4.266698360443115, 0.21377727389335632, 4.052921295166016]\n",
            "51: Train Loss: [2.346485137939453, 0.36513784527778625, 1.9813473224639893] | Test Loss: [4.262490749359131, 0.2125285416841507, 4.049962043762207]\n",
            "52: Train Loss: [2.6056675910949707, 0.3276023268699646, 2.2780652046203613] | Test Loss: [4.261209964752197, 0.21119633316993713, 4.050013542175293]\n",
            "53: Train Loss: [2.1461737155914307, 0.37934941053390503, 1.7668242454528809] | Test Loss: [4.25410795211792, 0.2121705263853073, 4.041937351226807]\n",
            "54: Train Loss: [2.787260055541992, 0.3445858061313629, 2.442674160003662] | Test Loss: [4.24326229095459, 0.21435528993606567, 4.02890682220459]\n",
            "55: Train Loss: [2.062774658203125, 0.32507583498954773, 1.7376989126205444] | Test Loss: [4.230307102203369, 0.21567313373088837, 4.014634132385254]\n",
            "56: Train Loss: [2.5598411560058594, 0.31854069232940674, 2.241300344467163] | Test Loss: [4.2107157707214355, 0.21481458842754364, 3.995901107788086]\n",
            "57: Train Loss: [2.4655494689941406, 0.3031598627567291, 2.1623895168304443] | Test Loss: [4.172297477722168, 0.2115035355091095, 3.9607937335968018]\n",
            "58: Train Loss: [2.3949899673461914, 0.28474751114845276, 2.1102423667907715] | Test Loss: [4.124710559844971, 0.20531633496284485, 3.919394016265869]\n",
            "59: Train Loss: [2.2935891151428223, 0.32971513271331787, 1.9638738632202148] | Test Loss: [4.089417934417725, 0.19977089762687683, 3.8896472454071045]\n",
            "60: Train Loss: [2.4847476482391357, 0.32791662216186523, 2.1568310260772705] | Test Loss: [4.063239574432373, 0.1961437612771988, 3.867095947265625]\n",
            "61: Train Loss: [2.383073091506958, 0.2905493676662445, 2.0925238132476807] | Test Loss: [4.046106338500977, 0.1926143318414688, 3.853492021560669]\n",
            "62: Train Loss: [2.1819379329681396, 0.3125624656677246, 1.8693755865097046] | Test Loss: [4.038361549377441, 0.1904810667037964, 3.8478806018829346]\n",
            "63: Train Loss: [2.471160650253296, 0.2871987223625183, 2.183961868286133] | Test Loss: [4.030938625335693, 0.18876460194587708, 3.8421740531921387]\n",
            "64: Train Loss: [2.1414589881896973, 0.32839977741241455, 1.8130592107772827] | Test Loss: [4.029311656951904, 0.18923136591911316, 3.8400802612304688]\n",
            "65: Train Loss: [2.93916654586792, 0.31787073612213135, 2.621295690536499] | Test Loss: [4.020812511444092, 0.1908455193042755, 3.8299670219421387]\n",
            "66: Train Loss: [2.2056281566619873, 0.3396661877632141, 1.865962028503418] | Test Loss: [4.019394397735596, 0.19389715790748596, 3.8254973888397217]\n",
            "67: Train Loss: [2.275928258895874, 0.2815278172492981, 1.9944005012512207] | Test Loss: [4.018256664276123, 0.19589324295520782, 3.8223636150360107]\n",
            "68: Train Loss: [2.2837538719177246, 0.2874835133552551, 1.9962702989578247] | Test Loss: [4.018404006958008, 0.19658344984054565, 3.8218207359313965]\n",
            "69: Train Loss: [2.559133529663086, 0.3028857111930847, 2.2562477588653564] | Test Loss: [4.024393081665039, 0.19534067809581757, 3.829052448272705]\n",
            "70: Train Loss: [2.682762384414673, 0.3335317075252533, 2.3492307662963867] | Test Loss: [4.023951530456543, 0.1948828399181366, 3.829068660736084]\n",
            "71: Train Loss: [2.240962028503418, 0.28248330950737, 1.9584788084030151] | Test Loss: [4.01909065246582, 0.19186149537563324, 3.8272292613983154]\n",
            "72: Train Loss: [2.830484628677368, 0.3342072665691376, 2.496277332305908] | Test Loss: [3.9907572269439697, 0.18955563008785248, 3.801201581954956]\n",
            "73: Train Loss: [2.382112741470337, 0.334372341632843, 2.0477404594421387] | Test Loss: [3.9610378742218018, 0.18798626959323883, 3.7730515003204346]\n",
            "74: Train Loss: [2.6523289680480957, 0.28671371936798096, 2.3656153678894043] | Test Loss: [3.9465079307556152, 0.18612012267112732, 3.760387897491455]\n",
            "75: Train Loss: [2.3842780590057373, 0.40986892580986023, 1.9744092226028442] | Test Loss: [3.9406187534332275, 0.1893993318080902, 3.7512195110321045]\n",
            "76: Train Loss: [2.5444700717926025, 0.38290366530418396, 2.1615664958953857] | Test Loss: [3.9305968284606934, 0.19657929241657257, 3.7340176105499268]\n",
            "77: Train Loss: [2.290668487548828, 0.3297460675239563, 1.9609224796295166] | Test Loss: [3.927199363708496, 0.20406237244606018, 3.7231369018554688]\n",
            "78: Train Loss: [2.4293293952941895, 0.2918465733528137, 2.1374828815460205] | Test Loss: [3.9191696643829346, 0.20704008638858795, 3.712129592895508]\n",
            "79: Train Loss: [1.8764140605926514, 0.30914199352264404, 1.5672720670700073] | Test Loss: [3.911318778991699, 0.20556676387786865, 3.705751895904541]\n",
            "80: Train Loss: [2.897588014602661, 0.26696401834487915, 2.6306240558624268] | Test Loss: [3.9036576747894287, 0.1995641440153122, 3.7040934562683105]\n",
            "81: Train Loss: [2.5869810581207275, 0.2946035861968994, 2.292377471923828] | Test Loss: [3.902435541152954, 0.19305258989334106, 3.709383010864258]\n",
            "82: Train Loss: [1.5984970331192017, 0.34516891837120056, 1.2533280849456787] | Test Loss: [3.909489393234253, 0.1890757828950882, 3.7204136848449707]\n",
            "83: Train Loss: [2.373462677001953, 0.32998335361480713, 2.0434792041778564] | Test Loss: [3.926853656768799, 0.1882086843252182, 3.738645076751709]\n",
            "84: Train Loss: [1.8690065145492554, 0.2772473394870758, 1.591759204864502] | Test Loss: [3.9447309970855713, 0.18734315037727356, 3.75738787651062]\n",
            "85: Train Loss: [2.3817148208618164, 0.28962016105651855, 2.092094659805298] | Test Loss: [3.9597055912017822, 0.18801763653755188, 3.7716879844665527]\n",
            "86: Train Loss: [2.8452625274658203, 0.2643875479698181, 2.5808749198913574] | Test Loss: [3.9748024940490723, 0.18962252140045166, 3.785179853439331]\n",
            "87: Train Loss: [2.452460527420044, 0.32069554924964905, 2.1317648887634277] | Test Loss: [3.9900732040405273, 0.19245049357414246, 3.7976226806640625]\n",
            "88: Train Loss: [2.6713502407073975, 0.3387729823589325, 2.3325772285461426] | Test Loss: [4.0074052810668945, 0.19584786891937256, 3.8115572929382324]\n",
            "89: Train Loss: [2.2963709831237793, 0.29949527978897095, 1.9968756437301636] | Test Loss: [4.021472930908203, 0.19947732985019684, 3.821995496749878]\n",
            "90: Train Loss: [2.5291035175323486, 0.26506465673446655, 2.2640388011932373] | Test Loss: [4.0390119552612305, 0.2040635049343109, 3.8349483013153076]\n",
            "91: Train Loss: [1.8686060905456543, 0.3331978917121887, 1.5354081392288208] | Test Loss: [4.04628324508667, 0.2095557451248169, 3.8367276191711426]\n",
            "92: Train Loss: [2.3828744888305664, 0.31807202100753784, 2.064802408218384] | Test Loss: [4.049516201019287, 0.2145012468099594, 3.835014820098877]\n",
            "93: Train Loss: [2.3341224193573, 0.27921128273010254, 2.0549111366271973] | Test Loss: [4.045658111572266, 0.21636533737182617, 3.8292930126190186]\n",
            "94: Train Loss: [2.6523115634918213, 0.26984015107154846, 2.3824713230133057] | Test Loss: [4.033627510070801, 0.21530120074748993, 3.818326473236084]\n",
            "95: Train Loss: [2.380934476852417, 0.2563132643699646, 2.1246211528778076] | Test Loss: [4.0116376876831055, 0.21243415772914886, 3.799203395843506]\n",
            "96: Train Loss: [2.450688362121582, 0.2997503876686096, 2.150938034057617] | Test Loss: [3.9931163787841797, 0.20991937816143036, 3.7831969261169434]\n",
            "97: Train Loss: [2.277261972427368, 0.2884694039821625, 1.9887926578521729] | Test Loss: [3.976045608520508, 0.20671342313289642, 3.76933217048645]\n",
            "98: Train Loss: [2.210346221923828, 0.316923588514328, 1.8934227228164673] | Test Loss: [3.962780714035034, 0.2058524638414383, 3.7569282054901123]\n",
            "99: Train Loss: [2.5470569133758545, 0.2985634207725525, 2.2484934329986572] | Test Loss: [3.9516143798828125, 0.20640629529953003, 3.7452080249786377]\n",
            "100: Train Loss: [2.434765577316284, 0.2885761559009552, 2.1461894512176514] | Test Loss: [3.952965259552002, 0.2066735029220581, 3.7462916374206543]\n",
            "101: Train Loss: [2.6395277976989746, 0.41149505972862244, 2.2280328273773193] | Test Loss: [3.960613965988159, 0.2117835432291031, 3.7488303184509277]\n",
            "102: Train Loss: [2.335623025894165, 0.3310892581939697, 2.0045340061187744] | Test Loss: [3.9702792167663574, 0.21894609928131104, 3.751333236694336]\n",
            "103: Train Loss: [2.259664297103882, 0.3308977484703064, 1.9287666082382202] | Test Loss: [3.9906373023986816, 0.22817350924015045, 3.7624638080596924]\n",
            "104: Train Loss: [2.2283642292022705, 0.2997756898403168, 1.928588628768921] | Test Loss: [4.000702857971191, 0.23452317714691162, 3.7661795616149902]\n",
            "105: Train Loss: [2.1968324184417725, 0.28034982085227966, 1.9164825677871704] | Test Loss: [3.995176315307617, 0.23285216093063354, 3.762324094772339]\n",
            "106: Train Loss: [2.5850119590759277, 0.2918919622898102, 2.2931199073791504] | Test Loss: [3.983181953430176, 0.22777920961380005, 3.7554028034210205]\n",
            "107: Train Loss: [2.492339611053467, 0.2863246500492096, 2.20601487159729] | Test Loss: [3.973667860031128, 0.22008787095546722, 3.753580093383789]\n",
            "108: Train Loss: [2.303809642791748, 0.2858051657676697, 2.0180044174194336] | Test Loss: [3.960322856903076, 0.21236683428287506, 3.7479560375213623]\n",
            "109: Train Loss: [2.3387155532836914, 0.2935124635696411, 2.04520320892334] | Test Loss: [3.9405062198638916, 0.20718447864055634, 3.7333216667175293]\n",
            "110: Train Loss: [2.29335880279541, 0.29075324535369873, 2.002605676651001] | Test Loss: [3.9218990802764893, 0.20372824370861053, 3.7181708812713623]\n",
            "111: Train Loss: [2.4883151054382324, 0.347368061542511, 2.140947103500366] | Test Loss: [3.905550956726074, 0.20296140015125275, 3.702589511871338]\n",
            "112: Train Loss: [1.9561847448349, 0.3829014301300049, 1.573283314704895] | Test Loss: [3.90401029586792, 0.20737548172473907, 3.6966347694396973]\n",
            "113: Train Loss: [2.264683246612549, 0.27603527903556824, 1.9886480569839478] | Test Loss: [3.9121294021606445, 0.21397413313388824, 3.698155164718628]\n",
            "114: Train Loss: [2.4997479915618896, 0.31428009271621704, 2.1854679584503174] | Test Loss: [3.922980785369873, 0.2225896269083023, 3.7003910541534424]\n",
            "115: Train Loss: [2.4015913009643555, 0.3324339985847473, 2.069157361984253] | Test Loss: [3.9370672702789307, 0.23271512985229492, 3.7043521404266357]\n",
            "116: Train Loss: [2.3985133171081543, 0.31252384185791016, 2.085989475250244] | Test Loss: [3.9553444385528564, 0.24023979902267456, 3.715104579925537]\n",
            "117: Train Loss: [2.329282760620117, 0.28360146284103394, 2.0456812381744385] | Test Loss: [3.971198081970215, 0.24511857330799103, 3.7260794639587402]\n",
            "118: Train Loss: [2.636568307876587, 0.2833227813243866, 2.353245496749878] | Test Loss: [3.9906256198883057, 0.242625892162323, 3.747999668121338]\n",
            "119: Train Loss: [2.584707260131836, 0.30167752504348755, 2.283029794692993] | Test Loss: [4.003501892089844, 0.23696188628673553, 3.766540050506592]\n",
            "120: Train Loss: [2.410640239715576, 0.3233656883239746, 2.0872745513916016] | Test Loss: [4.012617111206055, 0.23128673434257507, 3.7813305854797363]\n",
            "121: Train Loss: [2.0770390033721924, 0.344204843044281, 1.7328342199325562] | Test Loss: [4.024817943572998, 0.22816327214241028, 3.796654462814331]\n",
            "122: Train Loss: [1.1153498888015747, 0.29522621631622314, 0.8201236724853516] | Test Loss: [4.037006378173828, 0.22536563873291016, 3.811640501022339]\n",
            "123: Train Loss: [2.482206344604492, 0.32110196352005005, 2.161104440689087] | Test Loss: [4.045976161956787, 0.22266380488872528, 3.823312282562256]\n",
            "124: Train Loss: [2.7363686561584473, 0.3175317049026489, 2.418836832046509] | Test Loss: [4.044260501861572, 0.2209714651107788, 3.823289155960083]\n",
            "125: Train Loss: [2.69236421585083, 0.29859721660614014, 2.3937668800354004] | Test Loss: [4.042636394500732, 0.2197236716747284, 3.8229129314422607]\n",
            "126: Train Loss: [2.3502838611602783, 0.35274991393089294, 1.9975340366363525] | Test Loss: [4.038960933685303, 0.22193309664726257, 3.817028045654297]\n",
            "127: Train Loss: [2.013808488845825, 0.28784042596817017, 1.7259681224822998] | Test Loss: [4.034245014190674, 0.22370633482933044, 3.8105387687683105]\n",
            "128: Train Loss: [2.5135273933410645, 0.4125671684741974, 2.1009602546691895] | Test Loss: [4.048169136047363, 0.23293456435203552, 3.815234422683716]\n",
            "129: Train Loss: [2.4927165508270264, 0.4549615681171417, 2.037755012512207] | Test Loss: [4.078756332397461, 0.25342637300491333, 3.8253300189971924]\n",
            "130: Train Loss: [1.9765363931655884, 0.27301594614982605, 1.70352041721344] | Test Loss: [4.10393762588501, 0.26538267731666565, 3.838554859161377]\n",
            "131: Train Loss: [2.332712173461914, 0.30881306529045105, 2.0238990783691406] | Test Loss: [4.123529434204102, 0.2665656805038452, 3.856963634490967]\n",
            "132: Train Loss: [2.555083990097046, 0.3605095446109772, 2.1945743560791016] | Test Loss: [4.140857696533203, 0.25938552618026733, 3.88147234916687]\n",
            "133: Train Loss: [2.537940502166748, 0.27962440252304077, 2.2583160400390625] | Test Loss: [4.147372722625732, 0.2419666349887848, 3.9054062366485596]\n",
            "134: Train Loss: [2.3416261672973633, 0.3080500662326813, 2.033576011657715] | Test Loss: [4.152316570281982, 0.22261850535869598, 3.9296982288360596]\n",
            "135: Train Loss: [2.3620948791503906, 0.26524385809898376, 2.096851110458374] | Test Loss: [4.159117698669434, 0.20466695725917816, 3.9544506072998047]\n",
            "136: Train Loss: [2.527155876159668, 0.26001882553100586, 2.267137050628662] | Test Loss: [4.169845104217529, 0.19159705936908722, 3.978248119354248]\n",
            "137: Train Loss: [1.0362884998321533, 0.278838574886322, 0.7574498653411865] | Test Loss: [4.183637619018555, 0.18361634016036987, 4.000021457672119]\n",
            "138: Train Loss: [2.96486496925354, 0.32216551899909973, 2.6426994800567627] | Test Loss: [4.193902492523193, 0.18088598549365997, 4.013016700744629]\n",
            "139: Train Loss: [2.5295422077178955, 0.3011351227760315, 2.228407144546509] | Test Loss: [4.18817138671875, 0.1811361461877823, 4.007035255432129]\n",
            "140: Train Loss: [2.4344046115875244, 0.32840338349342346, 2.106001138687134] | Test Loss: [4.1851606369018555, 0.18266484141349792, 4.002495765686035]\n",
            "141: Train Loss: [1.8537741899490356, 0.39817285537719727, 1.4556013345718384] | Test Loss: [4.188629627227783, 0.19050118327140808, 3.9981284141540527]\n",
            "142: Train Loss: [2.5343406200408936, 0.30475831031799316, 2.2295823097229004] | Test Loss: [4.192963600158691, 0.2008599489927292, 3.9921035766601562]\n",
            "143: Train Loss: [2.5117597579956055, 0.3719974160194397, 2.1397624015808105] | Test Loss: [4.2056050300598145, 0.21719352900981903, 3.9884116649627686]\n",
            "144: Train Loss: [1.307940125465393, 0.29193249344825745, 1.016007661819458] | Test Loss: [4.219408988952637, 0.23298043012619019, 3.9864284992218018]\n",
            "145: Train Loss: [2.4921369552612305, 0.30083703994750977, 2.1912999153137207] | Test Loss: [4.23455810546875, 0.24643607437610626, 3.9881222248077393]\n",
            "146: Train Loss: [2.4408037662506104, 0.3091788589954376, 2.131624937057495] | Test Loss: [4.246447563171387, 0.254929780960083, 3.9915175437927246]\n",
            "147: Train Loss: [2.1034374237060547, 0.36464086174964905, 1.7387964725494385] | Test Loss: [4.241992473602295, 0.262306809425354, 3.9796855449676514]\n",
            "148: Train Loss: [2.6258907318115234, 0.3784250319004059, 2.2474656105041504] | Test Loss: [4.2420148849487305, 0.2694380283355713, 3.97257661819458]\n",
            "149: Train Loss: [2.511821985244751, 0.33086609840393066, 2.1809558868408203] | Test Loss: [4.237285137176514, 0.26994651556015015, 3.9673385620117188]\n",
            "150: Train Loss: [2.456820011138916, 0.32561105489730835, 2.131208896636963] | Test Loss: [4.2235107421875, 0.265547513961792, 3.957963466644287]\n",
            "151: Train Loss: [2.137726306915283, 0.3303854763507843, 1.8073407411575317] | Test Loss: [4.198519706726074, 0.26034894585609436, 3.938170909881592]\n",
            "152: Train Loss: [2.036273956298828, 0.2731548249721527, 1.7631192207336426] | Test Loss: [4.168254375457764, 0.2488049566745758, 3.9194495677948]\n",
            "153: Train Loss: [2.6375491619110107, 0.32783836126327515, 2.309710741043091] | Test Loss: [4.146371841430664, 0.24036931991577148, 3.9060025215148926]\n",
            "154: Train Loss: [2.2618930339813232, 0.2653426229953766, 1.9965503215789795] | Test Loss: [4.1242146492004395, 0.23102065920829773, 3.8931941986083984]\n",
            "155: Train Loss: [2.431736946105957, 0.28416234254837036, 2.1475746631622314] | Test Loss: [4.108285903930664, 0.22443746030330658, 3.8838486671447754]\n",
            "156: Train Loss: [2.2285573482513428, 0.29985201358795166, 1.9287053346633911] | Test Loss: [4.101754665374756, 0.2210233360528946, 3.8807311058044434]\n",
            "157: Train Loss: [2.5829806327819824, 0.3416914641857147, 2.2412891387939453] | Test Loss: [4.100193500518799, 0.2209681272506714, 3.879225254058838]\n",
            "158: Train Loss: [2.4499707221984863, 0.29694852232933044, 2.153022289276123] | Test Loss: [4.09074068069458, 0.22265999019145966, 3.8680808544158936]\n",
            "159: Train Loss: [2.2771801948547363, 0.3261018395423889, 1.9510782957077026] | Test Loss: [4.079784393310547, 0.22622868418693542, 3.853555679321289]\n",
            "160: Train Loss: [2.3419060707092285, 0.2878931164741516, 2.0540130138397217] | Test Loss: [4.0734710693359375, 0.23231050372123718, 3.841160535812378]\n",
            "161: Train Loss: [2.427625894546509, 0.3901568055152893, 2.0374691486358643] | Test Loss: [4.066829204559326, 0.24652178585529327, 3.8203072547912598]\n",
            "162: Train Loss: [2.371889114379883, 0.3059311807155609, 2.065958023071289] | Test Loss: [4.069275379180908, 0.26315417885780334, 3.806121349334717]\n",
            "163: Train Loss: [2.380716562271118, 0.2755047678947449, 2.1052117347717285] | Test Loss: [4.075015068054199, 0.2753814160823822, 3.799633741378784]\n",
            "164: Train Loss: [2.17132568359375, 0.39669477939605713, 1.7746307849884033] | Test Loss: [4.082334518432617, 0.2898675501346588, 3.7924671173095703]\n",
            "165: Train Loss: [1.2347850799560547, 0.3604607880115509, 0.8743243217468262] | Test Loss: [4.089382171630859, 0.3045199513435364, 3.7848620414733887]\n",
            "166: Train Loss: [2.0426993370056152, 0.3400382101535797, 1.702661156654358] | Test Loss: [4.0896501541137695, 0.31659600138664246, 3.7730541229248047]\n",
            "167: Train Loss: [2.1937882900238037, 0.305266797542572, 1.888521432876587] | Test Loss: [4.079500675201416, 0.31808122992515564, 3.7614192962646484]\n",
            "168: Train Loss: [2.192349433898926, 0.309246689081192, 1.8831026554107666] | Test Loss: [4.0649261474609375, 0.3124997317790985, 3.7524266242980957]\n",
            "169: Train Loss: [2.5490643978118896, 0.3716455101966858, 2.1774189472198486] | Test Loss: [4.041384696960449, 0.3070499897003174, 3.7343344688415527]\n",
            "170: Train Loss: [2.227365255355835, 0.28030627965927124, 1.947058916091919] | Test Loss: [4.014577865600586, 0.2955489754676819, 3.719028949737549]\n",
            "171: Train Loss: [2.65824818611145, 0.3444029688835144, 2.313845157623291] | Test Loss: [3.981572151184082, 0.2841009795665741, 3.6974711418151855]\n",
            "172: Train Loss: [2.3387277126312256, 0.26699408888816833, 2.0717337131500244] | Test Loss: [3.942385196685791, 0.27098432183265686, 3.671400785446167]\n",
            "173: Train Loss: [2.4187088012695312, 0.34237945079803467, 2.0763297080993652] | Test Loss: [3.911545991897583, 0.26138681173324585, 3.6501591205596924]\n",
            "174: Train Loss: [2.3208401203155518, 0.32626721262931824, 1.9945728778839111] | Test Loss: [3.887267589569092, 0.25502029061317444, 3.63224720954895]\n",
            "175: Train Loss: [2.6891777515411377, 0.3380512595176697, 2.3511264324188232] | Test Loss: [3.8731470108032227, 0.2536267340183258, 3.6195201873779297]\n",
            "176: Train Loss: [2.463592767715454, 0.3213452994823456, 2.142247438430786] | Test Loss: [3.8707635402679443, 0.2548341155052185, 3.615929365158081]\n",
            "177: Train Loss: [2.505232572555542, 0.2506925165653229, 2.254539966583252] | Test Loss: [3.8674135208129883, 0.2535478472709656, 3.613865613937378]\n",
            "178: Train Loss: [2.5090131759643555, 0.4114552438259125, 2.09755802154541] | Test Loss: [3.8767447471618652, 0.2580913305282593, 3.6186535358428955]\n",
            "179: Train Loss: [2.5605602264404297, 0.2733882665634155, 2.2871720790863037] | Test Loss: [3.8930983543395996, 0.26239609718322754, 3.630702257156372]\n",
            "180: Train Loss: [2.441063165664673, 0.3221833109855652, 2.118879795074463] | Test Loss: [3.912659168243408, 0.2653190493583679, 3.6473400592803955]\n",
            "181: Train Loss: [2.4639668464660645, 0.25027477741241455, 2.2136919498443604] | Test Loss: [3.9230713844299316, 0.2644910216331482, 3.6585803031921387]\n",
            "182: Train Loss: [2.423861265182495, 0.33483630418777466, 2.0890250205993652] | Test Loss: [3.928478479385376, 0.2635023891925812, 3.664976119995117]\n",
            "183: Train Loss: [2.6280157566070557, 0.33122798800468445, 2.296787738800049] | Test Loss: [3.929953098297119, 0.2671959698200226, 3.662757158279419]\n",
            "184: Train Loss: [2.28898286819458, 0.30255624651908875, 1.986426591873169] | Test Loss: [3.921804904937744, 0.2685372233390808, 3.6532676219940186]\n",
            "185: Train Loss: [2.537415027618408, 0.30807673931121826, 2.2293381690979004] | Test Loss: [3.9165709018707275, 0.2678110897541046, 3.6487598419189453]\n",
            "186: Train Loss: [2.511894702911377, 0.34540095925331116, 2.1664936542510986] | Test Loss: [3.91605544090271, 0.2704378068447113, 3.645617723464966]\n",
            "187: Train Loss: [1.8444571495056152, 0.2890704572200775, 1.5553866624832153] | Test Loss: [3.9153783321380615, 0.27063098549842834, 3.644747257232666]\n",
            "188: Train Loss: [2.3094608783721924, 0.32351168990135193, 1.9859492778778076] | Test Loss: [3.914039134979248, 0.26915305852890015, 3.644886016845703]\n",
            "189: Train Loss: [2.1326584815979004, 0.31481003761291504, 1.817848563194275] | Test Loss: [3.919151782989502, 0.26746857166290283, 3.6516833305358887]\n",
            "190: Train Loss: [2.520883083343506, 0.32317790389060974, 2.1977052688598633] | Test Loss: [3.9325501918792725, 0.26578769087791443, 3.666762590408325]\n",
            "191: Train Loss: [2.4225375652313232, 0.35507115721702576, 2.0674664974212646] | Test Loss: [3.9423203468322754, 0.26435285806655884, 3.6779675483703613]\n",
            "192: Train Loss: [2.2010574340820312, 0.3548986315727234, 1.8461588621139526] | Test Loss: [3.953908681869507, 0.26279088854789734, 3.691117763519287]\n",
            "193: Train Loss: [2.8563361167907715, 0.3209875822067261, 2.535348415374756] | Test Loss: [3.9764928817749023, 0.2614637017250061, 3.715029239654541]\n",
            "194: Train Loss: [1.8379122018814087, 0.30437004566192627, 1.5335421562194824] | Test Loss: [3.995396614074707, 0.2596414089202881, 3.735755205154419]\n",
            "195: Train Loss: [2.4976863861083984, 0.25985056161880493, 2.2378358840942383] | Test Loss: [4.003405570983887, 0.25396671891212463, 3.749438762664795]\n",
            "196: Train Loss: [2.0999057292938232, 0.3023589849472046, 1.7975467443466187] | Test Loss: [4.00276517868042, 0.24821168184280396, 3.7545535564422607]\n",
            "197: Train Loss: [2.049665927886963, 0.3184684216976166, 1.7311975955963135] | Test Loss: [3.9956586360931396, 0.24317963421344757, 3.752479076385498]\n",
            "198: Train Loss: [2.225696563720703, 0.28783896565437317, 1.9378575086593628] | Test Loss: [3.9812779426574707, 0.2375202476978302, 3.743757724761963]\n",
            "199: Train Loss: [2.449822425842285, 0.341282457113266, 2.1085400581359863] | Test Loss: [3.965876579284668, 0.2355794906616211, 3.730297088623047]\n",
            "200: Train Loss: [1.920145869255066, 0.2896624505519867, 1.6304833889007568] | Test Loss: [3.9554922580718994, 0.23338958621025085, 3.722102642059326]\n",
            "201: Train Loss: [2.7352406978607178, 0.353287011384964, 2.3819539546966553] | Test Loss: [3.949450969696045, 0.23300877213478088, 3.716442108154297]\n",
            "202: Train Loss: [2.303161382675171, 0.24721576273441315, 2.055945634841919] | Test Loss: [3.9431679248809814, 0.23126602172851562, 3.711901903152466]\n",
            "203: Train Loss: [2.1763651371002197, 0.35506346821784973, 1.8213015794754028] | Test Loss: [3.9404006004333496, 0.23121322691440582, 3.7091872692108154]\n",
            "204: Train Loss: [2.3533694744110107, 0.2915025055408478, 2.0618669986724854] | Test Loss: [3.9288313388824463, 0.23049502074718475, 3.698336362838745]\n",
            "205: Train Loss: [2.314016342163086, 0.29829561710357666, 2.0157206058502197] | Test Loss: [3.9190258979797363, 0.23026123642921448, 3.6887645721435547]\n",
            "206: Train Loss: [2.862123966217041, 0.32529816031455994, 2.5368258953094482] | Test Loss: [3.912407636642456, 0.23157267272472382, 3.680835008621216]\n",
            "207: Train Loss: [2.3520631790161133, 0.28808891773223877, 2.063974142074585] | Test Loss: [3.904878854751587, 0.23096606135368347, 3.673912763595581]\n",
            "208: Train Loss: [2.4197261333465576, 0.296237051486969, 2.1234891414642334] | Test Loss: [3.8935155868530273, 0.22943727672100067, 3.6640782356262207]\n",
            "209: Train Loss: [2.243661403656006, 0.29181864857673645, 1.9518426656723022] | Test Loss: [3.8781051635742188, 0.22657783329486847, 3.6515274047851562]\n",
            "210: Train Loss: [2.77060604095459, 0.2854517698287964, 2.485154390335083] | Test Loss: [3.8552093505859375, 0.22386205196380615, 3.631347179412842]\n",
            "211: Train Loss: [2.0788588523864746, 0.35987868905067444, 1.7189801931381226] | Test Loss: [3.8338980674743652, 0.22348859906196594, 3.6104094982147217]\n",
            "212: Train Loss: [2.010305166244507, 0.32002460956573486, 1.690280556678772] | Test Loss: [3.8160340785980225, 0.22445201873779297, 3.5915820598602295]\n",
            "213: Train Loss: [2.496406316757202, 0.3242478668689728, 2.1721584796905518] | Test Loss: [3.797366142272949, 0.22637341916561127, 3.5709927082061768]\n",
            "214: Train Loss: [2.4040615558624268, 0.36674898862838745, 2.0373125076293945] | Test Loss: [3.7828927040100098, 0.2305370569229126, 3.5523555278778076]\n",
            "215: Train Loss: [2.531750440597534, 0.2834897041320801, 2.248260736465454] | Test Loss: [3.769804000854492, 0.23295724391937256, 3.53684663772583]\n",
            "216: Train Loss: [2.331580400466919, 0.3087511956691742, 2.022829294204712] | Test Loss: [3.7595365047454834, 0.2343200296163559, 3.525216579437256]\n",
            "217: Train Loss: [1.8960793018341064, 0.3538452386856079, 1.5422340631484985] | Test Loss: [3.7476890087127686, 0.23674647510051727, 3.5109424591064453]\n",
            "218: Train Loss: [2.837876796722412, 0.3297702670097351, 2.5081064701080322] | Test Loss: [3.7367072105407715, 0.23605605959892273, 3.5006511211395264]\n",
            "219: Train Loss: [1.809195637702942, 0.29026779532432556, 1.518927812576294] | Test Loss: [3.7243683338165283, 0.2311515361070633, 3.4932167530059814]\n",
            "220: Train Loss: [1.9327398538589478, 0.3223351538181305, 1.6104047298431396] | Test Loss: [3.713738441467285, 0.22572726011276245, 3.488011121749878]\n",
            "221: Train Loss: [2.0213847160339355, 0.34206733107566833, 1.6793174743652344] | Test Loss: [3.706817388534546, 0.22123458981513977, 3.4855828285217285]\n",
            "222: Train Loss: [2.0814685821533203, 0.3249616324901581, 1.7565069198608398] | Test Loss: [3.7008168697357178, 0.2170834094285965, 3.4837334156036377]\n",
            "223: Train Loss: [2.314762830734253, 0.39424124360084534, 1.92052161693573] | Test Loss: [3.701329231262207, 0.21845340728759766, 3.4828758239746094]\n",
            "224: Train Loss: [2.554013729095459, 0.3463447690010071, 2.2076690196990967] | Test Loss: [3.700679302215576, 0.22232726216316223, 3.4783520698547363]\n",
            "225: Train Loss: [1.901390552520752, 0.2781032621860504, 1.623287320137024] | Test Loss: [3.700916290283203, 0.22364576160907745, 3.4772706031799316]\n",
            "226: Train Loss: [2.331153392791748, 0.2640438973903656, 2.0671095848083496] | Test Loss: [3.6948728561401367, 0.2208838164806366, 3.4739890098571777]\n",
            "227: Train Loss: [2.371863603591919, 0.2678963243961334, 2.1039671897888184] | Test Loss: [3.687892198562622, 0.21634460985660553, 3.4715476036071777]\n",
            "228: Train Loss: [2.349997043609619, 0.3050018548965454, 2.044995069503784] | Test Loss: [3.6775963306427, 0.21166308224201202, 3.465933322906494]\n",
            "229: Train Loss: [2.225464105606079, 0.33569297194480896, 1.8897711038589478] | Test Loss: [3.6716392040252686, 0.20981234312057495, 3.461826801300049]\n",
            "230: Train Loss: [1.6858415603637695, 0.3347620666027069, 1.3510794639587402] | Test Loss: [3.6623363494873047, 0.21021145582199097, 3.452124834060669]\n",
            "231: Train Loss: [2.478578805923462, 0.3276674747467041, 2.150911331176758] | Test Loss: [3.654752016067505, 0.21241430938243866, 3.44233775138855]\n",
            "232: Train Loss: [2.6878037452697754, 0.2774794399738312, 2.4103243350982666] | Test Loss: [3.6543033123016357, 0.21383848786354065, 3.440464735031128]\n",
            "233: Train Loss: [1.9513806104660034, 0.332727313041687, 1.6186532974243164] | Test Loss: [3.6634111404418945, 0.21931250393390656, 3.444098711013794]\n",
            "234: Train Loss: [1.9505475759506226, 0.3179309666156769, 1.6326167583465576] | Test Loss: [3.670691728591919, 0.22555238008499146, 3.4451394081115723]\n",
            "235: Train Loss: [2.6142420768737793, 0.3677387237548828, 2.2465033531188965] | Test Loss: [3.683239459991455, 0.23628012835979462, 3.4469592571258545]\n",
            "236: Train Loss: [2.3295769691467285, 0.35834938287734985, 1.9712275266647339] | Test Loss: [3.700082778930664, 0.24905654788017273, 3.451026201248169]\n",
            "237: Train Loss: [2.43475341796875, 0.3287017345428467, 2.1060516834259033] | Test Loss: [3.715221643447876, 0.2571471631526947, 3.4580745697021484]\n",
            "238: Train Loss: [2.0033681392669678, 0.33343306183815, 1.6699349880218506] | Test Loss: [3.728179454803467, 0.2619999051094055, 3.466179609298706]\n",
            "239: Train Loss: [1.3031681776046753, 0.3131139874458313, 0.990054190158844] | Test Loss: [3.7331504821777344, 0.2577854096889496, 3.475365161895752]\n",
            "240: Train Loss: [2.006136894226074, 0.2995564043521881, 1.706580400466919] | Test Loss: [3.7255678176879883, 0.2495063990354538, 3.4760613441467285]\n",
            "241: Train Loss: [2.1643612384796143, 0.32807743549346924, 1.836283802986145] | Test Loss: [3.7208786010742188, 0.24240820109844208, 3.4784703254699707]\n",
            "242: Train Loss: [2.3660571575164795, 0.2860279679298401, 2.080029249191284] | Test Loss: [3.7165262699127197, 0.23482468724250793, 3.481701612472534]\n",
            "243: Train Loss: [2.156005382537842, 0.3544235825538635, 1.801581859588623] | Test Loss: [3.7055442333221436, 0.23152998089790344, 3.4740142822265625]\n",
            "244: Train Loss: [2.343468189239502, 0.3842041790485382, 1.9592639207839966] | Test Loss: [3.701176166534424, 0.23227623105049133, 3.468899965286255]\n",
            "245: Train Loss: [2.2984089851379395, 0.3008229732513428, 1.9975861310958862] | Test Loss: [3.694453001022339, 0.23364274203777313, 3.4608101844787598]\n",
            "246: Train Loss: [2.4121172428131104, 0.33704060316085815, 2.0750765800476074] | Test Loss: [3.6854262351989746, 0.2371828258037567, 3.4482433795928955]\n",
            "247: Train Loss: [2.3600287437438965, 0.2795713543891907, 2.0804574489593506] | Test Loss: [3.677833080291748, 0.23979759216308594, 3.438035488128662]\n",
            "248: Train Loss: [1.7972053289413452, 0.28069183230400085, 1.516513466835022] | Test Loss: [3.6693806648254395, 0.24085994064807892, 3.428520679473877]\n",
            "249: Train Loss: [2.779116630554199, 0.30592191219329834, 2.4731945991516113] | Test Loss: [3.658402919769287, 0.24049854278564453, 3.4179043769836426]\n",
            "250: Train Loss: [2.6789910793304443, 0.26661303639411926, 2.4123780727386475] | Test Loss: [3.6474015712738037, 0.23971793055534363, 3.4076836109161377]\n",
            "251: Train Loss: [2.4885010719299316, 0.2781596779823303, 2.210341453552246] | Test Loss: [3.637721538543701, 0.2381114959716797, 3.3996100425720215]\n",
            "252: Train Loss: [2.471602439880371, 0.3452645242214203, 2.126338005065918] | Test Loss: [3.6324970722198486, 0.23984704911708832, 3.3926501274108887]\n",
            "253: Train Loss: [2.256516456604004, 0.3307724595069885, 1.9257440567016602] | Test Loss: [3.634061098098755, 0.24402114748954773, 3.3900399208068848]\n",
            "254: Train Loss: [2.570984125137329, 0.42892900109291077, 2.142055034637451] | Test Loss: [3.6473660469055176, 0.25376561284065247, 3.3936004638671875]\n",
            "255: Train Loss: [2.666231870651245, 0.28757110238075256, 2.3786606788635254] | Test Loss: [3.652287006378174, 0.2606631815433502, 3.3916237354278564]\n",
            "256: Train Loss: [2.2227513790130615, 0.2978564202785492, 1.92489492893219] | Test Loss: [3.654207944869995, 0.2641620934009552, 3.3900458812713623]\n",
            "257: Train Loss: [2.931425094604492, 0.36835241317749023, 2.563072681427002] | Test Loss: [3.6582372188568115, 0.2707517743110657, 3.3874855041503906]\n",
            "258: Train Loss: [2.3774054050445557, 0.320930153131485, 2.0564751625061035] | Test Loss: [3.659651517868042, 0.2734273374080658, 3.3862242698669434]\n",
            "259: Train Loss: [2.2709357738494873, 0.37570464611053467, 1.8952311277389526] | Test Loss: [3.6613783836364746, 0.27526354789733887, 3.3861148357391357]\n",
            "260: Train Loss: [2.472230911254883, 0.33552756905555725, 2.1367032527923584] | Test Loss: [3.651620864868164, 0.27432191371917725, 3.3772990703582764]\n",
            "261: Train Loss: [2.167595624923706, 0.2691926062107086, 1.8984030485153198] | Test Loss: [3.634904146194458, 0.26369136571884155, 3.3712127208709717]\n",
            "262: Train Loss: [2.160656690597534, 0.275889128446579, 1.8847675323486328] | Test Loss: [3.616610050201416, 0.24747654795646667, 3.369133472442627]\n",
            "263: Train Loss: [2.3956191539764404, 0.27965062856674194, 2.1159684658050537] | Test Loss: [3.608245611190796, 0.23166713118553162, 3.3765785694122314]\n",
            "264: Train Loss: [2.442817211151123, 0.2831932306289673, 2.159623861312866] | Test Loss: [3.6101982593536377, 0.21906834840774536, 3.391129970550537]\n",
            "265: Train Loss: [1.6523237228393555, 0.3477405607700348, 1.304583191871643] | Test Loss: [3.62069034576416, 0.21134726703166962, 3.4093430042266846]\n",
            "266: Train Loss: [2.302861213684082, 0.399646520614624, 1.9032148122787476] | Test Loss: [3.6358816623687744, 0.21074581146240234, 3.425135850906372]\n",
            "267: Train Loss: [2.3615243434906006, 0.3375631868839264, 2.023961067199707] | Test Loss: [3.654240131378174, 0.21559424698352814, 3.438645839691162]\n",
            "268: Train Loss: [2.4827089309692383, 0.31964653730392456, 2.163062334060669] | Test Loss: [3.678999185562134, 0.2240379899740219, 3.4549612998962402]\n",
            "269: Train Loss: [2.451641321182251, 0.3011486828327179, 2.1504926681518555] | Test Loss: [3.7001893520355225, 0.23283131420612335, 3.467358112335205]\n",
            "270: Train Loss: [0.9189846515655518, 0.293387770652771, 0.6255968809127808] | Test Loss: [3.717985153198242, 0.24006135761737823, 3.47792387008667]\n",
            "271: Train Loss: [2.3153226375579834, 0.28878504037857056, 2.0265376567840576] | Test Loss: [3.738689422607422, 0.24709618091583252, 3.4915931224823]\n",
            "272: Train Loss: [2.490306854248047, 0.30144503712654114, 2.188861846923828] | Test Loss: [3.75141978263855, 0.25209060311317444, 3.499329090118408]\n",
            "273: Train Loss: [2.177183151245117, 0.2544202506542206, 1.9227628707885742] | Test Loss: [3.7413065433502197, 0.24986562132835388, 3.491441011428833]\n",
            "274: Train Loss: [1.943552017211914, 0.2589375972747803, 1.6846144199371338] | Test Loss: [3.729806900024414, 0.24387477338314056, 3.4859321117401123]\n",
            "275: Train Loss: [2.2775723934173584, 0.4091770052909851, 1.8683955669403076] | Test Loss: [3.7218782901763916, 0.24325856566429138, 3.4786198139190674]\n",
            "276: Train Loss: [2.421658992767334, 0.350137859582901, 2.071521043777466] | Test Loss: [3.7169697284698486, 0.2438696175813675, 3.473100185394287]\n",
            "277: Train Loss: [2.514866828918457, 0.3027443587779999, 2.2121224403381348] | Test Loss: [3.7075352668762207, 0.24372589588165283, 3.4638094902038574]\n",
            "278: Train Loss: [2.1250264644622803, 0.273042768239975, 1.851983666419983] | Test Loss: [3.6997861862182617, 0.24252745509147644, 3.457258701324463]\n",
            "279: Train Loss: [2.089050054550171, 0.29599830508232117, 1.793051838874817] | Test Loss: [3.694729804992676, 0.24234803020954132, 3.4523818492889404]\n",
            "280: Train Loss: [2.0538675785064697, 0.34294670820236206, 1.7109209299087524] | Test Loss: [3.6863813400268555, 0.24752342700958252, 3.4388580322265625]\n",
            "281: Train Loss: [2.5875463485717773, 0.3175064027309418, 2.2700400352478027] | Test Loss: [3.6825761795043945, 0.2539275884628296, 3.4286484718322754]\n",
            "282: Train Loss: [2.847106456756592, 0.27805015444755554, 2.569056272506714] | Test Loss: [3.6774916648864746, 0.25790852308273315, 3.4195830821990967]\n",
            "283: Train Loss: [1.8973196744918823, 0.28466272354125977, 1.6126569509506226] | Test Loss: [3.667137622833252, 0.25900202989578247, 3.4081356525421143]\n",
            "284: Train Loss: [2.4592409133911133, 0.3667994439601898, 2.0924415588378906] | Test Loss: [3.670975685119629, 0.2660621106624603, 3.4049136638641357]\n",
            "285: Train Loss: [2.5820887088775635, 0.2584795355796814, 2.3236091136932373] | Test Loss: [3.676941394805908, 0.2717263698577881, 3.40521502494812]\n",
            "286: Train Loss: [2.6178741455078125, 0.2899632155895233, 2.327910900115967] | Test Loss: [3.6815266609191895, 0.2743581235408783, 3.4071686267852783]\n",
            "287: Train Loss: [1.6474390029907227, 0.29023289680480957, 1.357206106185913] | Test Loss: [3.6827495098114014, 0.2759152054786682, 3.406834363937378]\n",
            "288: Train Loss: [2.2541961669921875, 0.29463887214660645, 1.959557294845581] | Test Loss: [3.6743316650390625, 0.2764035165309906, 3.397928237915039]\n",
            "289: Train Loss: [2.531601905822754, 0.31866663694381714, 2.212935209274292] | Test Loss: [3.6652002334594727, 0.27965494990348816, 3.385545253753662]\n",
            "290: Train Loss: [2.173938035964966, 0.3390112817287445, 1.8349268436431885] | Test Loss: [3.6609573364257812, 0.2815060317516327, 3.379451274871826]\n",
            "291: Train Loss: [2.488821029663086, 0.28649550676345825, 2.2023255825042725] | Test Loss: [3.6642324924468994, 0.2827698886394501, 3.381462574005127]\n",
            "292: Train Loss: [2.2540924549102783, 0.2733753025531769, 1.9807171821594238] | Test Loss: [3.6711337566375732, 0.28258436918258667, 3.388549327850342]\n",
            "293: Train Loss: [2.45900297164917, 0.2994691729545593, 2.159533739089966] | Test Loss: [3.674013614654541, 0.2823345959186554, 3.391679048538208]\n",
            "294: Train Loss: [2.3782691955566406, 0.2998296022415161, 2.078439712524414] | Test Loss: [3.6725611686706543, 0.2820700705051422, 3.390491008758545]\n",
            "295: Train Loss: [2.318058967590332, 0.3037559390068054, 2.014302968978882] | Test Loss: [3.6713716983795166, 0.28022077679634094, 3.391150951385498]\n",
            "296: Train Loss: [2.25064754486084, 0.2920282483100891, 1.958619236946106] | Test Loss: [3.666653871536255, 0.27593469619750977, 3.390719175338745]\n",
            "297: Train Loss: [1.9882214069366455, 0.3162921071052551, 1.6719292402267456] | Test Loss: [3.6628549098968506, 0.2720063626766205, 3.3908486366271973]\n",
            "298: Train Loss: [2.2814793586730957, 0.3630877435207367, 1.9183915853500366] | Test Loss: [3.6697278022766113, 0.2722092270851135, 3.3975186347961426]\n",
            "299: Train Loss: [1.3342268466949463, 0.29968327283859253, 1.0345436334609985] | Test Loss: [3.681349277496338, 0.27340856194496155, 3.407940626144409]\n",
            "300: Train Loss: [2.5562403202056885, 0.34359148144721985, 2.212648868560791] | Test Loss: [3.699784278869629, 0.2784673273563385, 3.4213168621063232]\n",
            "301: Train Loss: [2.500791549682617, 0.29352691769599915, 2.2072646617889404] | Test Loss: [3.7194507122039795, 0.28358766436576843, 3.4358630180358887]\n",
            "302: Train Loss: [2.515011787414551, 0.3349485397338867, 2.180063247680664] | Test Loss: [3.736988067626953, 0.2879803478717804, 3.449007749557495]\n",
            "303: Train Loss: [2.2055022716522217, 0.30903443694114685, 1.896467924118042] | Test Loss: [3.747934103012085, 0.287426233291626, 3.460507869720459]\n",
            "304: Train Loss: [2.59342098236084, 0.3601248264312744, 2.2332961559295654] | Test Loss: [3.7670223712921143, 0.29062506556510925, 3.4763972759246826]\n",
            "305: Train Loss: [2.443873882293701, 0.3169231712818146, 2.126950740814209] | Test Loss: [3.7895851135253906, 0.29017889499664307, 3.499406337738037]\n",
            "306: Train Loss: [2.735426187515259, 0.3651806712150574, 2.3702454566955566] | Test Loss: [3.804448127746582, 0.29090866446495056, 3.5135395526885986]\n",
            "307: Train Loss: [2.1807894706726074, 0.40183505415916443, 1.7789545059204102] | Test Loss: [3.826378107070923, 0.29800382256507874, 3.528374195098877]\n",
            "308: Train Loss: [2.3185195922851562, 0.31880903244018555, 1.9997104406356812] | Test Loss: [3.8452703952789307, 0.29954561591148376, 3.545724868774414]\n",
            "309: Train Loss: [2.0861783027648926, 0.28464457392692566, 1.801533818244934] | Test Loss: [3.852172374725342, 0.2917787730693817, 3.5603935718536377]\n",
            "310: Train Loss: [2.0642380714416504, 0.3132404386997223, 1.750997543334961] | Test Loss: [3.852943181991577, 0.27986693382263184, 3.5730762481689453]\n",
            "311: Train Loss: [2.2659108638763428, 0.30620643496513367, 1.9597045183181763] | Test Loss: [3.8425135612487793, 0.26980486512184143, 3.5727086067199707]\n",
            "312: Train Loss: [1.8462846279144287, 0.3387228548526764, 1.5075618028640747] | Test Loss: [3.8437273502349854, 0.26798200607299805, 3.5757453441619873]\n",
            "313: Train Loss: [2.69564151763916, 0.30092349648475647, 2.3947179317474365] | Test Loss: [3.8411202430725098, 0.26521867513656616, 3.575901508331299]\n",
            "314: Train Loss: [2.205986738204956, 0.31855538487434387, 1.887431263923645] | Test Loss: [3.8430166244506836, 0.26310116052627563, 3.5799155235290527]\n",
            "315: Train Loss: [2.4256601333618164, 0.322099506855011, 2.10356068611145] | Test Loss: [3.8431756496429443, 0.262690007686615, 3.5804855823516846]\n",
            "316: Train Loss: [2.602226495742798, 0.45013198256492615, 2.152094602584839] | Test Loss: [3.850217819213867, 0.27604928612709045, 3.5741684436798096]\n",
            "317: Train Loss: [2.598848342895508, 0.3183874189853668, 2.280460834503174] | Test Loss: [3.8655171394348145, 0.293076753616333, 3.5724403858184814]\n",
            "318: Train Loss: [0.631757915019989, 0.37530407309532166, 0.25645384192466736] | Test Loss: [3.8907203674316406, 0.3162727355957031, 3.5744476318359375]\n",
            "319: Train Loss: [2.349475860595703, 0.32474371790885925, 2.0247321128845215] | Test Loss: [3.8938891887664795, 0.3289332091808319, 3.564955949783325]\n",
            "320: Train Loss: [2.639468193054199, 0.35422465205192566, 2.285243511199951] | Test Loss: [3.8784265518188477, 0.3250335454940796, 3.5533928871154785]\n",
            "321: Train Loss: [2.4530856609344482, 0.3358963429927826, 2.117189407348633] | Test Loss: [3.8514211177825928, 0.3170628547668457, 3.534358263015747]\n",
            "322: Train Loss: [2.7677080631256104, 0.3366737961769104, 2.4310343265533447] | Test Loss: [3.8225300312042236, 0.2998127043247223, 3.522717237472534]\n",
            "323: Train Loss: [2.5459673404693604, 0.30139607191085815, 2.2445712089538574] | Test Loss: [3.7891745567321777, 0.2807179093360901, 3.5084567070007324]\n",
            "324: Train Loss: [2.2880563735961914, 0.29361945390701294, 1.9944368600845337] | Test Loss: [3.7638309001922607, 0.26288527250289917, 3.500945568084717]\n",
            "325: Train Loss: [2.316047430038452, 0.3229285478591919, 1.9931188821792603] | Test Loss: [3.7511062622070312, 0.25332871079444885, 3.4977774620056152]\n",
            "326: Train Loss: [2.191934108734131, 0.251904159784317, 1.9400298595428467] | Test Loss: [3.7371268272399902, 0.24502021074295044, 3.4921066761016846]\n",
            "327: Train Loss: [2.54929256439209, 0.3740241229534149, 2.1752684116363525] | Test Loss: [3.733842372894287, 0.24464218318462372, 3.4892001152038574]\n",
            "328: Train Loss: [2.1824212074279785, 0.369880348443985, 1.812540888786316] | Test Loss: [3.7385823726654053, 0.24876998364925385, 3.4898123741149902]\n",
            "329: Train Loss: [2.367793083190918, 0.28657066822052, 2.0812225341796875] | Test Loss: [3.7534635066986084, 0.25606569647789, 3.4973978996276855]\n",
            "330: Train Loss: [2.0526647567749023, 0.27148276567459106, 1.7811819314956665] | Test Loss: [3.775153875350952, 0.2635793089866638, 3.5115745067596436]\n",
            "331: Train Loss: [2.1699185371398926, 0.3563348948955536, 1.8135836124420166] | Test Loss: [3.8003251552581787, 0.2737556993961334, 3.526569366455078]\n",
            "332: Train Loss: [2.120575428009033, 0.2657204568386078, 1.854854941368103] | Test Loss: [3.8195583820343018, 0.28077057003974915, 3.538787841796875]\n",
            "333: Train Loss: [2.5820939540863037, 0.312496155500412, 2.2695977687835693] | Test Loss: [3.8377082347869873, 0.2874407172203064, 3.550267457962036]\n",
            "334: Train Loss: [2.6507341861724854, 0.35129621624946594, 2.299437999725342] | Test Loss: [3.8596255779266357, 0.29490551352500916, 3.5647201538085938]\n",
            "335: Train Loss: [2.3308327198028564, 0.2845875918865204, 2.0462450981140137] | Test Loss: [3.8716776371002197, 0.29862385988235474, 3.5730538368225098]\n",
            "336: Train Loss: [2.2750518321990967, 0.35520169138908386, 1.9198501110076904] | Test Loss: [3.889284610748291, 0.3054151237010956, 3.583869457244873]\n",
            "337: Train Loss: [2.319730281829834, 0.2694154381752014, 2.0503149032592773] | Test Loss: [3.895606756210327, 0.30377891659736633, 3.591827869415283]\n",
            "338: Train Loss: [1.8380517959594727, 0.30116620659828186, 1.5368856191635132] | Test Loss: [3.898111581802368, 0.2990076243877411, 3.5991039276123047]\n",
            "339: Train Loss: [2.4694528579711914, 0.3305472731590271, 2.1389055252075195] | Test Loss: [3.8969852924346924, 0.2953498065471649, 3.601635456085205]\n",
            "340: Train Loss: [1.9708566665649414, 0.3498620390892029, 1.6209945678710938] | Test Loss: [3.8986103534698486, 0.29535555839538574, 3.603254795074463]\n",
            "341: Train Loss: [2.632892370223999, 0.35373061895370483, 2.2791616916656494] | Test Loss: [3.890852928161621, 0.2965370714664459, 3.594315767288208]\n",
            "342: Train Loss: [1.8508753776550293, 0.3425057530403137, 1.5083695650100708] | Test Loss: [3.883694887161255, 0.29701855778694153, 3.5866763591766357]\n",
            "343: Train Loss: [2.1195428371429443, 0.278668612241745, 1.840874195098877] | Test Loss: [3.8670694828033447, 0.29391762614250183, 3.5731518268585205]\n",
            "344: Train Loss: [1.9271003007888794, 0.28445032238960266, 1.6426500082015991] | Test Loss: [3.8513033390045166, 0.2891745865345001, 3.562128782272339]\n",
            "345: Train Loss: [2.201817274093628, 0.2961446940898895, 1.9056726694107056] | Test Loss: [3.8302738666534424, 0.28273501992225647, 3.5475387573242188]\n",
            "346: Train Loss: [2.1598992347717285, 0.2816837728023529, 1.8782155513763428] | Test Loss: [3.8094773292541504, 0.2764573097229004, 3.53302001953125]\n",
            "347: Train Loss: [2.409524917602539, 0.37358149886131287, 2.0359435081481934] | Test Loss: [3.79459285736084, 0.2785162627696991, 3.5160765647888184]\n",
            "348: Train Loss: [2.6045830249786377, 0.30123233795166016, 2.3033506870269775] | Test Loss: [3.784043788909912, 0.28044000267982483, 3.50360369682312]\n",
            "349: Train Loss: [2.2890594005584717, 0.31917914748191833, 1.9698803424835205] | Test Loss: [3.7760508060455322, 0.28455185890197754, 3.4914989471435547]\n",
            "350: Train Loss: [1.1938377618789673, 0.3442394733428955, 0.8495982885360718] | Test Loss: [3.773592233657837, 0.2929568588733673, 3.480635404586792]\n",
            "351: Train Loss: [2.4845988750457764, 0.32396477460861206, 2.1606340408325195] | Test Loss: [3.7768430709838867, 0.29931244254112244, 3.4775307178497314]\n",
            "352: Train Loss: [2.199638605117798, 0.27787813544273376, 1.9217604398727417] | Test Loss: [3.782949447631836, 0.30296215415000916, 3.479987382888794]\n",
            "353: Train Loss: [2.2002298831939697, 0.30189141631126404, 1.8983381986618042] | Test Loss: [3.7832443714141846, 0.30167892575263977, 3.481565475463867]\n",
            "354: Train Loss: [2.230079412460327, 0.22606082260608673, 2.004018545150757] | Test Loss: [3.7758538722991943, 0.2900216579437256, 3.4858322143554688]\n",
            "355: Train Loss: [2.526780128479004, 0.3302209675312042, 2.196559190750122] | Test Loss: [3.758162021636963, 0.27726733684539795, 3.4808948040008545]\n",
            "356: Train Loss: [2.1752917766571045, 0.28024330735206604, 1.8950484991073608] | Test Loss: [3.737752914428711, 0.2664722204208374, 3.471280574798584]\n",
            "357: Train Loss: [2.2645633220672607, 0.32542601227760315, 1.9391372203826904] | Test Loss: [3.7221169471740723, 0.25960859656333923, 3.4625084400177]\n",
            "358: Train Loss: [2.194268226623535, 0.30121228098869324, 1.8930559158325195] | Test Loss: [3.7091352939605713, 0.2534671127796173, 3.4556682109832764]\n",
            "359: Train Loss: [2.294029712677002, 0.3397939205169678, 1.9542357921600342] | Test Loss: [3.7056446075439453, 0.2538532018661499, 3.451791286468506]\n",
            "360: Train Loss: [2.8206632137298584, 0.3792765438556671, 2.4413869380950928] | Test Loss: [3.7208664417266846, 0.2621394991874695, 3.4587268829345703]\n",
            "361: Train Loss: [2.5629281997680664, 0.3374653160572052, 2.2254629135131836] | Test Loss: [3.741467237472534, 0.2714138925075531, 3.4700534343719482]\n",
            "362: Train Loss: [2.447274684906006, 0.3147144019603729, 2.1325602531433105] | Test Loss: [3.753261089324951, 0.2786155045032501, 3.4746456146240234]\n",
            "363: Train Loss: [2.491572380065918, 0.3351994752883911, 2.1563730239868164] | Test Loss: [3.7668309211730957, 0.28240326046943665, 3.4844276905059814]\n",
            "364: Train Loss: [2.366128921508789, 0.3367520868778229, 2.029376745223999] | Test Loss: [3.7731640338897705, 0.2815806269645691, 3.4915833473205566]\n",
            "365: Train Loss: [2.419431686401367, 0.3174020051956177, 2.102029800415039] | Test Loss: [3.76347017288208, 0.2738988399505615, 3.4895713329315186]\n",
            "366: Train Loss: [2.17193865776062, 0.3226543366909027, 1.849284291267395] | Test Loss: [3.747352123260498, 0.2634440064430237, 3.483908176422119]\n",
            "367: Train Loss: [2.5387730598449707, 0.30186405777931213, 2.2369089126586914] | Test Loss: [3.727736473083496, 0.2541601359844208, 3.473576307296753]\n",
            "368: Train Loss: [2.7071750164031982, 0.27834030985832214, 2.4288346767425537] | Test Loss: [3.699310541152954, 0.24311994016170502, 3.456190586090088]\n",
            "369: Train Loss: [2.177232027053833, 0.31173962354660034, 1.865492343902588] | Test Loss: [3.6776697635650635, 0.23430213332176208, 3.4433677196502686]\n",
            "370: Train Loss: [2.4217875003814697, 0.26517486572265625, 2.1566126346588135] | Test Loss: [3.6555283069610596, 0.22535893321037292, 3.4301693439483643]\n",
            "371: Train Loss: [2.4343068599700928, 0.29554957151412964, 2.1387572288513184] | Test Loss: [3.6407036781311035, 0.21839800477027893, 3.4223055839538574]\n",
            "372: Train Loss: [2.40975022315979, 0.2736048400402069, 2.1361453533172607] | Test Loss: [3.6246650218963623, 0.2129625529050827, 3.4117023944854736]\n",
            "373: Train Loss: [2.191659688949585, 0.2771308422088623, 1.9145288467407227] | Test Loss: [3.608250617980957, 0.20813018083572388, 3.400120496749878]\n",
            "374: Train Loss: [1.26475989818573, 0.3130534887313843, 0.9517064094543457] | Test Loss: [3.602238893508911, 0.20624031126499176, 3.395998477935791]\n",
            "375: Train Loss: [2.3466968536376953, 0.33076751232147217, 2.0159292221069336] | Test Loss: [3.5999248027801514, 0.20763149857521057, 3.3922932147979736]\n",
            "376: Train Loss: [2.0393643379211426, 0.32895272970199585, 1.7104116678237915] | Test Loss: [3.6008858680725098, 0.21106313169002533, 3.3898227214813232]\n",
            "377: Train Loss: [2.164802312850952, 0.3690926134586334, 1.7957097291946411] | Test Loss: [3.6103451251983643, 0.22100712358951569, 3.3893380165100098]\n",
            "378: Train Loss: [2.190699577331543, 0.33568060398101807, 1.8550188541412354] | Test Loss: [3.6265475749969482, 0.23421815037727356, 3.392329454421997]\n",
            "379: Train Loss: [2.177457571029663, 0.33751317858695984, 1.8399444818496704] | Test Loss: [3.649543285369873, 0.2493072748184204, 3.400236129760742]\n",
            "380: Train Loss: [2.0668087005615234, 0.2755662798881531, 1.7912424802780151] | Test Loss: [3.670201063156128, 0.2607032358646393, 3.4094977378845215]\n",
            "381: Train Loss: [2.7046332359313965, 0.3011079728603363, 2.4035253524780273] | Test Loss: [3.689876079559326, 0.26804065704345703, 3.421835422515869]\n",
            "382: Train Loss: [2.5349466800689697, 0.25584298372268677, 2.2791035175323486] | Test Loss: [3.7064950466156006, 0.26554474234580994, 3.440950393676758]\n",
            "383: Train Loss: [1.2497491836547852, 0.2749883234500885, 0.974760890007019] | Test Loss: [3.721616744995117, 0.2576165795326233, 3.4640002250671387]\n",
            "384: Train Loss: [2.1884214878082275, 0.29804614186286926, 1.8903753757476807] | Test Loss: [3.7330939769744873, 0.24951674044132233, 3.483577251434326]\n",
            "385: Train Loss: [2.349411964416504, 0.4167477488517761, 1.9326642751693726] | Test Loss: [3.755513906478882, 0.2495756596326828, 3.5059382915496826]\n",
            "386: Train Loss: [2.6865200996398926, 0.29640519618988037, 2.3901147842407227] | Test Loss: [3.763132333755493, 0.2506117522716522, 3.5125205516815186]\n",
            "387: Train Loss: [2.2553458213806152, 0.2875910699367523, 1.96775484085083] | Test Loss: [3.7639989852905273, 0.2519291937351227, 3.5120697021484375]\n",
            "388: Train Loss: [2.477635383605957, 0.37024539709091187, 2.1073899269104004] | Test Loss: [3.767988443374634, 0.2564527988433838, 3.51153564453125]\n",
            "389: Train Loss: [2.029709577560425, 0.33461794257164, 1.6950916051864624] | Test Loss: [3.7774577140808105, 0.2642272412776947, 3.513230562210083]\n",
            "390: Train Loss: [2.461021661758423, 0.2454608529806137, 2.2155609130859375] | Test Loss: [3.7749595642089844, 0.2690243721008301, 3.5059351921081543]\n",
            "391: Train Loss: [2.3304102420806885, 0.30940791964530945, 2.0210022926330566] | Test Loss: [3.774027109146118, 0.27318456768989563, 3.500842571258545]\n",
            "392: Train Loss: [2.3630757331848145, 0.32349371910095215, 2.0395820140838623] | Test Loss: [3.780301570892334, 0.27887865900993347, 3.501422882080078]\n",
            "393: Train Loss: [2.375708818435669, 0.272750586271286, 2.1029582023620605] | Test Loss: [3.776918888092041, 0.28063899278640747, 3.4962799549102783]\n",
            "394: Train Loss: [2.512126922607422, 0.2713688611984253, 2.240758180618286] | Test Loss: [3.7676186561584473, 0.279258131980896, 3.488360643386841]\n",
            "395: Train Loss: [2.3015687465667725, 0.28923773765563965, 2.012331008911133] | Test Loss: [3.75820255279541, 0.27591925859451294, 3.482283353805542]\n",
            "396: Train Loss: [2.5757198333740234, 0.299258828163147, 2.276461124420166] | Test Loss: [3.7547895908355713, 0.27388063073158264, 3.4809088706970215]\n",
            "397: Train Loss: [2.3567659854888916, 0.42483994364738464, 1.9319260120391846] | Test Loss: [3.7644762992858887, 0.2801871597766876, 3.4842891693115234]\n",
            "398: Train Loss: [2.4756827354431152, 0.2865819036960602, 2.189100742340088] | Test Loss: [3.777968645095825, 0.2817439138889313, 3.4962246417999268]\n",
            "399: Train Loss: [2.148911237716675, 0.29009774327278137, 1.8588135242462158] | Test Loss: [3.7886886596679688, 0.2802606225013733, 3.5084280967712402]\n",
            "400: Train Loss: [2.106656551361084, 0.2813264727592468, 1.825330138206482] | Test Loss: [3.789815664291382, 0.274624764919281, 3.515190839767456]\n",
            "401: Train Loss: [2.746832847595215, 0.2744549512863159, 2.4723780155181885] | Test Loss: [3.791947841644287, 0.26801103353500366, 3.5239367485046387]\n",
            "402: Train Loss: [2.231503486633301, 0.31169554591178894, 1.919808030128479] | Test Loss: [3.7921650409698486, 0.2624007761478424, 3.529764175415039]\n",
            "403: Train Loss: [2.607774257659912, 0.2943190932273865, 2.313455104827881] | Test Loss: [3.794977903366089, 0.25717735290527344, 3.5378005504608154]\n",
            "404: Train Loss: [1.9443655014038086, 0.30897557735443115, 1.6353899240493774] | Test Loss: [3.7980544567108154, 0.2512374818325043, 3.5468170642852783]\n",
            "405: Train Loss: [1.8842380046844482, 0.31447485089302063, 1.56976318359375] | Test Loss: [3.8032028675079346, 0.2482934445142746, 3.5549094676971436]\n",
            "406: Train Loss: [2.094931125640869, 0.3046190142631531, 1.7903120517730713] | Test Loss: [3.7931747436523438, 0.24583294987678528, 3.547341823577881]\n",
            "407: Train Loss: [2.138279676437378, 0.37205809354782104, 1.766221523284912] | Test Loss: [3.7871785163879395, 0.2451620101928711, 3.5420165061950684]\n",
            "408: Train Loss: [2.5641469955444336, 0.38331085443496704, 2.1808362007141113] | Test Loss: [3.7847537994384766, 0.24947036802768707, 3.535283327102661]\n",
            "409: Train Loss: [2.421802520751953, 0.36858105659484863, 2.0532214641571045] | Test Loss: [3.792574882507324, 0.2600214183330536, 3.5325534343719482]\n",
            "410: Train Loss: [1.1345038414001465, 0.292044460773468, 0.8424594402313232] | Test Loss: [3.7983319759368896, 0.2679569125175476, 3.5303750038146973]\n",
            "411: Train Loss: [2.334120750427246, 0.298672080039978, 2.0354485511779785] | Test Loss: [3.790379762649536, 0.27031591534614563, 3.520063877105713]\n",
            "412: Train Loss: [2.4880237579345703, 0.29280340671539307, 2.195220470428467] | Test Loss: [3.7652463912963867, 0.26525595784187317, 3.499990463256836]\n",
            "413: Train Loss: [2.544766664505005, 0.3303326964378357, 2.2144339084625244] | Test Loss: [3.7290563583374023, 0.2589673697948456, 3.4700889587402344]\n",
            "414: Train Loss: [1.632114291191101, 0.27421748638153076, 1.3578968048095703] | Test Loss: [3.6917009353637695, 0.24908672273159027, 3.4426143169403076]\n",
            "415: Train Loss: [2.2355663776397705, 0.27744895219802856, 1.9581174850463867] | Test Loss: [3.6528046131134033, 0.2376052588224411, 3.4151992797851562]\n",
            "416: Train Loss: [2.425591468811035, 0.33546245098114014, 2.0901291370391846] | Test Loss: [3.6273422241210938, 0.22988064587116241, 3.3974616527557373]\n",
            "417: Train Loss: [2.3386759757995605, 0.2925621271133423, 2.0461137294769287] | Test Loss: [3.5957815647125244, 0.2226984202861786, 3.3730831146240234]\n",
            "418: Train Loss: [2.0458567142486572, 0.2675323784351349, 1.7783243656158447] | Test Loss: [3.572709560394287, 0.21703945100307465, 3.355670213699341]\n",
            "419: Train Loss: [2.534480571746826, 0.2573179006576538, 2.277162551879883] | Test Loss: [3.562819480895996, 0.21380195021629333, 3.34901762008667]\n",
            "420: Train Loss: [2.368495464324951, 0.32384681701660156, 2.0446486473083496] | Test Loss: [3.5608978271484375, 0.21294966340065002, 3.3479480743408203]\n",
            "421: Train Loss: [2.5260939598083496, 0.28225624561309814, 2.243837833404541] | Test Loss: [3.567427635192871, 0.21263526380062103, 3.354792356491089]\n",
            "422: Train Loss: [2.188932180404663, 0.22318461537361145, 1.965747594833374] | Test Loss: [3.579300880432129, 0.21275083720684052, 3.3665499687194824]\n",
            "423: Train Loss: [2.0722570419311523, 0.4112592041492462, 1.660997748374939] | Test Loss: [3.5956571102142334, 0.21756848692893982, 3.3780887126922607]\n",
            "424: Train Loss: [2.122530698776245, 0.3295312523841858, 1.7929993867874146] | Test Loss: [3.608339786529541, 0.22255493700504303, 3.385784864425659]\n",
            "425: Train Loss: [2.12072491645813, 0.26333877444267273, 1.8573861122131348] | Test Loss: [3.6188554763793945, 0.22686247527599335, 3.3919930458068848]\n",
            "426: Train Loss: [2.4415221214294434, 0.3250097930431366, 2.1165122985839844] | Test Loss: [3.6193652153015137, 0.2327614575624466, 3.386603832244873]\n",
            "427: Train Loss: [2.756147623062134, 0.4015674889087677, 2.3545801639556885] | Test Loss: [3.629082679748535, 0.24307528138160706, 3.386007308959961]\n",
            "428: Train Loss: [2.5427634716033936, 0.28121572732925415, 2.261547803878784] | Test Loss: [3.6255943775177, 0.24820327758789062, 3.3773910999298096]\n",
            "429: Train Loss: [1.4134992361068726, 0.3382473289966583, 1.0752519369125366] | Test Loss: [3.6219565868377686, 0.2506764829158783, 3.3712801933288574]\n",
            "430: Train Loss: [2.4873507022857666, 0.3337346017360687, 2.153616189956665] | Test Loss: [3.6138088703155518, 0.251115620136261, 3.3626933097839355]\n",
            "431: Train Loss: [1.9791885614395142, 0.3036016523838043, 1.6755869388580322] | Test Loss: [3.604987382888794, 0.24710911512374878, 3.3578782081604004]\n",
            "432: Train Loss: [2.442230701446533, 0.3342756927013397, 2.107954978942871] | Test Loss: [3.5973761081695557, 0.24192887544631958, 3.355447292327881]\n",
            "433: Train Loss: [1.690527319908142, 0.30079731345176697, 1.3897299766540527] | Test Loss: [3.5900566577911377, 0.23550735414028168, 3.3545494079589844]\n",
            "434: Train Loss: [2.3380565643310547, 0.33339986205101013, 2.0046567916870117] | Test Loss: [3.580170154571533, 0.23050165176391602, 3.349668502807617]\n",
            "435: Train Loss: [2.3699307441711426, 0.3591916859149933, 2.0107390880584717] | Test Loss: [3.579371213912964, 0.22939994931221008, 3.349971294403076]\n",
            "436: Train Loss: [1.942328691482544, 0.2807067930698395, 1.6616219282150269] | Test Loss: [3.581080675125122, 0.228251114487648, 3.3528294563293457]\n",
            "437: Train Loss: [1.9753490686416626, 0.39825329184532166, 1.5770957469940186] | Test Loss: [3.590589761734009, 0.230514258146286, 3.3600754737854004]\n",
            "438: Train Loss: [1.5936737060546875, 0.2743641436100006, 1.3193095922470093] | Test Loss: [3.6007261276245117, 0.23258259892463684, 3.3681435585021973]\n",
            "439: Train Loss: [2.853701114654541, 0.35741639137268066, 2.4962847232818604] | Test Loss: [3.612610101699829, 0.23610396683216095, 3.3765060901641846]\n",
            "440: Train Loss: [1.1014676094055176, 0.37980419397354126, 0.7216633558273315] | Test Loss: [3.627021074295044, 0.24170736968517303, 3.3853137493133545]\n",
            "441: Train Loss: [2.2979249954223633, 0.28731274604797363, 2.0106122493743896] | Test Loss: [3.632004737854004, 0.2462904304265976, 3.385714292526245]\n",
            "442: Train Loss: [2.416325092315674, 0.3178888261318207, 2.0984363555908203] | Test Loss: [3.632214069366455, 0.24918867647647858, 3.3830254077911377]\n",
            "443: Train Loss: [2.3333895206451416, 0.2762860655784607, 2.057103395462036] | Test Loss: [3.6172614097595215, 0.24754011631011963, 3.3697211742401123]\n",
            "444: Train Loss: [2.1814613342285156, 0.2791394293308258, 1.9023219347000122] | Test Loss: [3.6001925468444824, 0.24321626126766205, 3.356976270675659]\n",
            "445: Train Loss: [2.493546485900879, 0.2887238562107086, 2.204822540283203] | Test Loss: [3.5867323875427246, 0.23790547251701355, 3.3488268852233887]\n",
            "446: Train Loss: [2.406029224395752, 0.288578063249588, 2.1174511909484863] | Test Loss: [3.57905650138855, 0.2338741421699524, 3.345182418823242]\n",
            "447: Train Loss: [2.1625523567199707, 0.34342336654663086, 1.8191288709640503] | Test Loss: [3.5716323852539062, 0.2313964068889618, 3.340235948562622]\n",
            "448: Train Loss: [2.3210487365722656, 0.39245736598968506, 1.928591251373291] | Test Loss: [3.5732898712158203, 0.23084402084350586, 3.3424458503723145]\n",
            "449: Train Loss: [2.590219020843506, 0.3058260679244995, 2.284392833709717] | Test Loss: [3.577833890914917, 0.23073071241378784, 3.3471031188964844]\n",
            "450: Train Loss: [2.1545608043670654, 0.3003881275653839, 1.8541725873947144] | Test Loss: [3.58640718460083, 0.23129455745220184, 3.3551125526428223]\n",
            "451: Train Loss: [1.9782698154449463, 0.30934396386146545, 1.6689258813858032] | Test Loss: [3.596296548843384, 0.2318461686372757, 3.364450454711914]\n",
            "452: Train Loss: [2.342813014984131, 0.2898162603378296, 2.0529966354370117] | Test Loss: [3.604918956756592, 0.23329058289527893, 3.3716282844543457]\n",
            "453: Train Loss: [2.1048598289489746, 0.2593870460987091, 1.845472812652588] | Test Loss: [3.6114978790283203, 0.2323375940322876, 3.3791604042053223]\n",
            "454: Train Loss: [2.0603809356689453, 0.2615574300289154, 1.7988234758377075] | Test Loss: [3.614041328430176, 0.23083917796611786, 3.383202075958252]\n",
            "455: Train Loss: [2.4277844429016113, 0.3391348123550415, 2.0886497497558594] | Test Loss: [3.62178373336792, 0.23018096387386322, 3.3916027545928955]\n",
            "456: Train Loss: [2.01149320602417, 0.26412197947502136, 1.7473711967468262] | Test Loss: [3.633889675140381, 0.22994525730609894, 3.403944492340088]\n",
            "457: Train Loss: [2.369793176651001, 0.3273586630821228, 2.0424344539642334] | Test Loss: [3.6393022537231445, 0.23107390105724335, 3.4082283973693848]\n",
            "458: Train Loss: [1.8309457302093506, 0.2953813374042511, 1.5355644226074219] | Test Loss: [3.6398961544036865, 0.23313622176647186, 3.4067599773406982]\n",
            "459: Train Loss: [2.577031135559082, 0.3604236841201782, 2.2166073322296143] | Test Loss: [3.6511733531951904, 0.23758502304553986, 3.413588285446167]\n",
            "460: Train Loss: [2.110466957092285, 0.37459099292755127, 1.7358758449554443] | Test Loss: [3.661817789077759, 0.24515680968761444, 3.416661024093628]\n",
            "461: Train Loss: [1.9160796403884888, 0.27014729380607605, 1.6459323167800903] | Test Loss: [3.6712241172790527, 0.2490726262331009, 3.422151565551758]\n",
            "462: Train Loss: [2.0925254821777344, 0.32727259397506714, 1.765252947807312] | Test Loss: [3.679250955581665, 0.25069060921669006, 3.428560256958008]\n",
            "463: Train Loss: [2.3210620880126953, 0.4021327793598175, 1.9189292192459106] | Test Loss: [3.694257974624634, 0.25537750124931335, 3.438880443572998]\n",
            "464: Train Loss: [1.5823006629943848, 0.3204527497291565, 1.261847972869873] | Test Loss: [3.700160264968872, 0.25408148765563965, 3.4460787773132324]\n",
            "465: Train Loss: [2.263756513595581, 0.29134324193000793, 1.972413182258606] | Test Loss: [3.6968164443969727, 0.2445312887430191, 3.452285051345825]\n",
            "466: Train Loss: [2.346534252166748, 0.3306923806667328, 2.0158419609069824] | Test Loss: [3.691903829574585, 0.23644667863845825, 3.4554572105407715]\n",
            "467: Train Loss: [2.269589424133301, 0.3074471950531006, 1.9621423482894897] | Test Loss: [3.688140392303467, 0.22702541947364807, 3.4611148834228516]\n",
            "468: Train Loss: [2.3835747241973877, 0.3351631760597229, 2.0484116077423096] | Test Loss: [3.689997673034668, 0.2230939418077469, 3.4669036865234375]\n",
            "469: Train Loss: [2.313323736190796, 0.2223588526248932, 2.0909647941589355] | Test Loss: [3.691171169281006, 0.21681083738803864, 3.474360227584839]\n",
            "470: Train Loss: [2.1558330059051514, 0.2580912411212921, 1.8977417945861816] | Test Loss: [3.6783061027526855, 0.21175484359264374, 3.4665513038635254]\n",
            "471: Train Loss: [2.662795066833496, 0.29282310605049133, 2.369971990585327] | Test Loss: [3.6654109954833984, 0.20846037566661835, 3.4569506645202637]\n",
            "472: Train Loss: [2.3185653686523438, 0.3060591220855713, 2.0125062465667725] | Test Loss: [3.6513431072235107, 0.20728176832199097, 3.444061279296875]\n",
            "473: Train Loss: [2.192728042602539, 0.3229330778121948, 1.8697948455810547] | Test Loss: [3.6467976570129395, 0.209178626537323, 3.4376189708709717]\n",
            "474: Train Loss: [2.4912173748016357, 0.26598119735717773, 2.225236177444458] | Test Loss: [3.641547918319702, 0.2127869725227356, 3.4287610054016113]\n",
            "475: Train Loss: [1.9664593935012817, 0.3286339342594147, 1.6378254890441895] | Test Loss: [3.6422510147094727, 0.22112247347831726, 3.421128511428833]\n",
            "476: Train Loss: [2.4141266345977783, 0.3815939426422119, 2.0325326919555664] | Test Loss: [3.6445679664611816, 0.23471438884735107, 3.40985369682312]\n",
            "477: Train Loss: [2.07403564453125, 0.28421664237976074, 1.7898191213607788] | Test Loss: [3.6408779621124268, 0.24587801098823547, 3.3949999809265137]\n",
            "478: Train Loss: [1.824394941329956, 0.2925879955291748, 1.5318070650100708] | Test Loss: [3.634654998779297, 0.25356554985046387, 3.381089448928833]\n",
            "479: Train Loss: [2.309368133544922, 0.36031070351600647, 1.9490575790405273] | Test Loss: [3.634096145629883, 0.2625461220741272, 3.3715500831604004]\n",
            "480: Train Loss: [2.2163591384887695, 0.38511398434638977, 1.8312450647354126] | Test Loss: [3.6440839767456055, 0.2752942740917206, 3.3687896728515625]\n",
            "481: Train Loss: [2.2551488876342773, 0.3261730372905731, 1.9289758205413818] | Test Loss: [3.650974750518799, 0.2833518087863922, 3.3676228523254395]\n",
            "482: Train Loss: [2.1281771659851074, 0.2993178963661194, 1.8288593292236328] | Test Loss: [3.64740252494812, 0.28178244829177856, 3.3656201362609863]\n",
            "483: Train Loss: [2.00026273727417, 0.2943924069404602, 1.705870270729065] | Test Loss: [3.633610248565674, 0.27044352889060974, 3.3631668090820312]\n",
            "484: Train Loss: [1.9751124382019043, 0.31265172362327576, 1.6624606847763062] | Test Loss: [3.618101119995117, 0.25790339708328247, 3.3601977825164795]\n",
            "485: Train Loss: [2.3271560668945312, 0.31135717034339905, 2.015798807144165] | Test Loss: [3.5976953506469727, 0.2481706440448761, 3.349524736404419]\n",
            "486: Train Loss: [2.3400919437408447, 0.23726505041122437, 2.1028268337249756] | Test Loss: [3.580742120742798, 0.23851419985294342, 3.3422279357910156]\n",
            "487: Train Loss: [2.3961293697357178, 0.3348544239997864, 2.061275005340576] | Test Loss: [3.573979377746582, 0.23437654972076416, 3.3396027088165283]\n",
            "488: Train Loss: [2.0116524696350098, 0.24808837473392487, 1.763564109802246] | Test Loss: [3.573025703430176, 0.2299560308456421, 3.343069553375244]\n",
            "489: Train Loss: [2.305582046508789, 0.23430302739143372, 2.0712790489196777] | Test Loss: [3.578254461288452, 0.22586281597614288, 3.3523917198181152]\n",
            "490: Train Loss: [1.7854596376419067, 0.2975986897945404, 1.487860918045044] | Test Loss: [3.5862221717834473, 0.22405819594860077, 3.36216402053833]\n",
            "491: Train Loss: [2.4490063190460205, 0.3127140998840332, 2.1362922191619873] | Test Loss: [3.5920567512512207, 0.22504928708076477, 3.3670074939727783]\n",
            "492: Train Loss: [2.194027900695801, 0.31069672107696533, 1.883331060409546] | Test Loss: [3.6024131774902344, 0.22820322215557098, 3.3742098808288574]\n",
            "493: Train Loss: [1.9223241806030273, 0.32477086782455444, 1.5975532531738281] | Test Loss: [3.618532657623291, 0.2342061549425125, 3.384326457977295]\n",
            "494: Train Loss: [2.2382402420043945, 0.30702653527259827, 1.931213617324829] | Test Loss: [3.6295359134674072, 0.24149851500988007, 3.3880374431610107]\n",
            "495: Train Loss: [2.2061448097229004, 0.31610915064811707, 1.890035629272461] | Test Loss: [3.6378731727600098, 0.2511070966720581, 3.386765956878662]\n",
            "496: Train Loss: [2.1925246715545654, 0.3075568974018097, 1.8849676847457886] | Test Loss: [3.645075559616089, 0.259753555059433, 3.385322093963623]\n",
            "497: Train Loss: [2.2703778743743896, 0.3025731146335602, 1.9678047895431519] | Test Loss: [3.6524996757507324, 0.26649799942970276, 3.3860015869140625]\n",
            "498: Train Loss: [2.4887797832489014, 0.3182068467140198, 2.1705729961395264] | Test Loss: [3.657924175262451, 0.27200084924697876, 3.385923385620117]\n",
            "499: Train Loss: [2.0552563667297363, 0.28524935245513916, 1.7700068950653076] | Test Loss: [3.6612420082092285, 0.275340735912323, 3.3859012126922607]\n",
            "500: Train Loss: [2.1700613498687744, 0.3297731876373291, 1.8402881622314453] | Test Loss: [3.6587905883789062, 0.2759799361228943, 3.382810592651367]\n",
            "501: Train Loss: [2.210886001586914, 0.3162378668785095, 1.8946481943130493] | Test Loss: [3.657914638519287, 0.2734377682209015, 3.384476900100708]\n",
            "502: Train Loss: [1.9762295484542847, 0.30091121792793274, 1.6753184795379639] | Test Loss: [3.6570472717285156, 0.26668429374694824, 3.3903629779815674]\n",
            "503: Train Loss: [2.0078980922698975, 0.3211391568183899, 1.6867588758468628] | Test Loss: [3.65769624710083, 0.2600017786026001, 3.3976943492889404]\n",
            "504: Train Loss: [2.1836953163146973, 0.304984986782074, 1.878710389137268] | Test Loss: [3.6601486206054688, 0.25223875045776367, 3.407909870147705]\n",
            "505: Train Loss: [2.085242509841919, 0.36343225836753845, 1.7218103408813477] | Test Loss: [3.6608400344848633, 0.24583573639392853, 3.415004253387451]\n",
            "506: Train Loss: [2.360255002975464, 0.3121304512023926, 2.0481245517730713] | Test Loss: [3.663011074066162, 0.24036411941051483, 3.422646999359131]\n",
            "507: Train Loss: [2.2360756397247314, 0.25020188093185425, 1.9858736991882324] | Test Loss: [3.6625635623931885, 0.23347316682338715, 3.4290904998779297]\n",
            "508: Train Loss: [1.9975594282150269, 0.2836127281188965, 1.71394681930542] | Test Loss: [3.6629652976989746, 0.2282462865114212, 3.4347190856933594]\n",
            "509: Train Loss: [1.280677080154419, 0.34372660517692566, 0.9369503855705261] | Test Loss: [3.6614394187927246, 0.22789262235164642, 3.433546781539917]\n",
            "510: Train Loss: [2.081235885620117, 0.33498844504356384, 1.7462475299835205] | Test Loss: [3.6614227294921875, 0.22924159467220306, 3.4321811199188232]\n",
            "511: Train Loss: [2.1666171550750732, 0.31932705640792847, 1.8472900390625] | Test Loss: [3.658691883087158, 0.2331429421901703, 3.425549030303955]\n",
            "512: Train Loss: [2.411993980407715, 0.27420446276664734, 2.137789487838745] | Test Loss: [3.656020402908325, 0.23620206117630005, 3.41981840133667]\n",
            "513: Train Loss: [2.2927417755126953, 0.31446993350982666, 1.978271722793579] | Test Loss: [3.6536459922790527, 0.23998142778873444, 3.4136645793914795]\n",
            "514: Train Loss: [2.258753776550293, 0.27198272943496704, 1.9867711067199707] | Test Loss: [3.652376413345337, 0.24132905900478363, 3.4110474586486816]\n",
            "515: Train Loss: [2.0927164554595947, 0.27995172142982483, 1.8127647638320923] | Test Loss: [3.6568214893341064, 0.2428058683872223, 3.414015531539917]\n",
            "516: Train Loss: [2.178419589996338, 0.31665414571762085, 1.8617653846740723] | Test Loss: [3.669619560241699, 0.24714067578315735, 3.4224789142608643]\n",
            "517: Train Loss: [2.1416711807250977, 0.4478597342967987, 1.6938114166259766] | Test Loss: [3.6870713233947754, 0.258264422416687, 3.428806781768799]\n",
            "518: Train Loss: [2.40681529045105, 0.2868139147758484, 2.1200013160705566] | Test Loss: [3.7035140991210938, 0.26659131050109863, 3.436922788619995]\n",
            "519: Train Loss: [2.082650899887085, 0.3499867618083954, 1.7326641082763672] | Test Loss: [3.705108642578125, 0.2723968029022217, 3.4327118396759033]\n",
            "520: Train Loss: [1.5169439315795898, 0.2960076928138733, 1.2209362983703613] | Test Loss: [3.702357530593872, 0.2718549370765686, 3.4305026531219482]\n",
            "521: Train Loss: [2.405524253845215, 0.2851310074329376, 2.1203932762145996] | Test Loss: [3.6956534385681152, 0.26441067457199097, 3.4312427043914795]\n",
            "522: Train Loss: [2.195718765258789, 0.27567869424819946, 1.9200401306152344] | Test Loss: [3.6869235038757324, 0.25391802191734314, 3.4330055713653564]\n",
            "523: Train Loss: [2.405913829803467, 0.2842491865158081, 2.1216647624969482] | Test Loss: [3.6734464168548584, 0.2429729849100113, 3.4304733276367188]\n",
            "524: Train Loss: [2.344231605529785, 0.2751523554325104, 2.0690791606903076] | Test Loss: [3.6656548976898193, 0.23278336226940155, 3.4328715801239014]\n",
            "525: Train Loss: [2.537550449371338, 0.26305654644966125, 2.274493932723999] | Test Loss: [3.6659493446350098, 0.22527290880680084, 3.44067645072937]\n",
            "Epoch 2\n",
            "0: Train Loss: [2.2986366748809814, 0.3371334373950958, 1.961503267288208] | Test Loss: [3.6710751056671143, 0.22284385561943054, 3.4482312202453613]\n",
            "1: Train Loss: [2.3692831993103027, 0.3111332654953003, 2.058149814605713] | Test Loss: [3.677727460861206, 0.22329933941364288, 3.454428195953369]\n",
            "2: Train Loss: [2.2649059295654297, 0.30408990383148193, 1.9608161449432373] | Test Loss: [3.6904795169830322, 0.2255466729402542, 3.464932918548584]\n",
            "3: Train Loss: [2.479092597961426, 0.2639213800430298, 2.2151713371276855] | Test Loss: [3.7003021240234375, 0.22916875779628754, 3.4711334705352783]\n",
            "4: Train Loss: [1.9820404052734375, 0.3857254385948181, 1.5963149070739746] | Test Loss: [3.7127914428710938, 0.23724430799484253, 3.4755470752716064]\n",
            "5: Train Loss: [1.1238045692443848, 0.33507299423217773, 0.788731575012207] | Test Loss: [3.731516122817993, 0.24855253100395203, 3.4829635620117188]\n",
            "6: Train Loss: [1.801880121231079, 0.32793328166007996, 1.4739468097686768] | Test Loss: [3.7510082721710205, 0.26398441195487976, 3.4870238304138184]\n",
            "7: Train Loss: [2.1948347091674805, 0.23588943481445312, 1.9589452743530273] | Test Loss: [3.7640292644500732, 0.27308982610702515, 3.4909393787384033]\n",
            "8: Train Loss: [2.419008731842041, 0.32615140080451965, 2.0928573608398438] | Test Loss: [3.767413854598999, 0.2791174352169037, 3.4882965087890625]\n",
            "9: Train Loss: [2.0796501636505127, 0.28851020336151123, 1.7911399602890015] | Test Loss: [3.765828847885132, 0.2780250608921051, 3.4878036975860596]\n",
            "10: Train Loss: [2.2064878940582275, 0.33626502752304077, 1.870222806930542] | Test Loss: [3.752589225769043, 0.2751263678073883, 3.4774627685546875]\n",
            "11: Train Loss: [1.6929643154144287, 0.31752821803092957, 1.3754360675811768] | Test Loss: [3.7373571395874023, 0.27040421962738037, 3.4669528007507324]\n",
            "12: Train Loss: [2.229429006576538, 0.28979820013046265, 1.9396307468414307] | Test Loss: [3.7216134071350098, 0.26211774349212646, 3.4594955444335938]\n",
            "13: Train Loss: [2.029550075531006, 0.3074972927570343, 1.722052812576294] | Test Loss: [3.705824136734009, 0.2528302073478699, 3.452993869781494]\n",
            "14: Train Loss: [2.062418222427368, 0.2578539550304413, 1.8045642375946045] | Test Loss: [3.6891963481903076, 0.24349525570869446, 3.4457011222839355]\n",
            "15: Train Loss: [2.288548231124878, 0.35291847586631775, 1.9356297254562378] | Test Loss: [3.6817169189453125, 0.2397429645061493, 3.441973924636841]\n",
            "16: Train Loss: [2.309983253479004, 0.28157609701156616, 2.028407096862793] | Test Loss: [3.6692187786102295, 0.23478630185127258, 3.4344325065612793]\n",
            "17: Train Loss: [2.2888729572296143, 0.28760725259780884, 2.00126576423645] | Test Loss: [3.6659746170043945, 0.23129665851593018, 3.434677839279175]\n",
            "18: Train Loss: [2.5492804050445557, 0.28317397832870483, 2.266106367111206] | Test Loss: [3.667335271835327, 0.22870488464832306, 3.4386303424835205]\n",
            "19: Train Loss: [2.2800254821777344, 0.2926923930644989, 1.9873331785202026] | Test Loss: [3.672654867172241, 0.22878099977970123, 3.443873882293701]\n",
            "20: Train Loss: [2.147010087966919, 0.3616349995136261, 1.7853749990463257] | Test Loss: [3.6841225624084473, 0.23289035260677338, 3.4512321949005127]\n",
            "21: Train Loss: [2.4295382499694824, 0.36433905363082886, 2.065199136734009] | Test Loss: [3.700995683670044, 0.24050308763980865, 3.4604926109313965]\n",
            "22: Train Loss: [2.276782989501953, 0.2769428789615631, 1.9998400211334229] | Test Loss: [3.711010694503784, 0.24671624600887299, 3.46429443359375]\n",
            "23: Train Loss: [2.2611260414123535, 0.35250115394592285, 1.9086248874664307] | Test Loss: [3.718325614929199, 0.25217205286026, 3.466153621673584]\n",
            "24: Train Loss: [2.3941731452941895, 0.3094463050365448, 2.0847268104553223] | Test Loss: [3.7184603214263916, 0.2582044005393982, 3.4602558612823486]\n",
            "25: Train Loss: [2.1935184001922607, 0.26888447999954224, 1.9246339797973633] | Test Loss: [3.713611364364624, 0.2576505243778229, 3.455960750579834]\n",
            "26: Train Loss: [1.9520301818847656, 0.2938968539237976, 1.6581333875656128] | Test Loss: [3.708974599838257, 0.258358895778656, 3.450615644454956]\n",
            "27: Train Loss: [2.55324125289917, 0.35541898012161255, 2.197822332382202] | Test Loss: [3.707566261291504, 0.25991496443748474, 3.4476513862609863]\n",
            "28: Train Loss: [2.3812553882598877, 0.2840522825717926, 2.097203016281128] | Test Loss: [3.7052974700927734, 0.2608923316001892, 3.4444050788879395]\n",
            "29: Train Loss: [2.2545318603515625, 0.27911943197250366, 1.9754124879837036] | Test Loss: [3.6965534687042236, 0.25510716438293457, 3.441446304321289]\n",
            "30: Train Loss: [2.2284984588623047, 0.3008556365966797, 1.9276427030563354] | Test Loss: [3.683616876602173, 0.24784542620182037, 3.4357714653015137]\n",
            "31: Train Loss: [2.3660595417022705, 0.2824717164039612, 2.083587884902954] | Test Loss: [3.6708102226257324, 0.24146470427513123, 3.4293456077575684]\n",
            "32: Train Loss: [1.9399380683898926, 0.2477002739906311, 1.6922377347946167] | Test Loss: [3.6623003482818604, 0.2338894158601761, 3.4284110069274902]\n",
            "33: Train Loss: [2.0581839084625244, 0.38939884305000305, 1.6687849760055542] | Test Loss: [3.66031551361084, 0.23430392146110535, 3.426011562347412]\n",
            "34: Train Loss: [2.0260391235351562, 0.3178398311138153, 1.7081992626190186] | Test Loss: [3.6681182384490967, 0.23915129899978638, 3.428966999053955]\n",
            "35: Train Loss: [1.9991254806518555, 0.26462846994400024, 1.7344969511032104] | Test Loss: [3.6755881309509277, 0.2431039810180664, 3.4324841499328613]\n",
            "36: Train Loss: [2.3742825984954834, 0.2948578894138336, 2.0794246196746826] | Test Loss: [3.687553882598877, 0.24849094450473785, 3.4390628337860107]\n",
            "37: Train Loss: [2.0854341983795166, 0.276999831199646, 1.8084343671798706] | Test Loss: [3.6953370571136475, 0.2507708966732025, 3.444566249847412]\n",
            "38: Train Loss: [2.302276849746704, 0.27971315383911133, 2.0225636959075928] | Test Loss: [3.698103666305542, 0.2507564127445221, 3.4473471641540527]\n",
            "39: Train Loss: [2.3890089988708496, 0.2551225423812866, 2.1338865756988525] | Test Loss: [3.6998445987701416, 0.24684996902942657, 3.4529945850372314]\n",
            "40: Train Loss: [1.86618971824646, 0.2667337954044342, 1.5994559526443481] | Test Loss: [3.6986331939697266, 0.24560081958770752, 3.4530322551727295]\n",
            "41: Train Loss: [2.018688440322876, 0.24715778231620789, 1.7715307474136353] | Test Loss: [3.6978015899658203, 0.24278295040130615, 3.4550185203552246]\n",
            "42: Train Loss: [2.2519490718841553, 0.24473340809345245, 2.007215738296509] | Test Loss: [3.698166608810425, 0.23966437578201294, 3.4585022926330566]\n",
            "43: Train Loss: [1.9525632858276367, 0.36270758509635925, 1.589855670928955] | Test Loss: [3.70013689994812, 0.24338093400001526, 3.4567558765411377]\n",
            "44: Train Loss: [1.7467963695526123, 0.3616732954978943, 1.3851231336593628] | Test Loss: [3.7033169269561768, 0.2516631484031677, 3.4516537189483643]\n",
            "45: Train Loss: [1.9327958822250366, 0.23750713467597961, 1.6952887773513794] | Test Loss: [3.699857473373413, 0.2581693232059479, 3.441688060760498]\n",
            "46: Train Loss: [2.2812106609344482, 0.2947876751422882, 1.9864228963851929] | Test Loss: [3.7030279636383057, 0.267121821641922, 3.435906171798706]\n",
            "47: Train Loss: [1.9105315208435059, 0.3557029664516449, 1.5548285245895386] | Test Loss: [3.7067906856536865, 0.2798301577568054, 3.4269604682922363]\n",
            "48: Train Loss: [2.408151388168335, 0.2558334171772003, 2.152318000793457] | Test Loss: [3.699605941772461, 0.28809934854507446, 3.4115066528320312]\n",
            "49: Train Loss: [2.1310436725616455, 0.2745068073272705, 1.856536865234375] | Test Loss: [3.692732095718384, 0.29207712411880493, 3.4006550312042236]\n",
            "50: Train Loss: [2.1275200843811035, 0.29239603877067566, 1.835124135017395] | Test Loss: [3.681499719619751, 0.2919328808784485, 3.3895668983459473]\n",
            "51: Train Loss: [2.070124387741089, 0.28990355134010315, 1.780220866203308] | Test Loss: [3.6630327701568604, 0.28687652945518494, 3.3761563301086426]\n",
            "52: Train Loss: [2.3735673427581787, 0.3147222697734833, 2.058845043182373] | Test Loss: [3.6363682746887207, 0.2803126871585846, 3.356055498123169]\n",
            "53: Train Loss: [2.2515687942504883, 0.2984200417995453, 1.9531487226486206] | Test Loss: [3.612941265106201, 0.2724863588809967, 3.3404548168182373]\n",
            "54: Train Loss: [2.1417670249938965, 0.40091609954833984, 1.7408509254455566] | Test Loss: [3.6040472984313965, 0.27351588010787964, 3.330531358718872]\n",
            "55: Train Loss: [2.266124963760376, 0.3137723505496979, 1.9523526430130005] | Test Loss: [3.5932705402374268, 0.27294036746025085, 3.3203301429748535]\n",
            "56: Train Loss: [2.213818073272705, 0.33225178718566895, 1.8815664052963257] | Test Loss: [3.580428123474121, 0.27226874232292175, 3.308159351348877]\n",
            "57: Train Loss: [2.3157546520233154, 0.28875789046287537, 2.0269968509674072] | Test Loss: [3.5701394081115723, 0.2694858908653259, 3.3006534576416016]\n",
            "58: Train Loss: [2.3953464031219482, 0.27004680037498474, 2.1252996921539307] | Test Loss: [3.5502219200134277, 0.26090186834335327, 3.2893199920654297]\n",
            "59: Train Loss: [2.0676751136779785, 0.28946056962013245, 1.778214454650879] | Test Loss: [3.532963752746582, 0.25093892216682434, 3.28202486038208]\n",
            "60: Train Loss: [1.9988027811050415, 0.2882039546966553, 1.7105988264083862] | Test Loss: [3.516085147857666, 0.23957988619804382, 3.2765052318573]\n",
            "61: Train Loss: [2.379647970199585, 0.354594886302948, 2.025053024291992] | Test Loss: [3.5050151348114014, 0.23584681749343872, 3.2691683769226074]\n",
            "62: Train Loss: [2.398878335952759, 0.2569649815559387, 2.141913414001465] | Test Loss: [3.4946646690368652, 0.23254387080669403, 3.2621207237243652]\n",
            "63: Train Loss: [2.2479636669158936, 0.3626466393470764, 1.8853169679641724] | Test Loss: [3.4984869956970215, 0.23883721232414246, 3.2596497535705566]\n",
            "64: Train Loss: [1.6724344491958618, 0.32768353819847107, 1.3447508811950684] | Test Loss: [3.5028908252716064, 0.2462226301431656, 3.2566680908203125]\n",
            "65: Train Loss: [1.9157277345657349, 0.3251626789569855, 1.5905650854110718] | Test Loss: [3.501742124557495, 0.25285258889198303, 3.248889446258545]\n",
            "66: Train Loss: [2.043557643890381, 0.3327706456184387, 1.710787057876587] | Test Loss: [3.5011839866638184, 0.261320024728775, 3.239863872528076]\n",
            "67: Train Loss: [2.2674765586853027, 0.32202643156051636, 1.9454501867294312] | Test Loss: [3.501108169555664, 0.26875194907188416, 3.232356309890747]\n",
            "68: Train Loss: [2.687872886657715, 0.33523029088974, 2.35264253616333] | Test Loss: [3.503753423690796, 0.27587249875068665, 3.2278809547424316]\n",
            "69: Train Loss: [2.2972164154052734, 0.33158761262893677, 1.965628743171692] | Test Loss: [3.509547710418701, 0.28327515721321106, 3.2262725830078125]\n",
            "70: Train Loss: [2.188542366027832, 0.2868916988372803, 1.9016505479812622] | Test Loss: [3.501241683959961, 0.2796870768070221, 3.2215545177459717]\n",
            "71: Train Loss: [2.2793571949005127, 0.28719592094421387, 1.9921612739562988] | Test Loss: [3.4925167560577393, 0.27363961935043335, 3.218877077102661]\n",
            "72: Train Loss: [2.2670018672943115, 0.3255699872970581, 1.9414318799972534] | Test Loss: [3.4814233779907227, 0.26347216963768005, 3.2179512977600098]\n",
            "73: Train Loss: [1.278938889503479, 0.2756456434726715, 1.0032932758331299] | Test Loss: [3.4736032485961914, 0.2548474669456482, 3.2187557220458984]\n",
            "74: Train Loss: [2.2542567253112793, 0.34292879700660706, 1.9113280773162842] | Test Loss: [3.4725308418273926, 0.2524808645248413, 3.220050096511841]\n",
            "75: Train Loss: [2.241030693054199, 0.3175898492336273, 1.923440933227539] | Test Loss: [3.4842398166656494, 0.25449082255363464, 3.2297489643096924]\n",
            "76: Train Loss: [2.1216726303100586, 0.29101553559303284, 1.8306570053100586] | Test Loss: [3.4969379901885986, 0.25648239254951477, 3.2404556274414062]\n",
            "77: Train Loss: [2.03442645072937, 0.280629426240921, 1.753796935081482] | Test Loss: [3.5076169967651367, 0.2581556737422943, 3.2494614124298096]\n",
            "78: Train Loss: [2.1661298274993896, 0.24001149833202362, 1.92611825466156] | Test Loss: [3.5080649852752686, 0.2575550675392151, 3.2505099773406982]\n",
            "79: Train Loss: [1.9767411947250366, 0.24976590275764465, 1.7269753217697144] | Test Loss: [3.505061149597168, 0.254474401473999, 3.250586748123169]\n",
            "80: Train Loss: [2.262432813644409, 0.2821744382381439, 1.9802584648132324] | Test Loss: [3.499682664871216, 0.2513778507709503, 3.248304843902588]\n",
            "81: Train Loss: [2.146944999694824, 0.26306527853012085, 1.8838797807693481] | Test Loss: [3.5015957355499268, 0.25084492564201355, 3.250750780105591]\n",
            "82: Train Loss: [2.0082671642303467, 0.24263663589954376, 1.7656304836273193] | Test Loss: [3.5065600872039795, 0.25179314613342285, 3.2547669410705566]\n",
            "83: Train Loss: [1.9798636436462402, 0.23734156787395477, 1.742522120475769] | Test Loss: [3.5135693550109863, 0.2509571611881256, 3.2626121044158936]\n",
            "84: Train Loss: [2.1607584953308105, 0.3161666691303253, 1.844591736793518] | Test Loss: [3.525639533996582, 0.2560344338417053, 3.2696051597595215]\n",
            "85: Train Loss: [2.283888578414917, 0.28152209520339966, 2.002366542816162] | Test Loss: [3.535275936126709, 0.26559901237487793, 3.269676923751831]\n",
            "86: Train Loss: [2.5331406593322754, 0.3570997714996338, 2.1760408878326416] | Test Loss: [3.5627050399780273, 0.28838711977005005, 3.274317979812622]\n",
            "87: Train Loss: [2.163849353790283, 0.30897387862205505, 1.8548755645751953] | Test Loss: [3.5929975509643555, 0.3127133250236511, 3.2802841663360596]\n",
            "88: Train Loss: [2.057006597518921, 0.27425631880760193, 1.7827502489089966] | Test Loss: [3.6159348487854004, 0.3321370482444763, 3.2837977409362793]\n",
            "89: Train Loss: [1.523040533065796, 0.3357881009578705, 1.187252402305603] | Test Loss: [3.6285886764526367, 0.345871239900589, 3.28271746635437]\n",
            "90: Train Loss: [2.277583599090576, 0.34543269872665405, 1.9321508407592773] | Test Loss: [3.626300811767578, 0.35456278920173645, 3.271738052368164]\n",
            "91: Train Loss: [1.458418607711792, 0.272716760635376, 1.185701847076416] | Test Loss: [3.6075448989868164, 0.3449868857860565, 3.2625579833984375]\n",
            "92: Train Loss: [2.480952024459839, 0.4078999161720276, 2.073052167892456] | Test Loss: [3.605973720550537, 0.33981460332870483, 3.2661590576171875]\n",
            "93: Train Loss: [2.306116819381714, 0.32462817430496216, 1.9814887046813965] | Test Loss: [3.585956335067749, 0.31932273507118225, 3.2666335105895996]\n",
            "94: Train Loss: [2.1713409423828125, 0.27105483412742615, 1.900286078453064] | Test Loss: [3.5660409927368164, 0.2939329743385315, 3.2721080780029297]\n",
            "95: Train Loss: [2.1048994064331055, 0.2727292776107788, 1.8321701288223267] | Test Loss: [3.5479612350463867, 0.26595839858055115, 3.2820029258728027]\n",
            "96: Train Loss: [2.562509059906006, 0.32334768772125244, 2.239161252975464] | Test Loss: [3.5382392406463623, 0.2439701408147812, 3.29426908493042]\n",
            "97: Train Loss: [1.7323241233825684, 0.37249213457107544, 1.3598319292068481] | Test Loss: [3.539876937866211, 0.23181793093681335, 3.308058977127075]\n",
            "98: Train Loss: [2.1502602100372314, 0.2532086968421936, 1.8970515727996826] | Test Loss: [3.5393199920654297, 0.2228972315788269, 3.316422700881958]\n",
            "99: Train Loss: [2.31732439994812, 0.2974449098110199, 2.0198795795440674] | Test Loss: [3.542543888092041, 0.21867725253105164, 3.323866605758667]\n",
            "100: Train Loss: [1.7567914724349976, 0.276513934135437, 1.4802775382995605] | Test Loss: [3.5503621101379395, 0.2176944613456726, 3.332667589187622]\n",
            "101: Train Loss: [2.5324268341064453, 0.29025712609291077, 2.2421696186065674] | Test Loss: [3.554710865020752, 0.21945849061012268, 3.335252285003662]\n",
            "102: Train Loss: [2.0108625888824463, 0.2895142436027527, 1.7213484048843384] | Test Loss: [3.5561318397521973, 0.22318583726882935, 3.3329460620880127]\n",
            "103: Train Loss: [2.3654680252075195, 0.30202868580818176, 2.06343936920166] | Test Loss: [3.5592570304870605, 0.2300080806016922, 3.3292489051818848]\n",
            "104: Train Loss: [2.084261655807495, 0.3539382815361023, 1.7303234338760376] | Test Loss: [3.570652484893799, 0.24056608974933624, 3.3300864696502686]\n",
            "105: Train Loss: [1.7460180521011353, 0.2671208083629608, 1.478897213935852] | Test Loss: [3.579896926879883, 0.25054681301116943, 3.329349994659424]\n",
            "106: Train Loss: [2.230186939239502, 0.29342567920684814, 1.9367611408233643] | Test Loss: [3.590097427368164, 0.2592836618423462, 3.3308136463165283]\n",
            "107: Train Loss: [1.9005756378173828, 0.30519381165504456, 1.5953818559646606] | Test Loss: [3.601386785507202, 0.26752257347106934, 3.333864212036133]\n",
            "108: Train Loss: [2.298769474029541, 0.3302885591983795, 1.9684809446334839] | Test Loss: [3.611985206604004, 0.2746290862560272, 3.3373560905456543]\n",
            "109: Train Loss: [1.4217588901519775, 0.3013356328010559, 1.1204231977462769] | Test Loss: [3.6168839931488037, 0.27600035071372986, 3.340883731842041]\n",
            "110: Train Loss: [2.18129563331604, 0.2795393168926239, 1.9017564058303833] | Test Loss: [3.610135078430176, 0.27225261926651, 3.3378825187683105]\n",
            "111: Train Loss: [1.6508793830871582, 0.336334764957428, 1.314544677734375] | Test Loss: [3.603893280029297, 0.26739704608917236, 3.336496353149414]\n",
            "112: Train Loss: [1.912071704864502, 0.30408909916877747, 1.6079826354980469] | Test Loss: [3.5946617126464844, 0.26225894689559937, 3.3324027061462402]\n",
            "113: Train Loss: [1.7510348558425903, 0.2862158417701721, 1.4648189544677734] | Test Loss: [3.587275743484497, 0.2574831545352936, 3.3297924995422363]\n",
            "114: Train Loss: [2.0854079723358154, 0.28263452649116516, 1.8027734756469727] | Test Loss: [3.5868945121765137, 0.2544919550418854, 3.332402467727661]\n",
            "115: Train Loss: [2.461906671524048, 0.29166319966316223, 2.170243501663208] | Test Loss: [3.5798094272613525, 0.2519908547401428, 3.3278186321258545]\n",
            "116: Train Loss: [2.0263686180114746, 0.31006860733032227, 1.716300129890442] | Test Loss: [3.567356824874878, 0.2501419484615326, 3.3172149658203125]\n",
            "117: Train Loss: [2.2889912128448486, 0.2762179970741272, 2.012773275375366] | Test Loss: [3.554891347885132, 0.25069594383239746, 3.3041954040527344]\n",
            "118: Train Loss: [2.1085729598999023, 0.2651539444923401, 1.8434189558029175] | Test Loss: [3.5413005352020264, 0.2505393624305725, 3.2907612323760986]\n",
            "119: Train Loss: [2.065979242324829, 0.3012310564517975, 1.764748215675354] | Test Loss: [3.539045810699463, 0.25290265679359436, 3.2861430644989014]\n",
            "120: Train Loss: [2.1189732551574707, 0.35769668221473694, 1.7612764835357666] | Test Loss: [3.546337842941284, 0.2605336010456085, 3.285804271697998]\n",
            "121: Train Loss: [2.4298174381256104, 0.35036349296569824, 2.079453945159912] | Test Loss: [3.55503249168396, 0.2717217803001404, 3.283310651779175]\n",
            "122: Train Loss: [2.1378958225250244, 0.2794700562953949, 1.8584257364273071] | Test Loss: [3.5624217987060547, 0.27957987785339355, 3.282841920852661]\n",
            "123: Train Loss: [1.9673211574554443, 0.3015121519565582, 1.6658090353012085] | Test Loss: [3.5629498958587646, 0.2836325466632843, 3.2793173789978027]\n",
            "124: Train Loss: [2.3539655208587646, 0.3386329710483551, 2.0153324604034424] | Test Loss: [3.5635476112365723, 0.2865796387195587, 3.276968002319336]\n",
            "125: Train Loss: [2.0971567630767822, 0.27289775013923645, 1.8242590427398682] | Test Loss: [3.5559239387512207, 0.28349408507347107, 3.272429943084717]\n",
            "126: Train Loss: [2.219728946685791, 0.2826181650161743, 1.9371106624603271] | Test Loss: [3.5491316318511963, 0.2750125527381897, 3.2741191387176514]\n",
            "127: Train Loss: [2.0073769092559814, 0.3036665618419647, 1.7037103176116943] | Test Loss: [3.541299343109131, 0.2643071711063385, 3.276992082595825]\n",
            "128: Train Loss: [2.473482131958008, 0.35440436005592346, 2.119077682495117] | Test Loss: [3.541809320449829, 0.258558988571167, 3.283250331878662]\n",
            "129: Train Loss: [1.8821167945861816, 0.30332228541374207, 1.5787944793701172] | Test Loss: [3.5395572185516357, 0.2533462941646576, 3.2862110137939453]\n",
            "130: Train Loss: [1.9711573123931885, 0.2947010397911072, 1.676456332206726] | Test Loss: [3.5247042179107666, 0.2460947334766388, 3.27860951423645]\n",
            "131: Train Loss: [2.1079354286193848, 0.3695918023586273, 1.7383437156677246] | Test Loss: [3.5180001258850098, 0.24367834627628326, 3.2743217945098877]\n",
            "132: Train Loss: [1.1830979585647583, 0.2771115303039551, 0.9059863090515137] | Test Loss: [3.5130741596221924, 0.24052047729492188, 3.2725536823272705]\n",
            "133: Train Loss: [2.299715042114258, 0.32419824600219727, 1.975516676902771] | Test Loss: [3.5106451511383057, 0.24038583040237427, 3.270259380340576]\n",
            "134: Train Loss: [2.497774362564087, 0.3077910542488098, 2.189983367919922] | Test Loss: [3.512436628341675, 0.24163280427455902, 3.270803928375244]\n",
            "135: Train Loss: [2.2899818420410156, 0.2825874984264374, 2.007394313812256] | Test Loss: [3.508122682571411, 0.23961642384529114, 3.2685062885284424]\n",
            "136: Train Loss: [2.1309478282928467, 0.2774287462234497, 1.853519082069397] | Test Loss: [3.4992637634277344, 0.23702357709407806, 3.262240171432495]\n",
            "137: Train Loss: [2.1721065044403076, 0.3359411656856537, 1.8361653089523315] | Test Loss: [3.495760440826416, 0.23706401884555817, 3.2586963176727295]\n",
            "138: Train Loss: [2.1274874210357666, 0.24187634885311127, 1.8856111764907837] | Test Loss: [3.48899245262146, 0.2342805415391922, 3.254711866378784]\n",
            "139: Train Loss: [2.1723899841308594, 0.28619593381881714, 1.886194109916687] | Test Loss: [3.4866604804992676, 0.23207136988639832, 3.254589080810547]\n",
            "140: Train Loss: [2.029794931411743, 0.2997105121612549, 1.7300844192504883] | Test Loss: [3.487705945968628, 0.22926481068134308, 3.258441209793091]\n",
            "141: Train Loss: [2.007864475250244, 0.2418745458126068, 1.7659900188446045] | Test Loss: [3.4830448627471924, 0.22536979615688324, 3.2576751708984375]\n",
            "142: Train Loss: [2.3334603309631348, 0.2792622745037079, 2.0541980266571045] | Test Loss: [3.4799232482910156, 0.22059567272663116, 3.2593276500701904]\n",
            "143: Train Loss: [2.322592258453369, 0.25627610087394714, 2.0663161277770996] | Test Loss: [3.474910020828247, 0.21607443690299988, 3.258835554122925]\n",
            "144: Train Loss: [2.180853843688965, 0.27264416217803955, 1.9082098007202148] | Test Loss: [3.4667694568634033, 0.213216170668602, 3.2535533905029297]\n",
            "145: Train Loss: [1.8764324188232422, 0.2776448130607605, 1.5987876653671265] | Test Loss: [3.4608230590820312, 0.21279720962047577, 3.248025894165039]\n",
            "146: Train Loss: [2.1484265327453613, 0.2812083959579468, 1.867218017578125] | Test Loss: [3.457744836807251, 0.21252979338169098, 3.2452149391174316]\n",
            "147: Train Loss: [2.4240033626556396, 0.29317644238471985, 2.130826950073242] | Test Loss: [3.460984230041504, 0.21611182391643524, 3.2448723316192627]\n",
            "148: Train Loss: [2.32476806640625, 0.30783671140670776, 2.0169312953948975] | Test Loss: [3.4681894779205322, 0.22251483798027039, 3.2456746101379395]\n",
            "149: Train Loss: [2.33202862739563, 0.30415549874305725, 2.0278730392456055] | Test Loss: [3.4811387062072754, 0.2313815951347351, 3.2497570514678955]\n",
            "150: Train Loss: [2.280853033065796, 0.2904532551765442, 1.9903998374938965] | Test Loss: [3.4936206340789795, 0.2373957335948944, 3.2562248706817627]\n",
            "151: Train Loss: [2.003593921661377, 0.3474483788013458, 1.6561458110809326] | Test Loss: [3.5102293491363525, 0.24498814344406128, 3.2652411460876465]\n",
            "152: Train Loss: [2.296295166015625, 0.29755985736846924, 1.9987351894378662] | Test Loss: [3.5280957221984863, 0.24993331730365753, 3.2781624794006348]\n",
            "153: Train Loss: [2.2215662002563477, 0.2968100905418396, 1.9247560501098633] | Test Loss: [3.53519344329834, 0.25030604004859924, 3.2848873138427734]\n",
            "154: Train Loss: [2.0491983890533447, 0.3030641973018646, 1.7461342811584473] | Test Loss: [3.531017780303955, 0.24485211074352264, 3.286165714263916]\n",
            "155: Train Loss: [2.201056718826294, 0.26782095432281494, 1.933235764503479] | Test Loss: [3.513779878616333, 0.2324257344007492, 3.2813541889190674]\n",
            "156: Train Loss: [2.299720287322998, 0.3124518096446991, 1.9872685670852661] | Test Loss: [3.4974493980407715, 0.2228878140449524, 3.274561643600464]\n",
            "157: Train Loss: [1.736128807067871, 0.2906743288040161, 1.445454478263855] | Test Loss: [3.484227418899536, 0.2151222676038742, 3.2691051959991455]\n",
            "158: Train Loss: [2.18936824798584, 0.3361167907714844, 1.8532514572143555] | Test Loss: [3.471849203109741, 0.21113187074661255, 3.2607173919677734]\n",
            "159: Train Loss: [2.1331236362457275, 0.3138695955276489, 1.8192540407180786] | Test Loss: [3.4641780853271484, 0.21184685826301575, 3.252331256866455]\n",
            "160: Train Loss: [2.283510684967041, 0.26697343587875366, 2.0165371894836426] | Test Loss: [3.4475107192993164, 0.21248604357242584, 3.2350246906280518]\n",
            "161: Train Loss: [2.0906381607055664, 0.3103359043598175, 1.7803021669387817] | Test Loss: [3.429354190826416, 0.2171592265367508, 3.2121949195861816]\n",
            "162: Train Loss: [2.2405433654785156, 0.3489755392074585, 1.8915677070617676] | Test Loss: [3.4190330505371094, 0.22769226133823395, 3.191340684890747]\n",
            "163: Train Loss: [1.9103535413742065, 0.26083505153656006, 1.6495184898376465] | Test Loss: [3.4133810997009277, 0.23603130877017975, 3.177349805831909]\n",
            "164: Train Loss: [2.350388765335083, 0.31862083077430725, 2.0317678451538086] | Test Loss: [3.4176034927368164, 0.24712088704109192, 3.170482635498047]\n",
            "165: Train Loss: [1.927418828010559, 0.29651907086372375, 1.6308997869491577] | Test Loss: [3.419616937637329, 0.2583100199699402, 3.161306858062744]\n",
            "166: Train Loss: [2.324890613555908, 0.33145296573638916, 1.9934377670288086] | Test Loss: [3.420881509780884, 0.2687227427959442, 3.152158737182617]\n",
            "167: Train Loss: [2.3562209606170654, 0.4324338138103485, 1.923787236213684] | Test Loss: [3.4362032413482666, 0.29680874943733215, 3.139394521713257]\n",
            "168: Train Loss: [1.7637879848480225, 0.30405953526496887, 1.459728479385376] | Test Loss: [3.4474096298217773, 0.31843042373657227, 3.128979206085205]\n",
            "169: Train Loss: [2.103130578994751, 0.294233113527298, 1.8088974952697754] | Test Loss: [3.4414539337158203, 0.3209587037563324, 3.120495319366455]\n",
            "170: Train Loss: [2.42571759223938, 0.34190884232521057, 2.083808660507202] | Test Loss: [3.4231789112091064, 0.3121631145477295, 3.111015796661377]\n",
            "171: Train Loss: [2.0478787422180176, 0.2920411229133606, 1.7558375597000122] | Test Loss: [3.3993241786956787, 0.29564687609672546, 3.103677272796631]\n",
            "172: Train Loss: [1.9831653833389282, 0.3085606098175049, 1.6746047735214233] | Test Loss: [3.3674867153167725, 0.2746717035770416, 3.0928149223327637]\n",
            "173: Train Loss: [2.1961593627929688, 0.2824023962020874, 1.9137568473815918] | Test Loss: [3.3320508003234863, 0.25597432255744934, 3.0760765075683594]\n",
            "174: Train Loss: [1.9193426370620728, 0.31344833970069885, 1.6058943271636963] | Test Loss: [3.308671474456787, 0.24609315395355225, 3.0625782012939453]\n",
            "175: Train Loss: [2.2642152309417725, 0.2674902081489563, 1.996725082397461] | Test Loss: [3.294461250305176, 0.24127821624279022, 3.053183078765869]\n",
            "176: Train Loss: [1.9140188694000244, 0.2386496365070343, 1.6753692626953125] | Test Loss: [3.2840628623962402, 0.23811756074428558, 3.045945405960083]\n",
            "177: Train Loss: [2.4751150608062744, 0.26508763432502747, 2.2100274562835693] | Test Loss: [3.2769784927368164, 0.23587548732757568, 3.0411031246185303]\n",
            "178: Train Loss: [2.0064644813537598, 0.2896784543991089, 1.7167860269546509] | Test Loss: [3.269749164581299, 0.23695684969425201, 3.032792329788208]\n",
            "179: Train Loss: [2.323326349258423, 0.3121612071990967, 2.011165142059326] | Test Loss: [3.272451639175415, 0.24109666049480438, 3.0313549041748047]\n",
            "180: Train Loss: [2.474266529083252, 0.3535148799419403, 2.1207516193389893] | Test Loss: [3.2835519313812256, 0.2512805461883545, 3.032271385192871]\n",
            "181: Train Loss: [1.8771010637283325, 0.3281261920928955, 1.548974871635437] | Test Loss: [3.299541711807251, 0.2658260762691498, 3.0337157249450684]\n",
            "182: Train Loss: [2.4023916721343994, 0.3003520667552948, 2.1020395755767822] | Test Loss: [3.3122215270996094, 0.281067430973053, 3.031154155731201]\n",
            "183: Train Loss: [2.0869505405426025, 0.3489210307598114, 1.7380295991897583] | Test Loss: [3.3335609436035156, 0.3017600178718567, 3.0318009853363037]\n",
            "184: Train Loss: [1.9756968021392822, 0.2653748393058777, 1.7103219032287598] | Test Loss: [3.346924066543579, 0.31534647941589355, 3.0315775871276855]\n",
            "185: Train Loss: [1.994012475013733, 0.2838402986526489, 1.710172176361084] | Test Loss: [3.349980354309082, 0.31989410519599915, 3.0300862789154053]\n",
            "186: Train Loss: [2.220731019973755, 0.2990744709968567, 1.9216564893722534] | Test Loss: [3.342506170272827, 0.31268975138664246, 3.0298163890838623]\n",
            "187: Train Loss: [2.0136585235595703, 0.2689131796360016, 1.7447453737258911] | Test Loss: [3.334841251373291, 0.30369246006011963, 3.031148910522461]\n",
            "188: Train Loss: [2.3196845054626465, 0.32040828466415405, 1.9992762804031372] | Test Loss: [3.318345308303833, 0.29261326789855957, 3.0257320404052734]\n",
            "189: Train Loss: [1.9039123058319092, 0.3797972798347473, 1.5241150856018066] | Test Loss: [3.306037664413452, 0.29072099924087524, 3.0153167247772217]\n",
            "190: Train Loss: [1.9802875518798828, 0.2849814295768738, 1.6953061819076538] | Test Loss: [3.2949843406677246, 0.28727155923843384, 3.0077128410339355]\n",
            "191: Train Loss: [2.5504274368286133, 0.28708726167678833, 2.2633402347564697] | Test Loss: [3.2882204055786133, 0.28253424167633057, 3.005686044692993]\n",
            "192: Train Loss: [1.789886713027954, 0.2869179844856262, 1.5029687881469727] | Test Loss: [3.2793164253234863, 0.27933457493782043, 2.9999818801879883]\n",
            "193: Train Loss: [2.070957660675049, 0.3233005106449127, 1.7476571798324585] | Test Loss: [3.2769932746887207, 0.2777804732322693, 2.9992127418518066]\n",
            "194: Train Loss: [2.141850233078003, 0.2715906798839569, 1.8702595233917236] | Test Loss: [3.276376247406006, 0.2757781147956848, 3.000598192214966]\n",
            "195: Train Loss: [1.8628474473953247, 0.2682575285434723, 1.5945899486541748] | Test Loss: [3.279083013534546, 0.2739613354206085, 3.0051217079162598]\n",
            "196: Train Loss: [2.5513057708740234, 0.24592994153499603, 2.305375814437866] | Test Loss: [3.2813730239868164, 0.2692370116710663, 3.0121359825134277]\n",
            "197: Train Loss: [2.35134220123291, 0.28166908025741577, 2.0696732997894287] | Test Loss: [3.291534185409546, 0.26576200127601624, 3.0257720947265625]\n",
            "198: Train Loss: [2.2242796421051025, 0.2718275487422943, 1.9524521827697754] | Test Loss: [3.30259370803833, 0.2646760046482086, 3.0379176139831543]\n",
            "199: Train Loss: [2.5371809005737305, 0.25696709752082825, 2.2802135944366455] | Test Loss: [3.3144662380218506, 0.2623646855354309, 3.0521016120910645]\n",
            "200: Train Loss: [2.584594249725342, 0.31276339292526245, 2.2718307971954346] | Test Loss: [3.3227956295013428, 0.26430344581604004, 3.0584921836853027]\n",
            "201: Train Loss: [2.2001683712005615, 0.3407110869884491, 1.85945725440979] | Test Loss: [3.326907157897949, 0.26839250326156616, 3.0585145950317383]\n",
            "202: Train Loss: [0.9854741096496582, 0.3143807351589203, 0.6710934042930603] | Test Loss: [3.3326001167297363, 0.2744234502315521, 3.0581767559051514]\n",
            "203: Train Loss: [1.963830590248108, 0.2846074402332306, 1.6792231798171997] | Test Loss: [3.3312571048736572, 0.28054872155189514, 3.050708293914795]\n",
            "204: Train Loss: [1.9136031866073608, 0.267362117767334, 1.6462410688400269] | Test Loss: [3.327483654022217, 0.28589239716529846, 3.041591167449951]\n",
            "205: Train Loss: [2.1125035285949707, 0.28573745489120483, 1.8267661333084106] | Test Loss: [3.3189752101898193, 0.2880759835243225, 3.0308992862701416]\n",
            "206: Train Loss: [2.172607660293579, 0.27523186802864075, 1.8973757028579712] | Test Loss: [3.3129589557647705, 0.28826966881752014, 3.024689197540283]\n",
            "207: Train Loss: [2.461480140686035, 0.3267708122730255, 2.134709358215332] | Test Loss: [3.3058454990386963, 0.28873151540756226, 3.0171139240264893]\n",
            "208: Train Loss: [2.13337779045105, 0.30331501364707947, 1.830062747001648] | Test Loss: [3.2917017936706543, 0.2869226038455963, 3.004779100418091]\n",
            "209: Train Loss: [2.0225937366485596, 0.37147754430770874, 1.651116132736206] | Test Loss: [3.2877211570739746, 0.2897377014160156, 2.997983455657959]\n",
            "210: Train Loss: [2.167304515838623, 0.2914029359817505, 1.875901460647583] | Test Loss: [3.27669620513916, 0.2874473035335541, 2.9892489910125732]\n",
            "211: Train Loss: [2.1245126724243164, 0.3497043251991272, 1.774808406829834] | Test Loss: [3.2670884132385254, 0.28788861632347107, 2.9791998863220215]\n",
            "212: Train Loss: [2.266768217086792, 0.3040836453437805, 1.9626845121383667] | Test Loss: [3.2612051963806152, 0.28822386264801025, 2.9729814529418945]\n",
            "213: Train Loss: [1.9771687984466553, 0.34371763467788696, 1.633451223373413] | Test Loss: [3.252969980239868, 0.28882816433906555, 2.964141845703125]\n",
            "214: Train Loss: [2.1652326583862305, 0.2611631453037262, 1.904069423675537] | Test Loss: [3.2496163845062256, 0.2844070792198181, 2.9652092456817627]\n",
            "215: Train Loss: [1.8635327816009521, 0.3982122838497162, 1.4653204679489136] | Test Loss: [3.253194808959961, 0.28488656878471375, 2.968308210372925]\n",
            "216: Train Loss: [2.344614267349243, 0.27543631196022034, 2.0691778659820557] | Test Loss: [3.258819341659546, 0.28248971700668335, 2.9763295650482178]\n",
            "217: Train Loss: [2.0127217769622803, 0.2972432076931, 1.7154786586761475] | Test Loss: [3.2626750469207764, 0.2776648998260498, 2.9850101470947266]\n",
            "218: Train Loss: [2.2192494869232178, 0.2961803376674652, 1.9230692386627197] | Test Loss: [3.2648866176605225, 0.2713618278503418, 2.9935247898101807]\n",
            "219: Train Loss: [2.3931851387023926, 0.3475520610809326, 2.04563307762146] | Test Loss: [3.270077705383301, 0.26644861698150635, 3.003629207611084]\n",
            "220: Train Loss: [2.4231560230255127, 0.30016738176345825, 2.122988700866699] | Test Loss: [3.2700815200805664, 0.25904136896133423, 3.011040210723877]\n",
            "221: Train Loss: [2.016058921813965, 0.30783697962760925, 1.7082220315933228] | Test Loss: [3.270953893661499, 0.250866174697876, 3.020087718963623]\n",
            "222: Train Loss: [1.9513390064239502, 0.2829405963420868, 1.668398380279541] | Test Loss: [3.2705326080322266, 0.24270479381084442, 3.027827739715576]\n",
            "223: Train Loss: [2.083831310272217, 0.3040560483932495, 1.7797752618789673] | Test Loss: [3.274449348449707, 0.23622646927833557, 3.0382227897644043]\n",
            "224: Train Loss: [1.9015002250671387, 0.24802610278129578, 1.6534740924835205] | Test Loss: [3.274528741836548, 0.23009277880191803, 3.044435977935791]\n",
            "225: Train Loss: [2.124223232269287, 0.4081428647041321, 1.7160803079605103] | Test Loss: [3.2836015224456787, 0.2327546626329422, 3.050846815109253]\n",
            "226: Train Loss: [2.0395302772521973, 0.3476254940032959, 1.6919046640396118] | Test Loss: [3.2926504611968994, 0.23838363587856293, 3.054266929626465]\n",
            "227: Train Loss: [2.430729389190674, 0.3522431552410126, 2.078486204147339] | Test Loss: [3.3052728176116943, 0.24528585374355316, 3.0599870681762695]\n",
            "228: Train Loss: [2.2505626678466797, 0.25237926840782166, 1.9981834888458252] | Test Loss: [3.3135900497436523, 0.2490004301071167, 3.064589738845825]\n",
            "229: Train Loss: [2.399009943008423, 0.3409365713596344, 2.0580732822418213] | Test Loss: [3.328245162963867, 0.25568336248397827, 3.072561740875244]\n",
            "230: Train Loss: [1.988909363746643, 0.3542746603488922, 1.6346347332000732] | Test Loss: [3.3380908966064453, 0.2622799873352051, 3.0758109092712402]\n",
            "231: Train Loss: [1.6956051588058472, 0.28587615489959717, 1.40972900390625] | Test Loss: [3.343745231628418, 0.2636417746543884, 3.0801033973693848]\n",
            "232: Train Loss: [2.5570809841156006, 0.275615394115448, 2.281465530395508] | Test Loss: [3.3452699184417725, 0.2605268657207489, 3.084743022918701]\n",
            "233: Train Loss: [2.3768248558044434, 0.29308652877807617, 2.083738327026367] | Test Loss: [3.335632085800171, 0.2527048587799072, 3.0829272270202637]\n",
            "234: Train Loss: [2.219008445739746, 0.312904953956604, 1.906103491783142] | Test Loss: [3.330134630203247, 0.24488288164138794, 3.085251808166504]\n",
            "235: Train Loss: [2.339158058166504, 0.2978818118572235, 2.041276216506958] | Test Loss: [3.3256008625030518, 0.23875731229782104, 3.086843490600586]\n",
            "236: Train Loss: [2.614816427230835, 0.35355207324028015, 2.2612643241882324] | Test Loss: [3.332897424697876, 0.2385862022638321, 3.094311237335205]\n",
            "237: Train Loss: [2.576936960220337, 0.31824272871017456, 2.2586941719055176] | Test Loss: [3.338890552520752, 0.2379823625087738, 3.1009082794189453]\n",
            "238: Train Loss: [2.3070669174194336, 0.26565346121788025, 2.0414135456085205] | Test Loss: [3.342355251312256, 0.23664844036102295, 3.1057066917419434]\n",
            "239: Train Loss: [2.005263328552246, 0.31903424859046936, 1.6862291097640991] | Test Loss: [3.346365213394165, 0.2346077710390091, 3.111757516860962]\n",
            "240: Train Loss: [2.1053574085235596, 0.2655128538608551, 1.8398445844650269] | Test Loss: [3.343024492263794, 0.23228320479393005, 3.110741376876831]\n",
            "241: Train Loss: [2.1419894695281982, 0.2855846583843231, 1.8564049005508423] | Test Loss: [3.342007875442505, 0.23150071501731873, 3.1105072498321533]\n",
            "242: Train Loss: [2.3352746963500977, 0.23690564930438995, 2.0983691215515137] | Test Loss: [3.3370823860168457, 0.2300025373697281, 3.1070797443389893]\n",
            "243: Train Loss: [2.0656421184539795, 0.2699281573295593, 1.795714020729065] | Test Loss: [3.3285977840423584, 0.22950346767902374, 3.0990943908691406]\n",
            "244: Train Loss: [1.8565711975097656, 0.37098604440689087, 1.4855852127075195] | Test Loss: [3.3304667472839355, 0.2362414300441742, 3.0942254066467285]\n",
            "245: Train Loss: [2.449634313583374, 0.3158120810985565, 2.133822202682495] | Test Loss: [3.326509475708008, 0.24441993236541748, 3.08208966255188]\n",
            "246: Train Loss: [2.3955588340759277, 0.2810940742492676, 2.11446475982666] | Test Loss: [3.3243958950042725, 0.25404077768325806, 3.070355176925659]\n",
            "247: Train Loss: [2.1570444107055664, 0.263558954000473, 1.8934855461120605] | Test Loss: [3.324064016342163, 0.26367563009262085, 3.0603883266448975]\n",
            "248: Train Loss: [1.1565802097320557, 0.31289613246917725, 0.8436840176582336] | Test Loss: [3.3292245864868164, 0.276455283164978, 3.052769422531128]\n",
            "249: Train Loss: [2.0806267261505127, 0.2983469069004059, 1.7822798490524292] | Test Loss: [3.334519386291504, 0.28743037581443787, 3.047089099884033]\n",
            "250: Train Loss: [0.860288679599762, 0.28997617959976196, 0.5703125] | Test Loss: [3.3370168209075928, 0.29463109374046326, 3.0423858165740967]\n",
            "251: Train Loss: [2.1913931369781494, 0.30241140723228455, 1.888981819152832] | Test Loss: [3.340794324874878, 0.29911550879478455, 3.0416789054870605]\n",
            "252: Train Loss: [1.833662986755371, 0.3336452841758728, 1.5000176429748535] | Test Loss: [3.341740608215332, 0.3008714020252228, 3.0408692359924316]\n",
            "253: Train Loss: [2.2942819595336914, 0.29799365997314453, 1.9962882995605469] | Test Loss: [3.3393168449401855, 0.2977890074253082, 3.04152774810791]\n",
            "254: Train Loss: [2.420858144760132, 0.27009373903274536, 2.1507644653320312] | Test Loss: [3.334444284439087, 0.29333463311195374, 3.041109561920166]\n",
            "255: Train Loss: [1.776277780532837, 0.25185132026672363, 1.5244264602661133] | Test Loss: [3.321666717529297, 0.28377825021743774, 3.037888526916504]\n",
            "256: Train Loss: [2.352207660675049, 0.3320431709289551, 2.0201644897460938] | Test Loss: [3.3160314559936523, 0.2793346047401428, 3.0366969108581543]\n",
            "257: Train Loss: [2.2773587703704834, 0.3137193024158478, 1.963639497756958] | Test Loss: [3.3105978965759277, 0.27423250675201416, 3.036365509033203]\n",
            "258: Train Loss: [1.7800949811935425, 0.24875809252262115, 1.531336784362793] | Test Loss: [3.303583860397339, 0.26836904883384705, 3.035214900970459]\n",
            "259: Train Loss: [2.2210135459899902, 0.24639733135700226, 1.9746161699295044] | Test Loss: [3.2902562618255615, 0.2630887031555176, 3.027167558670044]\n",
            "260: Train Loss: [1.8844465017318726, 0.23930224776268005, 1.6451442241668701] | Test Loss: [3.280574321746826, 0.25801506638526917, 3.02255916595459]\n",
            "261: Train Loss: [2.4850447177886963, 0.316377192735672, 2.1686675548553467] | Test Loss: [3.276806592941284, 0.25703567266464233, 3.019770860671997]\n",
            "262: Train Loss: [2.1540825366973877, 0.25857555866241455, 1.8955069780349731] | Test Loss: [3.2732841968536377, 0.2567579746246338, 3.016526222229004]\n",
            "263: Train Loss: [2.170227289199829, 0.28069207072257996, 1.8895351886749268] | Test Loss: [3.274479389190674, 0.2611914575099945, 3.0132880210876465]\n",
            "264: Train Loss: [1.7097878456115723, 0.301467627286911, 1.4083201885223389] | Test Loss: [3.2756540775299072, 0.268789142370224, 3.0068650245666504]\n",
            "265: Train Loss: [1.7649996280670166, 0.2599409222602844, 1.5050586462020874] | Test Loss: [3.2767906188964844, 0.27440395951271057, 3.0023865699768066]\n",
            "266: Train Loss: [2.3240058422088623, 0.36554810404777527, 1.9584578275680542] | Test Loss: [3.287947654724121, 0.28497663140296936, 3.0029709339141846]\n",
            "267: Train Loss: [1.9071072340011597, 0.24132461845874786, 1.6657825708389282] | Test Loss: [3.289724349975586, 0.29233089089393616, 2.9973933696746826]\n",
            "268: Train Loss: [2.267270088195801, 0.27135032415390015, 1.9959198236465454] | Test Loss: [3.293614149093628, 0.29931703209877014, 2.9942970275878906]\n",
            "269: Train Loss: [1.9651280641555786, 0.3232440650463104, 1.6418839693069458] | Test Loss: [3.2964138984680176, 0.3045445382595062, 2.9918694496154785]\n",
            "270: Train Loss: [2.073634624481201, 0.31391823291778564, 1.7597163915634155] | Test Loss: [3.300008535385132, 0.3086457848548889, 2.9913628101348877]\n",
            "271: Train Loss: [2.1779322624206543, 0.34030938148498535, 1.8376227617263794] | Test Loss: [3.3072609901428223, 0.3126297891139984, 2.994631290435791]\n",
            "272: Train Loss: [1.0752100944519043, 0.27608269453048706, 0.7991273403167725] | Test Loss: [3.309046745300293, 0.3093261122703552, 2.999720573425293]\n",
            "273: Train Loss: [2.241100549697876, 0.2739126980304718, 1.9671880006790161] | Test Loss: [3.3073995113372803, 0.30337709188461304, 3.0040223598480225]\n",
            "274: Train Loss: [2.022315740585327, 0.285648375749588, 1.7366673946380615] | Test Loss: [3.3039956092834473, 0.29571089148521423, 3.0082848072052]\n",
            "275: Train Loss: [2.369349956512451, 0.32649925351142883, 2.0428507328033447] | Test Loss: [3.3004398345947266, 0.2885317802429199, 3.0119080543518066]\n",
            "276: Train Loss: [2.333559036254883, 0.2577708661556244, 2.0757882595062256] | Test Loss: [3.2961807250976562, 0.28126761317253113, 3.0149130821228027]\n",
            "277: Train Loss: [2.237642288208008, 0.4021296203136444, 1.835512638092041] | Test Loss: [3.2970376014709473, 0.2833136022090912, 3.0137240886688232]\n",
            "278: Train Loss: [1.7373650074005127, 0.3666779696941376, 1.3706870079040527] | Test Loss: [3.30769944190979, 0.2948700487613678, 3.012829303741455]\n",
            "279: Train Loss: [1.521155595779419, 0.3287801444530487, 1.1923754215240479] | Test Loss: [3.3181028366088867, 0.3047623336315155, 3.013340473175049]\n",
            "280: Train Loss: [2.215832233428955, 0.3045617341995239, 1.9112704992294312] | Test Loss: [3.32167387008667, 0.3089890480041504, 3.0126848220825195]\n",
            "281: Train Loss: [2.0835437774658203, 0.2772921323776245, 1.8062517642974854] | Test Loss: [3.325258731842041, 0.3121235966682434, 3.0131351947784424]\n",
            "282: Train Loss: [2.2586469650268555, 0.31008103489875793, 1.94856595993042] | Test Loss: [3.3211162090301514, 0.31323355436325073, 3.007882595062256]\n",
            "283: Train Loss: [2.1924803256988525, 0.29279428720474243, 1.8996860980987549] | Test Loss: [3.318150043487549, 0.31246450543403625, 3.005685567855835]\n",
            "284: Train Loss: [2.1345415115356445, 0.3038872480392456, 1.8306543827056885] | Test Loss: [3.308840751647949, 0.3097459077835083, 2.9990949630737305]\n",
            "285: Train Loss: [2.396536111831665, 0.4149763286113739, 1.9815598726272583] | Test Loss: [3.3126511573791504, 0.31340956687927246, 2.999241590499878]\n",
            "286: Train Loss: [2.352583408355713, 0.30204784870147705, 2.0505354404449463] | Test Loss: [3.31935977935791, 0.314424604177475, 3.0049352645874023]\n",
            "287: Train Loss: [2.1712865829467773, 0.32964974641799927, 1.8416368961334229] | Test Loss: [3.3243207931518555, 0.31469860672950745, 3.009622097015381]\n",
            "288: Train Loss: [1.952884316444397, 0.2928714454174042, 1.6600128412246704] | Test Loss: [3.324284076690674, 0.3095798194408417, 3.0147042274475098]\n",
            "289: Train Loss: [1.978381872177124, 0.30198508501052856, 1.6763968467712402] | Test Loss: [3.312533378601074, 0.30121198296546936, 3.0113213062286377]\n",
            "290: Train Loss: [1.9029476642608643, 0.30343037843704224, 1.5995172262191772] | Test Loss: [3.303896188735962, 0.2921203076839447, 3.0117759704589844]\n",
            "291: Train Loss: [2.0222208499908447, 0.26425832509994507, 1.7579624652862549] | Test Loss: [3.294905662536621, 0.2824493944644928, 3.012456178665161]\n",
            "292: Train Loss: [1.9130942821502686, 0.28821176290512085, 1.6248825788497925] | Test Loss: [3.2849011421203613, 0.27495676279067993, 3.009944438934326]\n",
            "293: Train Loss: [1.9802346229553223, 0.32363468408584595, 1.6565998792648315] | Test Loss: [3.279296636581421, 0.26977112889289856, 3.0095255374908447]\n",
            "294: Train Loss: [1.851586937904358, 0.29277703166007996, 1.5588098764419556] | Test Loss: [3.2780885696411133, 0.266247421503067, 3.011841058731079]\n",
            "295: Train Loss: [2.3931713104248047, 0.33144721388816833, 2.0617241859436035] | Test Loss: [3.2753243446350098, 0.266042023897171, 3.009282350540161]\n",
            "296: Train Loss: [1.9732146263122559, 0.27341514825820923, 1.6997994184494019] | Test Loss: [3.26287579536438, 0.26754146814346313, 2.9953343868255615]\n",
            "297: Train Loss: [2.3709676265716553, 0.40244045853614807, 1.96852707862854] | Test Loss: [3.2612831592559814, 0.27501100301742554, 2.986272096633911]\n",
            "298: Train Loss: [2.0190322399139404, 0.294417142868042, 1.7246150970458984] | Test Loss: [3.261148452758789, 0.2836129367351532, 2.9775354862213135]\n",
            "299: Train Loss: [1.751983642578125, 0.2611536979675293, 1.4908299446105957] | Test Loss: [3.2617878913879395, 0.29084184765815735, 2.9709460735321045]\n",
            "300: Train Loss: [2.0593631267547607, 0.29920631647109985, 1.7601567506790161] | Test Loss: [3.2632837295532227, 0.2974492609500885, 2.965834379196167]\n",
            "301: Train Loss: [2.286315441131592, 0.34787753224372864, 1.9384379386901855] | Test Loss: [3.268043279647827, 0.30872267484664917, 2.959320545196533]\n",
            "302: Train Loss: [1.9229010343551636, 0.30482423305511475, 1.6180768013000488] | Test Loss: [3.269582509994507, 0.3179304897785187, 2.9516520500183105]\n",
            "303: Train Loss: [2.0668280124664307, 0.2960178256034851, 1.7708102464675903] | Test Loss: [3.2628488540649414, 0.320012629032135, 2.942836284637451]\n",
            "304: Train Loss: [2.4549334049224854, 0.2918164134025574, 2.163116931915283] | Test Loss: [3.2502481937408447, 0.31598442792892456, 2.9342637062072754]\n",
            "305: Train Loss: [2.125910758972168, 0.3176330626010895, 1.8082776069641113] | Test Loss: [3.235494613647461, 0.3089714050292969, 2.926523208618164]\n",
            "306: Train Loss: [1.7472662925720215, 0.3507739007472992, 1.3964923620224] | Test Loss: [3.2182304859161377, 0.3044058382511139, 2.9138245582580566]\n",
            "307: Train Loss: [2.3993237018585205, 0.28775936365127563, 2.1115643978118896] | Test Loss: [3.201664924621582, 0.29901552200317383, 2.902649402618408]\n",
            "308: Train Loss: [2.327134132385254, 0.36616280674934387, 1.9609713554382324] | Test Loss: [3.190490484237671, 0.29487988352775574, 2.8956105709075928]\n",
            "309: Train Loss: [2.0624442100524902, 0.28541234135627747, 1.7770320177078247] | Test Loss: [3.172736644744873, 0.2860311269760132, 2.8867053985595703]\n",
            "310: Train Loss: [2.1430845260620117, 0.29234227538108826, 1.8507423400878906] | Test Loss: [3.1531944274902344, 0.27618369460105896, 2.8770108222961426]\n",
            "311: Train Loss: [2.322268009185791, 0.32044097781181335, 2.0018270015716553] | Test Loss: [3.137439727783203, 0.26727938652038574, 2.8701603412628174]\n",
            "312: Train Loss: [2.038778305053711, 0.27796754240989685, 1.7608107328414917] | Test Loss: [3.126340866088867, 0.2582761347293854, 2.8680646419525146]\n",
            "313: Train Loss: [2.034172534942627, 0.3168278932571411, 1.7173446416854858] | Test Loss: [3.1178791522979736, 0.2524893581867218, 2.865389823913574]\n",
            "314: Train Loss: [2.449127674102783, 0.2807966470718384, 2.1683309078216553] | Test Loss: [3.1142940521240234, 0.2496458739042282, 2.8646481037139893]\n",
            "315: Train Loss: [2.1197545528411865, 0.3361945152282715, 1.783560037612915] | Test Loss: [3.112643241882324, 0.2533959448337555, 2.8592472076416016]\n",
            "316: Train Loss: [2.314176082611084, 0.2826978862285614, 2.0314781665802] | Test Loss: [3.1168019771575928, 0.2582854628562927, 2.8585164546966553]\n",
            "317: Train Loss: [2.085829734802246, 0.26506921648979187, 1.8207604885101318] | Test Loss: [3.119623899459839, 0.261494904756546, 2.8581290245056152]\n",
            "318: Train Loss: [1.2499394416809082, 0.3004719018936157, 0.9494674801826477] | Test Loss: [3.1288905143737793, 0.26746323704719543, 2.8614273071289062]\n",
            "319: Train Loss: [2.1003036499023438, 0.33035406470298767, 1.7699496746063232] | Test Loss: [3.140476942062378, 0.2741733193397522, 2.8663036823272705]\n",
            "320: Train Loss: [2.6436893939971924, 0.3725467622280121, 2.2711427211761475] | Test Loss: [3.1615753173828125, 0.280464231967926, 2.8811111450195312]\n",
            "321: Train Loss: [1.9911093711853027, 0.3252643942832947, 1.6658450365066528] | Test Loss: [3.1830880641937256, 0.2845289707183838, 2.898559093475342]\n",
            "322: Train Loss: [2.043905258178711, 0.28884223103523254, 1.7550630569458008] | Test Loss: [3.2055463790893555, 0.28624793887138367, 2.9192984104156494]\n",
            "323: Train Loss: [2.0679094791412354, 0.27536457777023315, 1.7925448417663574] | Test Loss: [3.2266271114349365, 0.2831832468509674, 2.943443775177002]\n",
            "324: Train Loss: [1.9288932085037231, 0.3208582401275635, 1.6080349683761597] | Test Loss: [3.2425856590270996, 0.280540406703949, 2.962045192718506]\n",
            "325: Train Loss: [2.358250379562378, 0.31569966673851013, 2.042550802230835] | Test Loss: [3.2559900283813477, 0.27703022956848145, 2.978959798812866]\n",
            "326: Train Loss: [2.2498421669006348, 0.31607192754745483, 1.9337704181671143] | Test Loss: [3.264326810836792, 0.27466797828674316, 2.989658832550049]\n",
            "327: Train Loss: [2.4130067825317383, 0.3334650695323944, 2.0795416831970215] | Test Loss: [3.273252010345459, 0.2773483693599701, 2.995903730392456]\n",
            "328: Train Loss: [2.0775468349456787, 0.3307328522205353, 1.7468138933181763] | Test Loss: [3.281435966491699, 0.28209254145622253, 2.9993433952331543]\n",
            "329: Train Loss: [1.5574164390563965, 0.2822000980377197, 1.2752163410186768] | Test Loss: [3.2789580821990967, 0.2838900089263916, 2.995068073272705]\n",
            "330: Train Loss: [1.9827831983566284, 0.31230899691581726, 1.6704741716384888] | Test Loss: [3.279409170150757, 0.2874927222728729, 2.9919164180755615]\n",
            "331: Train Loss: [2.4784727096557617, 0.3190802335739136, 2.1593923568725586] | Test Loss: [3.272601366043091, 0.2877844274044037, 2.9848170280456543]\n",
            "332: Train Loss: [2.1087839603424072, 0.2806738317012787, 1.8281102180480957] | Test Loss: [3.2577669620513916, 0.281802237033844, 2.9759647846221924]\n",
            "333: Train Loss: [1.0203461647033691, 0.29197072982788086, 0.7283753752708435] | Test Loss: [3.2510836124420166, 0.2798130512237549, 2.9712705612182617]\n",
            "334: Train Loss: [1.8259929418563843, 0.2740768492221832, 1.5519161224365234] | Test Loss: [3.247859001159668, 0.2789483666419983, 2.9689106941223145]\n",
            "335: Train Loss: [1.870926856994629, 0.3049672842025757, 1.5659595727920532] | Test Loss: [3.247619867324829, 0.28036293387413025, 2.967257022857666]\n",
            "336: Train Loss: [1.887622356414795, 0.26710471510887146, 1.620517611503601] | Test Loss: [3.2465224266052246, 0.28006303310394287, 2.9664595127105713]\n",
            "337: Train Loss: [1.8530640602111816, 0.31549346446990967, 1.537570595741272] | Test Loss: [3.24727463722229, 0.2842629551887512, 2.9630117416381836]\n",
            "338: Train Loss: [2.2387821674346924, 0.3141322433948517, 1.924649953842163] | Test Loss: [3.249610662460327, 0.29024139046669006, 2.95936918258667]\n",
            "339: Train Loss: [2.032428741455078, 0.2510572373867035, 1.7813715934753418] | Test Loss: [3.2464630603790283, 0.29272767901420593, 2.9537353515625]\n",
            "340: Train Loss: [1.650759220123291, 0.33283019065856934, 1.3179290294647217] | Test Loss: [3.248462200164795, 0.2974835932254791, 2.9509785175323486]\n",
            "341: Train Loss: [2.3722493648529053, 0.324334055185318, 2.047915458679199] | Test Loss: [3.256246566772461, 0.3018032908439636, 2.9544432163238525]\n",
            "342: Train Loss: [1.9697437286376953, 0.2690402567386627, 1.700703501701355] | Test Loss: [3.2617416381835938, 0.3024841547012329, 2.9592576026916504]\n",
            "343: Train Loss: [1.0579006671905518, 0.2946305572986603, 0.7632701396942139] | Test Loss: [3.268331527709961, 0.3000492453575134, 2.9682822227478027]\n",
            "344: Train Loss: [2.0693345069885254, 0.27409014105796814, 1.7952444553375244] | Test Loss: [3.272353172302246, 0.29559025168418884, 2.9767630100250244]\n",
            "345: Train Loss: [2.4524457454681396, 0.2767401337623596, 2.175705671310425] | Test Loss: [3.2653579711914062, 0.28996506333351135, 2.9753928184509277]\n",
            "346: Train Loss: [2.0933828353881836, 0.30714723467826843, 1.7862355709075928] | Test Loss: [3.257572650909424, 0.2860105037689209, 2.971562147140503]\n",
            "347: Train Loss: [1.9819538593292236, 0.29350346326828003, 1.6884503364562988] | Test Loss: [3.244863748550415, 0.2842877507209778, 2.960576057434082]\n",
            "348: Train Loss: [1.8596975803375244, 0.36476004123687744, 1.494937539100647] | Test Loss: [3.2364461421966553, 0.28785091638565063, 2.9485952854156494]\n",
            "349: Train Loss: [2.038123846054077, 0.3212505877017975, 1.716873288154602] | Test Loss: [3.2291698455810547, 0.29399770498275757, 2.9351720809936523]\n",
            "350: Train Loss: [1.9627835750579834, 0.2862950563430786, 1.6764885187149048] | Test Loss: [3.217707872390747, 0.29651743173599243, 2.9211905002593994]\n",
            "351: Train Loss: [2.111351251602173, 0.32030588388442993, 1.7910453081130981] | Test Loss: [3.2107021808624268, 0.302092969417572, 2.90860915184021]\n",
            "352: Train Loss: [1.6239396333694458, 0.29293254017829895, 1.3310070037841797] | Test Loss: [3.198640823364258, 0.30497464537620544, 2.8936662673950195]\n",
            "353: Train Loss: [1.9026401042938232, 0.3150418996810913, 1.587598204612732] | Test Loss: [3.192106008529663, 0.3089093267917633, 2.8831965923309326]\n",
            "354: Train Loss: [2.1594045162200928, 0.29549723863601685, 1.8639073371887207] | Test Loss: [3.1865878105163574, 0.31047192215919495, 2.8761157989501953]\n",
            "355: Train Loss: [2.390439987182617, 0.325935423374176, 2.064504623413086] | Test Loss: [3.193763494491577, 0.31370338797569275, 2.8800601959228516]\n",
            "356: Train Loss: [2.0126473903656006, 0.2615962326526642, 1.7510511875152588] | Test Loss: [3.194760322570801, 0.30917736887931824, 2.88558292388916]\n",
            "357: Train Loss: [2.2530736923217773, 0.2965097725391388, 1.9565638303756714] | Test Loss: [3.1960434913635254, 0.30181631445884705, 2.8942272663116455]\n",
            "358: Train Loss: [1.929984211921692, 0.24571825563907623, 1.6842659711837769] | Test Loss: [3.1957666873931885, 0.2894229292869568, 2.906343698501587]\n",
            "359: Train Loss: [1.7454447746276855, 0.2964390516281128, 1.4490057229995728] | Test Loss: [3.2015013694763184, 0.28168025612831116, 2.91982102394104]\n",
            "360: Train Loss: [2.471376657485962, 0.3189392685890198, 2.152437448501587] | Test Loss: [3.218484878540039, 0.281055212020874, 2.937429666519165]\n",
            "361: Train Loss: [1.7329050302505493, 0.3378286063671112, 1.3950763940811157] | Test Loss: [3.2411880493164062, 0.2869715094566345, 2.954216480255127]\n",
            "362: Train Loss: [2.291605234146118, 0.31290584802627563, 1.9786994457244873] | Test Loss: [3.260066270828247, 0.2946982979774475, 2.9653680324554443]\n",
            "363: Train Loss: [2.0070338249206543, 0.27181971073150635, 1.7352142333984375] | Test Loss: [3.271472930908203, 0.2989644706249237, 2.972508430480957]\n",
            "364: Train Loss: [1.741870403289795, 0.3127712309360504, 1.429099202156067] | Test Loss: [3.2852022647857666, 0.3061411380767822, 2.9790611267089844]\n",
            "365: Train Loss: [2.087818145751953, 0.28849878907203674, 1.7993193864822388] | Test Loss: [3.2978641986846924, 0.31470757722854614, 2.983156681060791]\n",
            "366: Train Loss: [2.078212022781372, 0.294133722782135, 1.7840783596038818] | Test Loss: [3.3078067302703857, 0.3204076290130615, 2.987399101257324]\n",
            "367: Train Loss: [1.9726371765136719, 0.2958630919456482, 1.6767741441726685] | Test Loss: [3.309791326522827, 0.318185031414032, 2.9916062355041504]\n",
            "368: Train Loss: [1.8468143939971924, 0.2662409245967865, 1.5805734395980835] | Test Loss: [3.3031370639801025, 0.30547410249710083, 2.9976630210876465]\n",
            "369: Train Loss: [2.3116297721862793, 0.38319599628448486, 1.9284335374832153] | Test Loss: [3.2997326850891113, 0.29854774475097656, 3.0011849403381348]\n",
            "370: Train Loss: [1.2775434255599976, 0.43520715832710266, 0.8423362970352173] | Test Loss: [3.31195068359375, 0.3056039810180664, 3.0063467025756836]\n",
            "371: Train Loss: [2.3794586658477783, 0.40254372358322144, 1.976914882659912] | Test Loss: [3.3378381729125977, 0.32283586263656616, 3.0150022506713867]\n",
            "372: Train Loss: [2.210460662841797, 0.3181907832622528, 1.8922698497772217] | Test Loss: [3.357419967651367, 0.33288416266441345, 3.024535894393921]\n",
            "373: Train Loss: [2.114352226257324, 0.40576204657554626, 1.7085902690887451] | Test Loss: [3.3778586387634277, 0.34542545676231384, 3.032433271408081]\n",
            "374: Train Loss: [2.224003791809082, 0.30909043550491333, 1.9149134159088135] | Test Loss: [3.3785510063171387, 0.3416825532913208, 3.0368685722351074]\n",
            "375: Train Loss: [1.9514210224151611, 0.324617862701416, 1.6268031597137451] | Test Loss: [3.3645589351654053, 0.3250548243522644, 3.039504051208496]\n",
            "376: Train Loss: [2.364419937133789, 0.3233932852745056, 2.0410265922546387] | Test Loss: [3.340512275695801, 0.3014904260635376, 3.0390219688415527]\n",
            "377: Train Loss: [2.2666914463043213, 0.28152137994766235, 1.9851701259613037] | Test Loss: [3.3142006397247314, 0.28004422783851624, 3.034156322479248]\n",
            "378: Train Loss: [1.8500807285308838, 0.29167240858078003, 1.558408260345459] | Test Loss: [3.288410186767578, 0.25873881578445435, 3.0296714305877686]\n",
            "379: Train Loss: [1.907626748085022, 0.3283085823059082, 1.5793181657791138] | Test Loss: [3.2738397121429443, 0.24714882671833038, 3.02669095993042]\n",
            "380: Train Loss: [2.1331920623779297, 0.31226450204849243, 1.820927619934082] | Test Loss: [3.2608306407928467, 0.2367507666349411, 3.0240797996520996]\n",
            "381: Train Loss: [1.8914668560028076, 0.3003966212272644, 1.591070294380188] | Test Loss: [3.2517712116241455, 0.2307630330324173, 3.021008253097534]\n",
            "382: Train Loss: [2.1252801418304443, 0.281821072101593, 1.8434590101242065] | Test Loss: [3.249833345413208, 0.2293531745672226, 3.020480155944824]\n",
            "383: Train Loss: [2.306057929992676, 0.39595258235931396, 1.9101051092147827] | Test Loss: [3.2535226345062256, 0.23643900454044342, 3.0170836448669434]\n",
            "384: Train Loss: [1.485593318939209, 0.2894687056541443, 1.19612455368042] | Test Loss: [3.2588934898376465, 0.2443705052137375, 3.0145230293273926]\n",
            "385: Train Loss: [2.063551902770996, 0.23663480579853058, 1.8269169330596924] | Test Loss: [3.2626161575317383, 0.2498825192451477, 3.0127336978912354]\n",
            "386: Train Loss: [1.975243091583252, 0.30737411975860596, 1.667868971824646] | Test Loss: [3.267681121826172, 0.2547744810581207, 3.012906551361084]\n",
            "387: Train Loss: [2.0693416595458984, 0.25843408703804016, 1.8109076023101807] | Test Loss: [3.2743735313415527, 0.25839561223983765, 3.0159778594970703]\n",
            "388: Train Loss: [1.9506109952926636, 0.2987034022808075, 1.6519076824188232] | Test Loss: [3.2841999530792236, 0.26208657026290894, 3.02211332321167]\n",
            "389: Train Loss: [2.2363357543945312, 0.31273266673088074, 1.9236031770706177] | Test Loss: [3.2973179817199707, 0.2690696120262146, 3.0282483100891113]\n",
            "390: Train Loss: [1.7705847024917603, 0.2197137027978897, 1.5508710145950317] | Test Loss: [3.300361156463623, 0.2710648775100708, 3.0292961597442627]\n",
            "391: Train Loss: [2.3889784812927246, 0.2742992639541626, 2.1146793365478516] | Test Loss: [3.2953226566314697, 0.2686575949192047, 3.026664972305298]\n",
            "392: Train Loss: [2.122886896133423, 0.31873175501823425, 1.8041552305221558] | Test Loss: [3.2891223430633545, 0.2689388692378998, 3.020183563232422]\n",
            "393: Train Loss: [2.3037707805633545, 0.2634962499141693, 2.0402746200561523] | Test Loss: [3.289278030395508, 0.268730103969574, 3.020547866821289]\n",
            "394: Train Loss: [2.510204315185547, 0.3399790823459625, 2.170225143432617] | Test Loss: [3.2925608158111572, 0.2727256417274475, 3.0198352336883545]\n",
            "395: Train Loss: [1.7438626289367676, 0.34138399362564087, 1.402478575706482] | Test Loss: [3.3011035919189453, 0.27973246574401855, 3.0213711261749268]\n",
            "396: Train Loss: [1.959001064300537, 0.25566789507865906, 1.7033331394195557] | Test Loss: [3.310480833053589, 0.2839992046356201, 3.0264816284179688]\n",
            "397: Train Loss: [2.25773286819458, 0.26617658138275146, 1.9915564060211182] | Test Loss: [3.3146939277648926, 0.28752806782722473, 3.0271658897399902]\n",
            "398: Train Loss: [1.584020972251892, 0.29755842685699463, 1.2864625453948975] | Test Loss: [3.322009325027466, 0.291532427072525, 3.0304768085479736]\n",
            "399: Train Loss: [1.8627370595932007, 0.2451210916042328, 1.6176159381866455] | Test Loss: [3.329730272293091, 0.2930935323238373, 3.0366368293762207]\n",
            "400: Train Loss: [2.4062485694885254, 0.2484581172466278, 2.157790422439575] | Test Loss: [3.3374361991882324, 0.28999412059783936, 3.0474421977996826]\n",
            "401: Train Loss: [1.9767515659332275, 0.31932589411735535, 1.6574256420135498] | Test Loss: [3.345745325088501, 0.2880704700946808, 3.0576748847961426]\n",
            "402: Train Loss: [2.21415114402771, 0.22104734182357788, 1.9931038618087769] | Test Loss: [3.3401618003845215, 0.28051069378852844, 3.0596511363983154]\n",
            "403: Train Loss: [1.9738150835037231, 0.3388122618198395, 1.635002851486206] | Test Loss: [3.3389692306518555, 0.2776336967945099, 3.061335563659668]\n",
            "404: Train Loss: [2.0952813625335693, 0.3235900104045868, 1.7716913223266602] | Test Loss: [3.3358190059661865, 0.2786555886268616, 3.0571634769439697]\n",
            "405: Train Loss: [2.3976213932037354, 0.3310263156890869, 2.0665950775146484] | Test Loss: [3.338144540786743, 0.2892504334449768, 3.048894166946411]\n",
            "406: Train Loss: [1.9682506322860718, 0.2839946448802948, 1.6842559576034546] | Test Loss: [3.3361053466796875, 0.29955777525901794, 3.0365476608276367]\n",
            "407: Train Loss: [2.1554622650146484, 0.32454583048820496, 1.8309165239334106] | Test Loss: [3.3372700214385986, 0.31144019961357117, 3.025829792022705]\n",
            "408: Train Loss: [2.1016619205474854, 0.2661692798137665, 1.835492730140686] | Test Loss: [3.335952043533325, 0.3210988938808441, 3.0148532390594482]\n",
            "409: Train Loss: [2.0469322204589844, 0.2932788133621216, 1.7536535263061523] | Test Loss: [3.3313567638397217, 0.32802271842956543, 3.0033340454101562]\n",
            "410: Train Loss: [1.8949081897735596, 0.28271645307540894, 1.6121916770935059] | Test Loss: [3.323798656463623, 0.33097347617149353, 2.9928252696990967]\n",
            "411: Train Loss: [2.028333902359009, 0.2566542327404022, 1.7716797590255737] | Test Loss: [3.3151378631591797, 0.33177414536476135, 2.983363628387451]\n",
            "412: Train Loss: [2.1314797401428223, 0.30270063877105713, 1.8287792205810547] | Test Loss: [3.3040318489074707, 0.3319077789783478, 2.9721240997314453]\n",
            "413: Train Loss: [1.8151366710662842, 0.24528168141841888, 1.569854974746704] | Test Loss: [3.292320966720581, 0.32734084129333496, 2.964980125427246]\n",
            "414: Train Loss: [2.1433420181274414, 0.31810232996940613, 1.8252397775650024] | Test Loss: [3.2854185104370117, 0.324847012758255, 2.960571527481079]\n",
            "415: Train Loss: [2.288975238800049, 0.38166454434394836, 1.9073107242584229] | Test Loss: [3.2829909324645996, 0.3264729976654053, 2.9565179347991943]\n",
            "416: Train Loss: [2.1268253326416016, 0.344233900308609, 1.7825913429260254] | Test Loss: [3.289185047149658, 0.3347461223602295, 2.9544389247894287]\n",
            "417: Train Loss: [1.8793339729309082, 0.2754090428352356, 1.6039249897003174] | Test Loss: [3.2916133403778076, 0.34001556038856506, 2.9515976905822754]\n",
            "418: Train Loss: [2.1744496822357178, 0.29052481055259705, 1.8839248418807983] | Test Loss: [3.285430431365967, 0.33596253395080566, 2.949467897415161]\n",
            "419: Train Loss: [1.9643118381500244, 0.32606127858161926, 1.6382505893707275] | Test Loss: [3.2759721279144287, 0.3305912911891937, 2.945380926132202]\n",
            "420: Train Loss: [2.0867652893066406, 0.27589768171310425, 1.8108675479888916] | Test Loss: [3.2527382373809814, 0.31792566180229187, 2.934812545776367]\n",
            "421: Train Loss: [2.3489227294921875, 0.37538817524909973, 1.9735344648361206] | Test Loss: [3.240074872970581, 0.3123118281364441, 2.927762985229492]\n",
            "422: Train Loss: [2.195517063140869, 0.2881760597229004, 1.9073408842086792] | Test Loss: [3.234778881072998, 0.30711472034454346, 2.927664279937744]\n",
            "423: Train Loss: [2.019788980484009, 0.27523618936538696, 1.7445528507232666] | Test Loss: [3.2329061031341553, 0.3028600513935089, 2.9300460815429688]\n",
            "424: Train Loss: [2.127436876296997, 0.2664338946342468, 1.8610029220581055] | Test Loss: [3.234105110168457, 0.2992137372493744, 2.93489146232605]\n",
            "425: Train Loss: [2.2885472774505615, 0.30694061517715454, 1.9816066026687622] | Test Loss: [3.238809108734131, 0.30043795704841614, 2.938371181488037]\n",
            "426: Train Loss: [2.421417474746704, 0.27066537737846375, 2.150752067565918] | Test Loss: [3.246166229248047, 0.30445367097854614, 2.9417126178741455]\n",
            "427: Train Loss: [1.8771811723709106, 0.22480466961860657, 1.6523765325546265] | Test Loss: [3.252889394760132, 0.30606207251548767, 2.9468274116516113]\n",
            "428: Train Loss: [2.178403615951538, 0.302385151386261, 1.8760185241699219] | Test Loss: [3.2655296325683594, 0.31017807126045227, 2.9553515911102295]\n",
            "429: Train Loss: [1.9058606624603271, 0.3519824743270874, 1.5538781881332397] | Test Loss: [3.280546188354492, 0.31977206468582153, 2.9607741832733154]\n",
            "430: Train Loss: [2.0907020568847656, 0.2865663468837738, 1.804135799407959] | Test Loss: [3.292607545852661, 0.329885333776474, 2.9627223014831543]\n",
            "431: Train Loss: [2.4192559719085693, 0.28244274854660034, 2.136813163757324] | Test Loss: [3.3003153800964355, 0.33850640058517456, 2.961808919906616]\n",
            "432: Train Loss: [2.3013625144958496, 0.2960326075553894, 2.0053298473358154] | Test Loss: [3.3095884323120117, 0.34530845284461975, 2.964279890060425]\n",
            "433: Train Loss: [1.9672735929489136, 0.24279561638832092, 1.7244781255722046] | Test Loss: [3.3156955242156982, 0.3469775319099426, 2.9687180519104004]\n",
            "434: Train Loss: [2.3264715671539307, 0.31335052847862244, 2.0131211280822754] | Test Loss: [3.3204543590545654, 0.3503517806529999, 2.970102548599243]\n",
            "435: Train Loss: [1.3327609300613403, 0.36123156547546387, 0.9715293645858765] | Test Loss: [3.324329137802124, 0.3566291332244873, 2.9677000045776367]\n",
            "436: Train Loss: [2.2585630416870117, 0.2867920994758606, 1.971771001815796] | Test Loss: [3.3229360580444336, 0.36048316955566406, 2.9624528884887695]\n",
            "437: Train Loss: [1.7506886720657349, 0.30782604217529297, 1.4428625106811523] | Test Loss: [3.319586753845215, 0.3621199131011963, 2.9574668407440186]\n",
            "438: Train Loss: [1.5867561101913452, 0.27460983395576477, 1.3121463060379028] | Test Loss: [3.3096935749053955, 0.3548303246498108, 2.9548633098602295]\n",
            "439: Train Loss: [1.294389247894287, 0.24645033478736877, 1.0479389429092407] | Test Loss: [3.2933096885681152, 0.33936676383018494, 2.9539430141448975]\n",
            "440: Train Loss: [2.2332451343536377, 0.2933208644390106, 1.9399243593215942] | Test Loss: [3.269212007522583, 0.3263094127178192, 2.9429025650024414]\n",
            "441: Train Loss: [2.00195050239563, 0.23654474318027496, 1.7654056549072266] | Test Loss: [3.246224880218506, 0.31096240878105164, 2.935262441635132]\n",
            "442: Train Loss: [2.339076280593872, 0.24780064821243286, 2.091275691986084] | Test Loss: [3.2224316596984863, 0.29343414306640625, 2.92899751663208]\n",
            "443: Train Loss: [1.6954128742218018, 0.3067980110645294, 1.3886148929595947] | Test Loss: [3.204108238220215, 0.28072696924209595, 2.9233813285827637]\n",
            "444: Train Loss: [2.4862074851989746, 0.30603283643722534, 2.1801745891571045] | Test Loss: [3.191155195236206, 0.27419015765190125, 2.9169650077819824]\n",
            "445: Train Loss: [2.047945976257324, 0.30493244528770447, 1.743013620376587] | Test Loss: [3.1824209690093994, 0.27160578966140747, 2.9108152389526367]\n",
            "446: Train Loss: [2.045185089111328, 0.3157248795032501, 1.7294601202011108] | Test Loss: [3.177081346511841, 0.2744731903076172, 2.9026081562042236]\n",
            "447: Train Loss: [2.20866322517395, 0.314998060464859, 1.8936651945114136] | Test Loss: [3.1780107021331787, 0.28073152899742126, 2.8972792625427246]\n",
            "448: Train Loss: [1.9929990768432617, 0.28233200311660767, 1.7106670141220093] | Test Loss: [3.1798622608184814, 0.28806543350219727, 2.891796827316284]\n",
            "449: Train Loss: [2.1979150772094727, 0.2788795232772827, 1.9190354347229004] | Test Loss: [3.1811695098876953, 0.2925172448158264, 2.8886523246765137]\n",
            "450: Train Loss: [2.112290620803833, 0.37472814321517944, 1.7375625371932983] | Test Loss: [3.194753646850586, 0.3051255941390991, 2.8896281719207764]\n",
            "451: Train Loss: [2.2850711345672607, 0.3170999586582184, 1.9679710865020752] | Test Loss: [3.214719295501709, 0.31596267223358154, 2.898756504058838]\n",
            "452: Train Loss: [1.8212004899978638, 0.30617091059684753, 1.5150295495986938] | Test Loss: [3.2285895347595215, 0.31937021017074585, 2.909219264984131]\n",
            "453: Train Loss: [2.434723377227783, 0.2997879385948181, 2.1349353790283203] | Test Loss: [3.240746259689331, 0.3161489963531494, 2.9245972633361816]\n",
            "454: Train Loss: [2.4315080642700195, 0.3353309631347656, 2.096177101135254] | Test Loss: [3.2434916496276855, 0.3084317147731781, 2.9350600242614746]\n",
            "455: Train Loss: [2.242076873779297, 0.35190755128860474, 1.890169382095337] | Test Loss: [3.236252546310425, 0.3025168478488922, 2.9337356090545654]\n",
            "456: Train Loss: [2.388932704925537, 0.26199185848236084, 2.1269407272338867] | Test Loss: [3.2236838340759277, 0.29144981503486633, 2.932234048843384]\n",
            "457: Train Loss: [2.064081907272339, 0.26580044627189636, 1.7982814311981201] | Test Loss: [3.2069687843322754, 0.2795514762401581, 2.927417278289795]\n",
            "458: Train Loss: [1.9406543970108032, 0.2966839075088501, 1.6439704895019531] | Test Loss: [3.1906003952026367, 0.2685645818710327, 2.9220359325408936]\n",
            "459: Train Loss: [1.9282102584838867, 0.2798103094100952, 1.6483999490737915] | Test Loss: [3.173633337020874, 0.2598762810230255, 2.913757085800171]\n",
            "460: Train Loss: [2.0751216411590576, 0.3202422261238098, 1.754879355430603] | Test Loss: [3.1635260581970215, 0.2567180395126343, 2.9068078994750977]\n",
            "461: Train Loss: [2.1244828701019287, 0.30006808042526245, 1.824414849281311] | Test Loss: [3.1597909927368164, 0.2558292746543884, 2.903961658477783]\n",
            "462: Train Loss: [1.7984098196029663, 0.28894999623298645, 1.5094598531723022] | Test Loss: [3.1599442958831787, 0.2601037323474884, 2.8998405933380127]\n",
            "463: Train Loss: [1.9140980243682861, 0.2761276662349701, 1.6379703283309937] | Test Loss: [3.1563830375671387, 0.2640523612499237, 2.8923306465148926]\n",
            "464: Train Loss: [2.40779709815979, 0.2844989001750946, 2.123298168182373] | Test Loss: [3.15675950050354, 0.26973167061805725, 2.8870277404785156]\n",
            "465: Train Loss: [1.646154761314392, 0.35328856110572815, 1.2928662300109863] | Test Loss: [3.161933422088623, 0.2798546552658081, 2.8820788860321045]\n",
            "466: Train Loss: [2.192833185195923, 0.2322721928358078, 1.9605610370635986] | Test Loss: [3.1656219959259033, 0.2862807810306549, 2.8793411254882812]\n",
            "467: Train Loss: [1.9423835277557373, 0.317070335149765, 1.62531316280365] | Test Loss: [3.1733956336975098, 0.2922675311565399, 2.8811280727386475]\n",
            "468: Train Loss: [2.079923152923584, 0.25565606355667114, 1.824267029762268] | Test Loss: [3.183058977127075, 0.295148640871048, 2.8879103660583496]\n",
            "469: Train Loss: [1.97735595703125, 0.2966901361942291, 1.6806658506393433] | Test Loss: [3.1909570693969727, 0.2949597239494324, 2.8959972858428955]\n",
            "470: Train Loss: [1.9519058465957642, 0.30465376377105713, 1.647252082824707] | Test Loss: [3.20078706741333, 0.2950250208377838, 2.905761957168579]\n",
            "471: Train Loss: [2.0212466716766357, 0.26821792125701904, 1.7530287504196167] | Test Loss: [3.200383424758911, 0.29042530059814453, 2.9099581241607666]\n",
            "472: Train Loss: [1.9623541831970215, 0.31205374002456665, 1.65030038356781] | Test Loss: [3.1939280033111572, 0.2846240997314453, 2.909303903579712]\n",
            "473: Train Loss: [2.2274081707000732, 0.2824763357639313, 1.9449318647384644] | Test Loss: [3.1804544925689697, 0.27825644612312317, 2.902198076248169]\n",
            "474: Train Loss: [1.9768410921096802, 0.2776814103126526, 1.6991595029830933] | Test Loss: [3.1674013137817383, 0.27372777462005615, 2.8936734199523926]\n",
            "475: Train Loss: [1.9926350116729736, 0.2623026371002197, 1.730332374572754] | Test Loss: [3.1532130241394043, 0.2694811224937439, 2.8837318420410156]\n",
            "476: Train Loss: [2.115208148956299, 0.37318292260169983, 1.7420252561569214] | Test Loss: [3.143954038619995, 0.27186959981918335, 2.872084379196167]\n",
            "477: Train Loss: [1.93539297580719, 0.30984440445899963, 1.6255486011505127] | Test Loss: [3.1424877643585205, 0.27494096755981445, 2.867546796798706]\n",
            "478: Train Loss: [2.26473331451416, 0.3408795893192291, 1.9238536357879639] | Test Loss: [3.151398181915283, 0.2846464514732361, 2.8667516708374023]\n",
            "479: Train Loss: [2.1222269535064697, 0.2602181136608124, 1.8620089292526245] | Test Loss: [3.155367851257324, 0.28978875279426575, 2.865579128265381]\n",
            "480: Train Loss: [1.7794286012649536, 0.29188621044158936, 1.4875423908233643] | Test Loss: [3.162766218185425, 0.29253581166267395, 2.8702304363250732]\n",
            "481: Train Loss: [2.1979453563690186, 0.2641727030277252, 1.9337726831436157] | Test Loss: [3.167998790740967, 0.29304254055023193, 2.8749563694000244]\n",
            "482: Train Loss: [2.4134387969970703, 0.3960476517677307, 2.0173912048339844] | Test Loss: [3.1888599395751953, 0.30148011445999146, 2.8873798847198486]\n",
            "483: Train Loss: [2.6939196586608887, 0.39701348543167114, 2.2969062328338623] | Test Loss: [3.2210702896118164, 0.31622976064682007, 2.9048404693603516]\n",
            "484: Train Loss: [1.705030918121338, 0.2930261492729187, 1.4120047092437744] | Test Loss: [3.243586540222168, 0.3206956386566162, 2.9228909015655518]\n",
            "485: Train Loss: [2.3973922729492188, 0.2646275460720062, 2.1327648162841797] | Test Loss: [3.2510178089141846, 0.32051244378089905, 2.9305052757263184]\n",
            "486: Train Loss: [2.194336414337158, 0.2773558497428894, 1.916980504989624] | Test Loss: [3.24343204498291, 0.3134705722332001, 2.9299614429473877]\n",
            "487: Train Loss: [2.1265311241149902, 0.2816389203071594, 1.8448922634124756] | Test Loss: [3.23152494430542, 0.3087064027786255, 2.922818660736084]\n",
            "488: Train Loss: [2.2230451107025146, 0.2803657352924347, 1.9426794052124023] | Test Loss: [3.218475341796875, 0.3018612265586853, 2.916614055633545]\n",
            "489: Train Loss: [2.185447931289673, 0.29171380400657654, 1.8937342166900635] | Test Loss: [3.206866979598999, 0.2962247431278229, 2.910642147064209]\n",
            "490: Train Loss: [1.6987253427505493, 0.320449560880661, 1.378275752067566] | Test Loss: [3.202497959136963, 0.2963413596153259, 2.906156539916992]\n",
            "491: Train Loss: [2.119342803955078, 0.32739928364753723, 1.7919434309005737] | Test Loss: [3.202397584915161, 0.30115294456481934, 2.901244640350342]\n",
            "492: Train Loss: [2.4262852668762207, 0.2917673885822296, 2.1345179080963135] | Test Loss: [3.205026149749756, 0.3071025609970093, 2.897923707962036]\n",
            "493: Train Loss: [2.06817626953125, 0.28196555376052856, 1.7862107753753662] | Test Loss: [3.2104837894439697, 0.3127969205379486, 2.8976869583129883]\n",
            "494: Train Loss: [2.1690094470977783, 0.25454577803611755, 1.914463758468628] | Test Loss: [3.2213518619537354, 0.3197800815105438, 2.901571750640869]\n",
            "495: Train Loss: [2.2554569244384766, 0.3093591034412384, 1.9460978507995605] | Test Loss: [3.2353079319000244, 0.3281019330024719, 2.9072060585021973]\n",
            "496: Train Loss: [1.7132951021194458, 0.2585388720035553, 1.454756259918213] | Test Loss: [3.2438156604766846, 0.3313213884830475, 2.91249418258667]\n",
            "497: Train Loss: [2.3736424446105957, 0.3122328817844391, 2.0614094734191895] | Test Loss: [3.255945920944214, 0.33852896094322205, 2.917417049407959]\n",
            "498: Train Loss: [2.2483646869659424, 0.27059417963027954, 1.9777705669403076] | Test Loss: [3.266350030899048, 0.3413465917110443, 2.9250035285949707]\n",
            "499: Train Loss: [2.1946873664855957, 0.31700971722602844, 1.8776776790618896] | Test Loss: [3.268845319747925, 0.34161025285720825, 2.9272351264953613]\n",
            "500: Train Loss: [2.5179684162139893, 0.296664834022522, 2.221303701400757] | Test Loss: [3.2822937965393066, 0.3469310402870178, 2.9353628158569336]\n",
            "501: Train Loss: [1.9269371032714844, 0.4127691388130188, 1.5141680240631104] | Test Loss: [3.303473711013794, 0.35555222630500793, 2.9479215145111084]\n",
            "502: Train Loss: [1.8896117210388184, 0.35147956013679504, 1.5381321907043457] | Test Loss: [3.325901746749878, 0.3656039535999298, 2.9602978229522705]\n",
            "503: Train Loss: [1.9213855266571045, 0.2759627401828766, 1.6454228162765503] | Test Loss: [3.3361377716064453, 0.36932528018951416, 2.9668123722076416]\n",
            "504: Train Loss: [2.3370203971862793, 0.37845325469970703, 1.9585671424865723] | Test Loss: [3.344271183013916, 0.3732065260410309, 2.971064567565918]\n",
            "505: Train Loss: [2.2350330352783203, 0.3129594922065735, 1.922073483467102] | Test Loss: [3.3417575359344482, 0.3706706464290619, 2.9710869789123535]\n",
            "506: Train Loss: [2.606236696243286, 0.3292969763278961, 2.276939630508423] | Test Loss: [3.3344976902008057, 0.36533015966415405, 2.969167470932007]\n",
            "507: Train Loss: [1.6227298974990845, 0.3476332724094391, 1.2750966548919678] | Test Loss: [3.324791193008423, 0.35735633969306946, 2.967434883117676]\n",
            "508: Train Loss: [2.0989038944244385, 0.27989521622657776, 1.8190085887908936] | Test Loss: [3.303652048110962, 0.34193316102027893, 2.961718797683716]\n",
            "509: Train Loss: [1.1426551342010498, 0.29948434233665466, 0.8431707620620728] | Test Loss: [3.283334732055664, 0.3266521692276001, 2.9566826820373535]\n",
            "510: Train Loss: [2.398109197616577, 0.2963240444660187, 2.101785182952881] | Test Loss: [3.263878345489502, 0.31249719858169556, 2.951381206512451]\n",
            "511: Train Loss: [1.7347520589828491, 0.27464768290519714, 1.4601043462753296] | Test Loss: [3.243422746658325, 0.30034464597702026, 2.94307804107666]\n",
            "512: Train Loss: [2.1195948123931885, 0.2883405089378357, 1.8312543630599976] | Test Loss: [3.2204105854034424, 0.29004451632499695, 2.930366039276123]\n",
            "513: Train Loss: [2.363922119140625, 0.30224406719207764, 2.061677932739258] | Test Loss: [3.2003772258758545, 0.28473490476608276, 2.915642261505127]\n",
            "514: Train Loss: [1.1510854959487915, 0.35627928376197815, 0.7948062419891357] | Test Loss: [3.188965082168579, 0.28479212522506714, 2.904172897338867]\n",
            "515: Train Loss: [2.3305301666259766, 0.29727497696876526, 2.033255100250244] | Test Loss: [3.189755916595459, 0.2871524393558502, 2.9026033878326416]\n",
            "516: Train Loss: [2.034283399581909, 0.2858124077320099, 1.7484710216522217] | Test Loss: [3.1895549297332764, 0.2921338975429535, 2.89742112159729]\n",
            "517: Train Loss: [2.2504971027374268, 0.3101591169834137, 1.9403380155563354] | Test Loss: [3.1906509399414062, 0.2979990243911743, 2.8926520347595215]\n",
            "518: Train Loss: [1.996409296989441, 0.3400973975658417, 1.6563118696212769] | Test Loss: [3.1908490657806396, 0.30400824546813965, 2.8868408203125]\n",
            "519: Train Loss: [1.9875359535217285, 0.28245964646339417, 1.7050763368606567] | Test Loss: [3.1886515617370605, 0.3089057207107544, 2.8797459602355957]\n",
            "520: Train Loss: [0.5712568163871765, 0.3494594395160675, 0.22179736196994781] | Test Loss: [3.192039728164673, 0.31769615411758423, 2.8743436336517334]\n",
            "521: Train Loss: [2.0110459327697754, 0.30790233612060547, 1.7031437158584595] | Test Loss: [3.190094232559204, 0.3214581310749054, 2.868636131286621]\n",
            "522: Train Loss: [1.9705173969268799, 0.3080660104751587, 1.6624513864517212] | Test Loss: [3.18163800239563, 0.32287469506263733, 2.8587632179260254]\n",
            "523: Train Loss: [2.1100234985351562, 0.28110289573669434, 1.828920602798462] | Test Loss: [3.1722774505615234, 0.31826266646385193, 2.8540148735046387]\n",
            "524: Train Loss: [2.1605522632598877, 0.27095937728881836, 1.8895928859710693] | Test Loss: [3.1606953144073486, 0.30686551332473755, 2.853829860687256]\n",
            "525: Train Loss: [2.732963800430298, 0.40225476026535034, 2.3307089805603027] | Test Loss: [3.163973331451416, 0.3074033260345459, 2.85657000541687]\n",
            "Epoch 3\n",
            "0: Train Loss: [2.0163800716400146, 0.29403048753738403, 1.7223496437072754] | Test Loss: [3.1720480918884277, 0.3042553961277008, 2.8677926063537598]\n",
            "1: Train Loss: [2.185577869415283, 0.2729090750217438, 1.9126688241958618] | Test Loss: [3.17478346824646, 0.2970125675201416, 2.8777709007263184]\n",
            "2: Train Loss: [1.661171317100525, 0.24761341512203217, 1.4135578870773315] | Test Loss: [3.1717891693115234, 0.28419193625450134, 2.8875973224639893]\n",
            "3: Train Loss: [2.017106056213379, 0.34102171659469604, 1.676084280014038] | Test Loss: [3.1712520122528076, 0.278418630361557, 2.8928334712982178]\n",
            "4: Train Loss: [1.75962233543396, 0.2342030107975006, 1.5254193544387817] | Test Loss: [3.168139934539795, 0.27002090215682983, 2.8981189727783203]\n",
            "5: Train Loss: [2.214677333831787, 0.2924036979675293, 1.9222737550735474] | Test Loss: [3.1699304580688477, 0.2644672989845276, 2.905463218688965]\n",
            "6: Train Loss: [1.9356321096420288, 0.2693568766117096, 1.6662752628326416] | Test Loss: [3.1809468269348145, 0.26141127943992615, 2.9195356369018555]\n",
            "7: Train Loss: [1.73538076877594, 0.2690834701061249, 1.4662972688674927] | Test Loss: [3.1917245388031006, 0.25860923528671265, 2.933115243911743]\n",
            "8: Train Loss: [2.3381903171539307, 0.30123141407966614, 2.036958932876587] | Test Loss: [3.2027320861816406, 0.2565075755119324, 2.9462244510650635]\n",
            "9: Train Loss: [2.133073091506958, 0.27495133876800537, 1.8581217527389526] | Test Loss: [3.2134764194488525, 0.2545444965362549, 2.9589319229125977]\n",
            "10: Train Loss: [2.069648504257202, 0.27938225865364075, 1.7902661561965942] | Test Loss: [3.207756757736206, 0.25210040807724, 2.9556562900543213]\n",
            "11: Train Loss: [1.488417625427246, 0.3192974925041199, 1.169120192527771] | Test Loss: [3.204120397567749, 0.25387027859687805, 2.9502501487731934]\n",
            "12: Train Loss: [1.8292776346206665, 0.24853016436100006, 1.5807474851608276] | Test Loss: [3.199810266494751, 0.25457075238227844, 2.945239543914795]\n",
            "13: Train Loss: [2.097430944442749, 0.3002135455608368, 1.7972174882888794] | Test Loss: [3.197618246078491, 0.25523319840431213, 2.942384958267212]\n",
            "14: Train Loss: [1.9534461498260498, 0.34747323393821716, 1.6059728860855103] | Test Loss: [3.200862407684326, 0.2602583169937134, 2.9406039714813232]\n",
            "15: Train Loss: [2.1011435985565186, 0.2827564775943756, 1.8183871507644653] | Test Loss: [3.2044131755828857, 0.2626521587371826, 2.941761016845703]\n",
            "16: Train Loss: [2.323467493057251, 0.2630954086780548, 2.0603721141815186] | Test Loss: [3.2080776691436768, 0.26321008801460266, 2.9448676109313965]\n",
            "17: Train Loss: [2.2696969509124756, 0.2950856685638428, 1.9746112823486328] | Test Loss: [3.2180285453796387, 0.26420238614082336, 2.9538261890411377]\n",
            "18: Train Loss: [1.7161892652511597, 0.3233116865158081, 1.3928776979446411] | Test Loss: [3.2315664291381836, 0.26849573850631714, 2.9630706310272217]\n",
            "19: Train Loss: [2.0847818851470947, 0.3172318637371063, 1.7675502300262451] | Test Loss: [3.243311643600464, 0.2727413773536682, 2.9705703258514404]\n",
            "20: Train Loss: [1.9481110572814941, 0.30336543917655945, 1.6447455883026123] | Test Loss: [3.2508087158203125, 0.27691277861595154, 2.973896026611328]\n",
            "21: Train Loss: [1.7509710788726807, 0.27965664863586426, 1.4713144302368164] | Test Loss: [3.2567927837371826, 0.27784833312034607, 2.9789445400238037]\n",
            "22: Train Loss: [2.0577704906463623, 0.25216108560562134, 1.8056093454360962] | Test Loss: [3.2580816745758057, 0.27686506509780884, 2.9812166690826416]\n",
            "23: Train Loss: [2.077505111694336, 0.28201642632484436, 1.795488715171814] | Test Loss: [3.259185552597046, 0.27643316984176636, 2.9827523231506348]\n",
            "24: Train Loss: [2.041229248046875, 0.29513317346572876, 1.746096134185791] | Test Loss: [3.259416103363037, 0.28046372532844543, 2.978952407836914]\n",
            "25: Train Loss: [1.0923194885253906, 0.25994542241096497, 0.8323740363121033] | Test Loss: [3.2618932723999023, 0.28258174657821655, 2.979311466217041]\n",
            "26: Train Loss: [2.057816505432129, 0.2874000072479248, 1.7704163789749146] | Test Loss: [3.2701053619384766, 0.28861844539642334, 2.9814867973327637]\n",
            "27: Train Loss: [1.7530689239501953, 0.21974550187587738, 1.5333234071731567] | Test Loss: [3.271381378173828, 0.2878110706806183, 2.9835703372955322]\n",
            "28: Train Loss: [1.5223264694213867, 0.27873674035072327, 1.2435897588729858] | Test Loss: [3.274160385131836, 0.28617197275161743, 2.9879884719848633]\n",
            "29: Train Loss: [1.9024343490600586, 0.29820913076400757, 1.6042250394821167] | Test Loss: [3.2805957794189453, 0.28857892751693726, 2.9920167922973633]\n",
            "30: Train Loss: [2.155862808227539, 0.28994372487068176, 1.8659189939498901] | Test Loss: [3.2966063022613525, 0.2927258312702179, 3.003880500793457]\n",
            "31: Train Loss: [2.280813694000244, 0.30096861720085144, 1.9798450469970703] | Test Loss: [3.3175368309020996, 0.30197715759277344, 3.015559673309326]\n",
            "32: Train Loss: [2.2254436016082764, 0.3193190395832062, 1.906124472618103] | Test Loss: [3.3388962745666504, 0.31528240442276, 3.023613929748535]\n",
            "33: Train Loss: [2.141826629638672, 0.29433029890060425, 1.8474963903427124] | Test Loss: [3.3525655269622803, 0.3234223425388336, 3.0291430950164795]\n",
            "34: Train Loss: [1.8892581462860107, 0.2624351382255554, 1.6268230676651] | Test Loss: [3.350775957107544, 0.32583585381507874, 3.024940013885498]\n",
            "35: Train Loss: [1.8786706924438477, 0.2571730613708496, 1.621497631072998] | Test Loss: [3.3336737155914307, 0.3237026631832123, 3.0099711418151855]\n",
            "36: Train Loss: [2.040008544921875, 0.33586424589157104, 1.7041442394256592] | Test Loss: [3.313462972640991, 0.319715678691864, 2.9937472343444824]\n",
            "37: Train Loss: [2.1943254470825195, 0.2256661355495453, 1.9686592817306519] | Test Loss: [3.2818570137023926, 0.3097621500492096, 2.972094774246216]\n",
            "38: Train Loss: [1.8848466873168945, 0.24829477071762085, 1.636551856994629] | Test Loss: [3.2431740760803223, 0.2940940260887146, 2.949079990386963]\n",
            "39: Train Loss: [1.645545244216919, 0.265765905380249, 1.37977933883667] | Test Loss: [3.2098944187164307, 0.2792952358722687, 2.9305992126464844]\n",
            "40: Train Loss: [2.131741523742676, 0.3010711669921875, 1.8306702375411987] | Test Loss: [3.1789214611053467, 0.2671547532081604, 2.911766767501831]\n",
            "41: Train Loss: [1.7922276258468628, 0.2327231466770172, 1.559504508972168] | Test Loss: [3.1461329460144043, 0.2563491761684418, 2.8897838592529297]\n",
            "42: Train Loss: [2.6276724338531494, 0.34967461228370667, 2.2779977321624756] | Test Loss: [3.1282005310058594, 0.25213342905044556, 2.8760671615600586]\n",
            "43: Train Loss: [2.041062593460083, 0.28341415524482727, 1.7576485872268677] | Test Loss: [3.1193230152130127, 0.252019464969635, 2.8673036098480225]\n",
            "44: Train Loss: [2.1755051612854004, 0.27046653628349304, 1.9050387144088745] | Test Loss: [3.1121022701263428, 0.25518450140953064, 2.8569178581237793]\n",
            "45: Train Loss: [2.048983097076416, 0.2690674066543579, 1.7799158096313477] | Test Loss: [3.114856481552124, 0.2608187198638916, 2.8540377616882324]\n",
            "46: Train Loss: [1.8039987087249756, 0.37296515703201294, 1.4310334920883179] | Test Loss: [3.1243534088134766, 0.27539685368537903, 2.84895658493042]\n",
            "47: Train Loss: [2.055853843688965, 0.2824508249759674, 1.7734030485153198] | Test Loss: [3.1358044147491455, 0.29009631276130676, 2.845708131790161]\n",
            "48: Train Loss: [2.441786289215088, 0.3500198423862457, 2.091766357421875] | Test Loss: [3.1661198139190674, 0.3168061673641205, 2.849313735961914]\n",
            "49: Train Loss: [2.166349411010742, 0.3513629138469696, 1.8149864673614502] | Test Loss: [3.1953799724578857, 0.3367655873298645, 2.858614444732666]\n",
            "50: Train Loss: [2.215799570083618, 0.29030880331993103, 1.9254907369613647] | Test Loss: [3.2045485973358154, 0.33640000224113464, 2.8681485652923584]\n",
            "51: Train Loss: [2.0149707794189453, 0.2953450381755829, 1.71962571144104] | Test Loss: [3.2021877765655518, 0.3221491277217865, 2.8800387382507324]\n",
            "52: Train Loss: [2.2729837894439697, 0.3228176534175873, 1.9501662254333496] | Test Loss: [3.195868730545044, 0.30648675560951233, 2.8893818855285645]\n",
            "53: Train Loss: [2.0531165599823, 0.31833285093307495, 1.73478364944458] | Test Loss: [3.1965677738189697, 0.2959497272968292, 2.900618076324463]\n",
            "54: Train Loss: [2.1316771507263184, 0.24549299478530884, 1.8861842155456543] | Test Loss: [3.187922954559326, 0.2788315415382385, 2.9090914726257324]\n",
            "55: Train Loss: [1.9381619691848755, 0.2956857979297638, 1.642476201057434] | Test Loss: [3.184204578399658, 0.26637282967567444, 2.9178316593170166]\n",
            "56: Train Loss: [1.1922305822372437, 0.2981739640235901, 0.8940566182136536] | Test Loss: [3.182020902633667, 0.25834113359451294, 2.923679828643799]\n",
            "57: Train Loss: [2.0808842182159424, 0.30092212557792664, 1.7799620628356934] | Test Loss: [3.1843743324279785, 0.25445523858070374, 2.9299190044403076]\n",
            "58: Train Loss: [2.073263645172119, 0.32773852348327637, 1.7455251216888428] | Test Loss: [3.194559097290039, 0.25871649384498596, 2.935842514038086]\n",
            "59: Train Loss: [1.834535837173462, 0.23252901434898376, 1.6020067930221558] | Test Loss: [3.201185941696167, 0.2620165944099426, 2.939169406890869]\n",
            "60: Train Loss: [2.1794838905334473, 0.2476758360862732, 1.9318079948425293] | Test Loss: [3.2032806873321533, 0.26530224084854126, 2.937978506088257]\n",
            "61: Train Loss: [2.3862266540527344, 0.2858022451400757, 2.100424289703369] | Test Loss: [3.2049686908721924, 0.2685859203338623, 2.93638277053833]\n",
            "62: Train Loss: [1.6641385555267334, 0.27396148443222046, 1.3901770114898682] | Test Loss: [3.203355550765991, 0.2717660963535309, 2.931589365005493]\n",
            "63: Train Loss: [2.290147066116333, 0.2753393352031708, 2.01480770111084] | Test Loss: [3.2086164951324463, 0.2724568545818329, 2.936159610748291]\n",
            "64: Train Loss: [1.6498671770095825, 0.28176310658454895, 1.368104100227356] | Test Loss: [3.2125608921051025, 0.2753450870513916, 2.937215805053711]\n",
            "65: Train Loss: [2.1842517852783203, 0.2948364019393921, 1.8894155025482178] | Test Loss: [3.2080795764923096, 0.28264784812927246, 2.925431728363037]\n",
            "66: Train Loss: [2.1757075786590576, 0.2696242034435272, 1.906083345413208] | Test Loss: [3.203695058822632, 0.2891583740711212, 2.914536714553833]\n",
            "67: Train Loss: [2.077418804168701, 0.27319517731666565, 1.804223656654358] | Test Loss: [3.197446823120117, 0.29606473445892334, 2.9013819694519043]\n",
            "68: Train Loss: [1.9628666639328003, 0.29573795199394226, 1.6671286821365356] | Test Loss: [3.18624210357666, 0.3034168779850006, 2.8828251361846924]\n",
            "69: Train Loss: [1.3071107864379883, 0.29843106865882874, 1.008679747581482] | Test Loss: [3.178997278213501, 0.31289321184158325, 2.8661041259765625]\n",
            "70: Train Loss: [1.882971167564392, 0.3080250024795532, 1.5749461650848389] | Test Loss: [3.1777522563934326, 0.32165077328681946, 2.8561015129089355]\n",
            "71: Train Loss: [1.4724608659744263, 0.29209136962890625, 1.18036949634552] | Test Loss: [3.1758224964141846, 0.32933953404426575, 2.846482992172241]\n",
            "72: Train Loss: [1.8513599634170532, 0.22021715342998505, 1.6311428546905518] | Test Loss: [3.1675336360931396, 0.3306853473186493, 2.836848258972168]\n",
            "73: Train Loss: [1.8970063924789429, 0.32046613097190857, 1.576540231704712] | Test Loss: [3.162116050720215, 0.33186230063438416, 2.830253839492798]\n",
            "74: Train Loss: [2.3160088062286377, 0.2969112694263458, 2.0190975666046143] | Test Loss: [3.1636085510253906, 0.336026132106781, 2.827582359313965]\n",
            "75: Train Loss: [1.9095484018325806, 0.26595771312713623, 1.6435906887054443] | Test Loss: [3.164212465286255, 0.3365807831287384, 2.827631711959839]\n",
            "76: Train Loss: [1.7639731168746948, 0.279739648103714, 1.4842334985733032] | Test Loss: [3.1699748039245605, 0.3378501534461975, 2.832124710083008]\n",
            "77: Train Loss: [2.1492083072662354, 0.29328981041908264, 1.8559184074401855] | Test Loss: [3.1745691299438477, 0.3388383388519287, 2.835730791091919]\n",
            "78: Train Loss: [1.6725304126739502, 0.25908949971199036, 1.4134409427642822] | Test Loss: [3.180609703063965, 0.3378928303718567, 2.842716932296753]\n",
            "79: Train Loss: [2.1519463062286377, 0.29999759793281555, 1.851948618888855] | Test Loss: [3.1876778602600098, 0.33931073546409607, 2.848367214202881]\n",
            "80: Train Loss: [2.1617980003356934, 0.30583488941192627, 1.855963110923767] | Test Loss: [3.1912143230438232, 0.34459444880485535, 2.8466198444366455]\n",
            "81: Train Loss: [1.7461826801300049, 0.2473912537097931, 1.4987914562225342] | Test Loss: [3.1951448917388916, 0.3487374186515808, 2.846407413482666]\n",
            "82: Train Loss: [2.1311051845550537, 0.2687561810016632, 1.8623489141464233] | Test Loss: [3.194793701171875, 0.3482397496700287, 2.8465540409088135]\n",
            "83: Train Loss: [1.9950283765792847, 0.31263914704322815, 1.682389259338379] | Test Loss: [3.2047975063323975, 0.3544886112213135, 2.850308895111084]\n",
            "84: Train Loss: [2.37113881111145, 0.2757260203361511, 2.0954127311706543] | Test Loss: [3.2131521701812744, 0.36087697744369507, 2.8522751331329346]\n",
            "85: Train Loss: [2.060342788696289, 0.3210357427597046, 1.739307165145874] | Test Loss: [3.2249650955200195, 0.36980071663856506, 2.8551642894744873]\n",
            "86: Train Loss: [2.1250481605529785, 0.22977444529533386, 1.8952736854553223] | Test Loss: [3.23136568069458, 0.3693561553955078, 2.8620095252990723]\n",
            "87: Train Loss: [2.1293530464172363, 0.355480819940567, 1.7738721370697021] | Test Loss: [3.2461695671081543, 0.372405469417572, 2.8737640380859375]\n",
            "88: Train Loss: [2.0109505653381348, 0.25716233253479004, 1.7537882328033447] | Test Loss: [3.252790927886963, 0.3676936626434326, 2.8850972652435303]\n",
            "89: Train Loss: [1.7775578498840332, 0.2986612915992737, 1.4788964986801147] | Test Loss: [3.25028133392334, 0.36297836899757385, 2.887302875518799]\n",
            "90: Train Loss: [1.8137789964675903, 0.2840122878551483, 1.5297666788101196] | Test Loss: [3.2479805946350098, 0.360113263130188, 2.8878674507141113]\n",
            "91: Train Loss: [2.4376025199890137, 0.32588887214660645, 2.1117138862609863] | Test Loss: [3.243643045425415, 0.35593611001968384, 2.887706995010376]\n",
            "92: Train Loss: [2.0599520206451416, 0.3167645037174225, 1.7431875467300415] | Test Loss: [3.2353827953338623, 0.3498202860355377, 2.8855624198913574]\n",
            "93: Train Loss: [2.1571056842803955, 0.2386522740125656, 1.918453335762024] | Test Loss: [3.213630437850952, 0.3385348916053772, 2.8750956058502197]\n",
            "94: Train Loss: [2.0746145248413086, 0.30480504035949707, 1.7698094844818115] | Test Loss: [3.194796323776245, 0.3304755389690399, 2.864320755004883]\n",
            "95: Train Loss: [1.779921054840088, 0.29897168278694153, 1.4809494018554688] | Test Loss: [3.1757938861846924, 0.3228442668914795, 2.852949619293213]\n",
            "96: Train Loss: [1.7856656312942505, 0.26205411553382874, 1.5236115455627441] | Test Loss: [3.161454439163208, 0.3140900433063507, 2.8473644256591797]\n",
            "97: Train Loss: [2.078735828399658, 0.2206728607416153, 1.8580628633499146] | Test Loss: [3.1478612422943115, 0.3043977618217468, 2.84346342086792]\n",
            "98: Train Loss: [2.0841689109802246, 0.3311580717563629, 1.753010869026184] | Test Loss: [3.1409435272216797, 0.2998878061771393, 2.8410556316375732]\n",
            "99: Train Loss: [2.0090324878692627, 0.240421861410141, 1.7686105966567993] | Test Loss: [3.1356606483459473, 0.29593467712402344, 2.839725971221924]\n",
            "100: Train Loss: [1.742499589920044, 0.26300159096717834, 1.4794981479644775] | Test Loss: [3.1340603828430176, 0.2948320508003235, 2.839228391647339]\n",
            "101: Train Loss: [2.3309781551361084, 0.3167315423488617, 2.014246702194214] | Test Loss: [3.134343147277832, 0.29741454124450684, 2.836928606033325]\n",
            "102: Train Loss: [1.7192965745925903, 0.2468642294406891, 1.4724323749542236] | Test Loss: [3.1311018466949463, 0.29745572805404663, 2.833646059036255]\n",
            "103: Train Loss: [1.8626995086669922, 0.33885085582733154, 1.5238486528396606] | Test Loss: [3.1300666332244873, 0.3030192255973816, 2.827047348022461]\n",
            "104: Train Loss: [2.1370720863342285, 0.3494317829608917, 1.7876403331756592] | Test Loss: [3.1286158561706543, 0.31202614307403564, 2.816589832305908]\n",
            "105: Train Loss: [1.924163818359375, 0.23029540479183197, 1.6938683986663818] | Test Loss: [3.1195614337921143, 0.3123490512371063, 2.8072123527526855]\n",
            "106: Train Loss: [1.679591178894043, 0.3295857310295105, 1.3500055074691772] | Test Loss: [3.1174674034118652, 0.3170073330402374, 2.80046010017395]\n",
            "107: Train Loss: [1.3225619792938232, 0.2894192636013031, 1.0331425666809082] | Test Loss: [3.1149966716766357, 0.3224288821220398, 2.792567729949951]\n",
            "108: Train Loss: [2.3566718101501465, 0.2451879233121872, 2.1114838123321533] | Test Loss: [3.1081130504608154, 0.3174747824668884, 2.7906382083892822]\n",
            "109: Train Loss: [1.9208158254623413, 0.2933499813079834, 1.627465844154358] | Test Loss: [3.103739023208618, 0.31372132897377014, 2.790017604827881]\n",
            "110: Train Loss: [2.015040397644043, 0.26439088582992554, 1.7506495714187622] | Test Loss: [3.099691152572632, 0.3063354194164276, 2.793355703353882]\n",
            "111: Train Loss: [2.000833511352539, 0.2550226151943207, 1.745810866355896] | Test Loss: [3.092554807662964, 0.29371193051338196, 2.7988429069519043]\n",
            "112: Train Loss: [1.6165642738342285, 0.25076454877853394, 1.3657997846603394] | Test Loss: [3.0865278244018555, 0.28413718938827515, 2.8023905754089355]\n",
            "113: Train Loss: [0.9838740825653076, 0.2715934216976166, 0.7122806906700134] | Test Loss: [3.084475517272949, 0.27894270420074463, 2.805532932281494]\n",
            "114: Train Loss: [1.999678611755371, 0.32027262449264526, 1.679405927658081] | Test Loss: [3.087418556213379, 0.2796075940132141, 2.8078110218048096]\n",
            "115: Train Loss: [1.9036824703216553, 0.3192991316318512, 1.5843833684921265] | Test Loss: [3.0926241874694824, 0.28375837206840515, 2.808865785598755]\n",
            "116: Train Loss: [1.8488948345184326, 0.39769741892814636, 1.4511973857879639] | Test Loss: [3.1038565635681152, 0.29780668020248413, 2.8060498237609863]\n",
            "117: Train Loss: [2.1623690128326416, 0.29911860823631287, 1.8632503747940063] | Test Loss: [3.1211018562316895, 0.3180035650730133, 2.803098201751709]\n",
            "118: Train Loss: [2.0280489921569824, 0.26973477005958557, 1.7583142518997192] | Test Loss: [3.1352434158325195, 0.33223384618759155, 2.803009510040283]\n",
            "119: Train Loss: [2.017197370529175, 0.3368668854236603, 1.6803303956985474] | Test Loss: [3.144655466079712, 0.3439891040325165, 2.800666332244873]\n",
            "120: Train Loss: [1.773086667060852, 0.2729698717594147, 1.5001168251037598] | Test Loss: [3.1463053226470947, 0.3461768925189972, 2.80012845993042]\n",
            "121: Train Loss: [2.0771307945251465, 0.30138495564460754, 1.7757458686828613] | Test Loss: [3.1455187797546387, 0.345599502325058, 2.799919366836548]\n",
            "122: Train Loss: [2.135796546936035, 0.28146451711654663, 1.8543320894241333] | Test Loss: [3.1433827877044678, 0.3389382064342499, 2.8044445514678955]\n",
            "123: Train Loss: [2.158285617828369, 0.2840089499950409, 1.8742766380310059] | Test Loss: [3.1413094997406006, 0.32994407415390015, 2.8113653659820557]\n",
            "124: Train Loss: [2.036099672317505, 0.2706235945224762, 1.7654759883880615] | Test Loss: [3.1346595287323, 0.3187269866466522, 2.815932512283325]\n",
            "125: Train Loss: [2.083146572113037, 0.3374088704586029, 1.7457376718521118] | Test Loss: [3.1235885620117188, 0.30919647216796875, 2.81439208984375]\n",
            "126: Train Loss: [1.851050615310669, 0.2656800150871277, 1.585370659828186] | Test Loss: [3.1101200580596924, 0.2988794445991516, 2.8112406730651855]\n",
            "127: Train Loss: [2.4562034606933594, 0.3390367031097412, 2.117166757583618] | Test Loss: [3.1033315658569336, 0.29423511028289795, 2.809096336364746]\n",
            "128: Train Loss: [1.8031916618347168, 0.2516504228115082, 1.5515412092208862] | Test Loss: [3.0941967964172363, 0.28900426626205444, 2.805192470550537]\n",
            "129: Train Loss: [1.8641963005065918, 0.2758399248123169, 1.588356375694275] | Test Loss: [3.0939645767211914, 0.28944745659828186, 2.8045170307159424]\n",
            "130: Train Loss: [0.5576136112213135, 0.31924423575401306, 0.2383693903684616] | Test Loss: [3.09909725189209, 0.2941589951515198, 2.804938316345215]\n",
            "131: Train Loss: [2.17720627784729, 0.35653555393218994, 1.8206707239151] | Test Loss: [3.1141271591186523, 0.30782923102378845, 2.806298017501831]\n",
            "132: Train Loss: [1.8258095979690552, 0.38890016078948975, 1.4369094371795654] | Test Loss: [3.142308235168457, 0.33119475841522217, 2.8111135959625244]\n",
            "133: Train Loss: [2.138420343399048, 0.2906230092048645, 1.8477972745895386] | Test Loss: [3.1604743003845215, 0.3426564931869507, 2.8178179264068604]\n",
            "134: Train Loss: [2.2043182849884033, 0.28924456238746643, 1.9150736331939697] | Test Loss: [3.1700611114501953, 0.3423389792442322, 2.8277220726013184]\n",
            "135: Train Loss: [2.3005878925323486, 0.319739431142807, 1.9808484315872192] | Test Loss: [3.1700079441070557, 0.33539479970932007, 2.834613084793091]\n",
            "136: Train Loss: [1.7603228092193604, 0.29239800572395325, 1.4679248332977295] | Test Loss: [3.1715221405029297, 0.32753223180770874, 2.843989849090576]\n",
            "137: Train Loss: [2.148449659347534, 0.27615422010421753, 1.8722954988479614] | Test Loss: [3.1742773056030273, 0.3161473572254181, 2.8581299781799316]\n",
            "138: Train Loss: [1.9170174598693848, 0.3492792844772339, 1.5677381753921509] | Test Loss: [3.1817188262939453, 0.31245648860931396, 2.869262218475342]\n",
            "139: Train Loss: [2.0626063346862793, 0.3037761449813843, 1.7588303089141846] | Test Loss: [3.1757264137268066, 0.3068114221096039, 2.86891508102417]\n",
            "140: Train Loss: [1.6885087490081787, 0.2879539132118225, 1.400554895401001] | Test Loss: [3.167052745819092, 0.29799696803092957, 2.86905574798584]\n",
            "141: Train Loss: [1.9319512844085693, 0.2695450186729431, 1.662406325340271] | Test Loss: [3.158940315246582, 0.2904738187789917, 2.868466377258301]\n",
            "142: Train Loss: [1.7555499076843262, 0.3485684096813202, 1.4069814682006836] | Test Loss: [3.1524083614349365, 0.287330687046051, 2.8650777339935303]\n",
            "143: Train Loss: [2.0235185623168945, 0.2517477571964264, 1.771770715713501] | Test Loss: [3.143294334411621, 0.2809585630893707, 2.862335681915283]\n",
            "144: Train Loss: [1.9965696334838867, 0.2571045160293579, 1.7394651174545288] | Test Loss: [3.1375982761383057, 0.27634572982788086, 2.861252546310425]\n",
            "145: Train Loss: [1.8587124347686768, 0.2670065760612488, 1.5917059183120728] | Test Loss: [3.13240647315979, 0.2703978419303894, 2.862008571624756]\n",
            "146: Train Loss: [2.334352970123291, 0.30314964056015015, 2.031203269958496] | Test Loss: [3.132782459259033, 0.2657495439052582, 2.867033004760742]\n",
            "147: Train Loss: [1.0546011924743652, 0.26316049695014954, 0.7914407253265381] | Test Loss: [3.1334996223449707, 0.26065343618392944, 2.8728461265563965]\n",
            "148: Train Loss: [1.9612841606140137, 0.24541035294532776, 1.7158738374710083] | Test Loss: [3.1411681175231934, 0.25687354803085327, 2.8842945098876953]\n",
            "149: Train Loss: [2.0750112533569336, 0.27802908420562744, 1.7969821691513062] | Test Loss: [3.152123212814331, 0.2557573914527893, 2.8963658809661865]\n",
            "150: Train Loss: [1.9887514114379883, 0.3179609477519989, 1.670790433883667] | Test Loss: [3.1636178493499756, 0.2555142045021057, 2.9081037044525146]\n",
            "151: Train Loss: [2.149010181427002, 0.3021584451198578, 1.8468517065048218] | Test Loss: [3.170747995376587, 0.25777339935302734, 2.9129745960235596]\n",
            "152: Train Loss: [2.023400068283081, 0.30102506279945374, 1.7223750352859497] | Test Loss: [3.173676013946533, 0.260925829410553, 2.912750244140625]\n",
            "153: Train Loss: [1.9359354972839355, 0.227323517203331, 1.7086119651794434] | Test Loss: [3.1729838848114014, 0.26350507140159607, 2.9094789028167725]\n",
            "154: Train Loss: [1.8849765062332153, 0.28349757194519043, 1.601478934288025] | Test Loss: [3.170048952102661, 0.26736384630203247, 2.9026851654052734]\n",
            "155: Train Loss: [1.7693583965301514, 0.3001560866832733, 1.4692022800445557] | Test Loss: [3.166247844696045, 0.2723962068557739, 2.8938517570495605]\n",
            "156: Train Loss: [1.9663830995559692, 0.24893386662006378, 1.7174491882324219] | Test Loss: [3.1559910774230957, 0.2744750380516052, 2.8815159797668457]\n",
            "157: Train Loss: [2.10691499710083, 0.2468060553073883, 1.8601089715957642] | Test Loss: [3.15017032623291, 0.27589356899261475, 2.874276638031006]\n",
            "158: Train Loss: [2.1108293533325195, 0.2814972698688507, 1.8293321132659912] | Test Loss: [3.1498634815216064, 0.2767581343650818, 2.87310528755188]\n",
            "159: Train Loss: [2.170822858810425, 0.33032819628715515, 1.8404947519302368] | Test Loss: [3.155994176864624, 0.283618688583374, 2.87237548828125]\n",
            "160: Train Loss: [2.0814058780670166, 0.24486775696277618, 1.8365381956100464] | Test Loss: [3.15040922164917, 0.28503286838531494, 2.8653762340545654]\n",
            "161: Train Loss: [2.1128089427948, 0.34121909737586975, 1.7715898752212524] | Test Loss: [3.150644302368164, 0.2886280417442322, 2.862016201019287]\n",
            "162: Train Loss: [1.8683488368988037, 0.26552510261535645, 1.6028237342834473] | Test Loss: [3.151949167251587, 0.29036086797714233, 2.8615882396698]\n",
            "163: Train Loss: [2.039226770401001, 0.334254652261734, 1.7049721479415894] | Test Loss: [3.156001567840576, 0.2984141409397125, 2.8575873374938965]\n",
            "164: Train Loss: [1.9259366989135742, 0.276844322681427, 1.6490923166275024] | Test Loss: [3.1574244499206543, 0.30314913392066956, 2.8542752265930176]\n",
            "165: Train Loss: [1.830193281173706, 0.2928309440612793, 1.5373623371124268] | Test Loss: [3.158647060394287, 0.3083152770996094, 2.8503317832946777]\n",
            "166: Train Loss: [1.9977573156356812, 0.296364426612854, 1.7013928890228271] | Test Loss: [3.1556921005249023, 0.3085271716117859, 2.8471648693084717]\n",
            "167: Train Loss: [2.197604179382324, 0.3139878511428833, 1.883616328239441] | Test Loss: [3.1645467281341553, 0.31303516030311584, 2.8515114784240723]\n",
            "168: Train Loss: [1.8793151378631592, 0.2824312448501587, 1.5968838930130005] | Test Loss: [3.172222375869751, 0.31520703434944153, 2.857015371322632]\n",
            "169: Train Loss: [1.9182100296020508, 0.2568150460720062, 1.6613949537277222] | Test Loss: [3.173158884048462, 0.3099813759326935, 2.863177537918091]\n",
            "170: Train Loss: [2.021707534790039, 0.2523046135902405, 1.7694029808044434] | Test Loss: [3.1698508262634277, 0.3007374703884125, 2.8691134452819824]\n",
            "171: Train Loss: [1.9804110527038574, 0.2586838901042938, 1.7217271327972412] | Test Loss: [3.1648201942443848, 0.28903070092201233, 2.8757894039154053]\n",
            "172: Train Loss: [2.105257034301758, 0.29209405183792114, 1.8131630420684814] | Test Loss: [3.1585628986358643, 0.2808679938316345, 2.877694845199585]\n",
            "173: Train Loss: [2.101926803588867, 0.3573159873485565, 1.7446107864379883] | Test Loss: [3.1616485118865967, 0.2825470268726349, 2.879101514816284]\n",
            "174: Train Loss: [2.3011770248413086, 0.24585020542144775, 2.0553267002105713] | Test Loss: [3.1553354263305664, 0.28358981013298035, 2.8717455863952637]\n",
            "175: Train Loss: [2.081555128097534, 0.29458531737327576, 1.7869699001312256] | Test Loss: [3.1493613719940186, 0.28462880849838257, 2.864732503890991]\n",
            "176: Train Loss: [1.8893072605133057, 0.28988122940063477, 1.599426031112671] | Test Loss: [3.142148733139038, 0.28770917654037476, 2.8544394969940186]\n",
            "177: Train Loss: [2.395862102508545, 0.2682865560054779, 2.127575635910034] | Test Loss: [3.1309103965759277, 0.29052725434303284, 2.8403830528259277]\n",
            "178: Train Loss: [1.8794667720794678, 0.32011890411376953, 1.5593478679656982] | Test Loss: [3.1237950325012207, 0.2966807782649994, 2.8271143436431885]\n",
            "179: Train Loss: [1.9455218315124512, 0.2859649956226349, 1.6595568656921387] | Test Loss: [3.114173412322998, 0.3006595969200134, 2.81351375579834]\n",
            "180: Train Loss: [1.5496747493743896, 0.31614312529563904, 1.2335315942764282] | Test Loss: [3.106382131576538, 0.3039751648902893, 2.8024070262908936]\n",
            "181: Train Loss: [1.8833229541778564, 0.30275988578796387, 1.5805630683898926] | Test Loss: [3.104076862335205, 0.30955418944358826, 2.794522762298584]\n",
            "182: Train Loss: [2.2721524238586426, 0.29430878162384033, 1.9778437614440918] | Test Loss: [3.1034176349639893, 0.3136117458343506, 2.7898058891296387]\n",
            "183: Train Loss: [1.8981157541275024, 0.2648223340511322, 1.6332933902740479] | Test Loss: [3.099294900894165, 0.3122938573360443, 2.787001132965088]\n",
            "184: Train Loss: [2.1660101413726807, 0.260132372379303, 1.905877709388733] | Test Loss: [3.0871994495391846, 0.3022010326385498, 2.7849984169006348]\n",
            "185: Train Loss: [1.973069190979004, 0.24306870996952057, 1.7300004959106445] | Test Loss: [3.0772244930267334, 0.28994646668434143, 2.787277936935425]\n",
            "186: Train Loss: [1.8752906322479248, 0.22069351375102997, 1.6545971632003784] | Test Loss: [3.0656678676605225, 0.27964186668395996, 2.7860260009765625]\n",
            "187: Train Loss: [2.000072956085205, 0.339948832988739, 1.6601243019104004] | Test Loss: [3.062201738357544, 0.27604180574417114, 2.7861599922180176]\n",
            "188: Train Loss: [1.988696575164795, 0.3198680579662323, 1.6688284873962402] | Test Loss: [3.05914306640625, 0.27536311745643616, 2.7837798595428467]\n",
            "189: Train Loss: [2.092022657394409, 0.30707985162734985, 1.784942865371704] | Test Loss: [3.0671164989471436, 0.28293731808662415, 2.784179210662842]\n",
            "190: Train Loss: [2.0478336811065674, 0.25907841324806213, 1.7887552976608276] | Test Loss: [3.0754497051239014, 0.28865867853164673, 2.7867910861968994]\n",
            "191: Train Loss: [0.8121381998062134, 0.22179727256298065, 0.5903409123420715] | Test Loss: [3.0838735103607178, 0.2925516664981842, 2.7913217544555664]\n",
            "192: Train Loss: [1.8930023908615112, 0.2553349435329437, 1.6376674175262451] | Test Loss: [3.0910849571228027, 0.29455816745758057, 2.7965269088745117]\n",
            "193: Train Loss: [2.0386672019958496, 0.29580652713775635, 1.7428605556488037] | Test Loss: [3.094162940979004, 0.2983929216861725, 2.7957699298858643]\n",
            "194: Train Loss: [2.0285418033599854, 0.31121399998664856, 1.7173278331756592] | Test Loss: [3.0929112434387207, 0.3024393618106842, 2.7904717922210693]\n",
            "195: Train Loss: [1.8174080848693848, 0.3052811920642853, 1.5121269226074219] | Test Loss: [3.0936338901519775, 0.30968841910362244, 2.7839455604553223]\n",
            "196: Train Loss: [2.048760414123535, 0.2994207441806793, 1.7493395805358887] | Test Loss: [3.0947256088256836, 0.31550657749176025, 2.779219150543213]\n",
            "197: Train Loss: [2.058565616607666, 0.2695426046848297, 1.7890229225158691] | Test Loss: [3.0927093029022217, 0.3211536407470703, 2.7715556621551514]\n",
            "198: Train Loss: [2.0350592136383057, 0.3408990502357483, 1.6941601037979126] | Test Loss: [3.102599859237671, 0.3303670287132263, 2.7722327709198]\n",
            "199: Train Loss: [2.0271661281585693, 0.2741359770298004, 1.7530301809310913] | Test Loss: [3.119492530822754, 0.34206438064575195, 2.777428150177002]\n",
            "200: Train Loss: [1.8336150646209717, 0.26493459939956665, 1.5686804056167603] | Test Loss: [3.129098892211914, 0.3496078848838806, 2.7794909477233887]\n",
            "201: Train Loss: [1.985133171081543, 0.29858916997909546, 1.6865440607070923] | Test Loss: [3.13960599899292, 0.35599783062934875, 2.7836081981658936]\n",
            "202: Train Loss: [1.6988773345947266, 0.2616860568523407, 1.4371912479400635] | Test Loss: [3.145862579345703, 0.35843661427497864, 2.787425994873047]\n",
            "203: Train Loss: [2.1805880069732666, 0.30470800399780273, 1.8758796453475952] | Test Loss: [3.156582832336426, 0.3662286400794983, 2.7903542518615723]\n",
            "204: Train Loss: [2.0351295471191406, 0.2444877326488495, 1.7906417846679688] | Test Loss: [3.1587696075439453, 0.36428534984588623, 2.7944843769073486]\n",
            "205: Train Loss: [1.9982837438583374, 0.2691470682621002, 1.7291367053985596] | Test Loss: [3.159625291824341, 0.36124587059020996, 2.798379421234131]\n",
            "206: Train Loss: [2.0562026500701904, 0.2618764340877533, 1.7943261861801147] | Test Loss: [3.1548404693603516, 0.3563882112503052, 2.798452377319336]\n",
            "207: Train Loss: [2.1743550300598145, 0.29208725690841675, 1.882267713546753] | Test Loss: [3.154773235321045, 0.3516797423362732, 2.803093433380127]\n",
            "208: Train Loss: [2.3195056915283203, 0.29072219133377075, 2.0287835597991943] | Test Loss: [3.1571385860443115, 0.3499171733856201, 2.8072214126586914]\n",
            "209: Train Loss: [2.1132078170776367, 0.2513965666294098, 1.8618111610412598] | Test Loss: [3.1612470149993896, 0.3526651859283447, 2.808581829071045]\n",
            "210: Train Loss: [2.2227697372436523, 0.26749539375305176, 1.9552744626998901] | Test Loss: [3.161951780319214, 0.3553176820278168, 2.8066341876983643]\n",
            "211: Train Loss: [1.854236364364624, 0.30482858419418335, 1.5494078397750854] | Test Loss: [3.1653494834899902, 0.35923993587493896, 2.8061094284057617]\n",
            "212: Train Loss: [1.9630903005599976, 0.2828986644744873, 1.6801916360855103] | Test Loss: [3.1700382232666016, 0.36310428380966187, 2.806933879852295]\n",
            "213: Train Loss: [2.113701820373535, 0.29671725630760193, 1.8169846534729004] | Test Loss: [3.1649317741394043, 0.36218273639678955, 2.802748918533325]\n",
            "214: Train Loss: [2.230692148208618, 0.31314969062805176, 1.9175424575805664] | Test Loss: [3.164259672164917, 0.3661157190799713, 2.7981438636779785]\n",
            "215: Train Loss: [1.9234522581100464, 0.29082632064819336, 1.632625937461853] | Test Loss: [3.166999340057373, 0.37020745873451233, 2.7967917919158936]\n",
            "216: Train Loss: [2.158705234527588, 0.27147120237350464, 1.887234091758728] | Test Loss: [3.1686084270477295, 0.37106505036354065, 2.7975432872772217]\n",
            "217: Train Loss: [1.7596057653427124, 0.2949804365634918, 1.464625358581543] | Test Loss: [3.1710140705108643, 0.37118083238601685, 2.799833297729492]\n",
            "218: Train Loss: [1.5764992237091064, 0.3100294768810272, 1.2664697170257568] | Test Loss: [3.174891948699951, 0.37193763256073, 2.8029544353485107]\n",
            "219: Train Loss: [1.862642765045166, 0.33397752046585083, 1.5286651849746704] | Test Loss: [3.177027940750122, 0.3701436519622803, 2.806884288787842]\n",
            "220: Train Loss: [1.9004368782043457, 0.3059430718421936, 1.5944937467575073] | Test Loss: [3.1771044731140137, 0.3659304976463318, 2.811173915863037]\n",
            "221: Train Loss: [1.9125444889068604, 0.2863032817840576, 1.6262412071228027] | Test Loss: [3.172952175140381, 0.3574446141719818, 2.815507650375366]\n",
            "222: Train Loss: [0.8076099157333374, 0.26974308490753174, 0.5378668308258057] | Test Loss: [3.1675798892974854, 0.3468109667301178, 2.8207688331604004]\n",
            "223: Train Loss: [2.074383020401001, 0.2836832106113434, 1.7906997203826904] | Test Loss: [3.164242744445801, 0.3353591561317444, 2.828883647918701]\n",
            "224: Train Loss: [1.5918598175048828, 0.29771724343299866, 1.2941426038742065] | Test Loss: [3.1592910289764404, 0.32444262504577637, 2.834848403930664]\n",
            "225: Train Loss: [2.0602786540985107, 0.28847846388816833, 1.7718002796173096] | Test Loss: [3.1578173637390137, 0.31574130058288574, 2.842076063156128]\n",
            "226: Train Loss: [1.917262315750122, 0.2967086434364319, 1.620553731918335] | Test Loss: [3.157576322555542, 0.3095601499080658, 2.8480162620544434]\n",
            "227: Train Loss: [1.8876036405563354, 0.2786467969417572, 1.6089568138122559] | Test Loss: [3.160651206970215, 0.30520111322402954, 2.85545015335083]\n",
            "228: Train Loss: [2.2823493480682373, 0.27736613154411316, 2.0049831867218018] | Test Loss: [3.1667652130126953, 0.30712926387786865, 2.859635829925537]\n",
            "229: Train Loss: [1.9083455801010132, 0.2494877725839615, 1.658857822418213] | Test Loss: [3.1687381267547607, 0.30703598260879517, 2.8617022037506104]\n",
            "230: Train Loss: [2.182584762573242, 0.251179963350296, 1.931404709815979] | Test Loss: [3.167480945587158, 0.3038855195045471, 2.863595485687256]\n",
            "231: Train Loss: [0.9690679311752319, 0.3173849880695343, 0.6516829133033752] | Test Loss: [3.173150062561035, 0.3054580092430115, 2.867691993713379]\n",
            "232: Train Loss: [2.155679702758789, 0.25639399886131287, 1.8992857933044434] | Test Loss: [3.1762914657592773, 0.30142784118652344, 2.874863624572754]\n",
            "233: Train Loss: [1.770430088043213, 0.2668536305427551, 1.5035765171051025] | Test Loss: [3.1777968406677246, 0.29842713475227356, 2.8793697357177734]\n",
            "234: Train Loss: [1.973705768585205, 0.2819627523422241, 1.691743016242981] | Test Loss: [3.1773431301116943, 0.2929389476776123, 2.884404182434082]\n",
            "235: Train Loss: [2.1679229736328125, 0.310177206993103, 1.8577457666397095] | Test Loss: [3.1751208305358887, 0.28815463185310364, 2.8869662284851074]\n",
            "236: Train Loss: [2.1163501739501953, 0.26878267526626587, 1.8475674390792847] | Test Loss: [3.158871650695801, 0.28031885623931885, 2.8785526752471924]\n",
            "237: Train Loss: [2.1446609497070312, 0.26100778579711914, 1.8836530447006226] | Test Loss: [3.1425530910491943, 0.2705373167991638, 2.8720157146453857]\n",
            "238: Train Loss: [2.104416847229004, 0.3318530321121216, 1.7725639343261719] | Test Loss: [3.133362054824829, 0.26419949531555176, 2.8691625595092773]\n",
            "239: Train Loss: [2.1590967178344727, 0.3602512776851654, 1.7988455295562744] | Test Loss: [3.1403281688690186, 0.2709040939807892, 2.8694241046905518]\n",
            "240: Train Loss: [2.0017404556274414, 0.2939167320728302, 1.7078237533569336] | Test Loss: [3.163407325744629, 0.28619837760925293, 2.877208948135376]\n",
            "241: Train Loss: [2.0969748497009277, 0.26865601539611816, 1.82831871509552] | Test Loss: [3.1891262531280518, 0.3017120361328125, 2.8874142169952393]\n",
            "242: Train Loss: [1.976564645767212, 0.3365432024002075, 1.6400214433670044] | Test Loss: [3.210409641265869, 0.3157241642475128, 2.8946855068206787]\n",
            "243: Train Loss: [2.11366605758667, 0.26768288016319275, 1.8459831476211548] | Test Loss: [3.2125167846679688, 0.3237862288951874, 2.888730525970459]\n",
            "244: Train Loss: [2.0576844215393066, 0.265588641166687, 1.7920958995819092] | Test Loss: [3.2077534198760986, 0.32440000772476196, 2.8833534717559814]\n",
            "245: Train Loss: [2.104055643081665, 0.2693421244621277, 1.8347134590148926] | Test Loss: [3.201127290725708, 0.3174108862876892, 2.883716344833374]\n",
            "246: Train Loss: [2.067473888397217, 0.30560287833213806, 1.761871099472046] | Test Loss: [3.1974565982818604, 0.3111394941806793, 2.886317014694214]\n",
            "247: Train Loss: [2.098322629928589, 0.32987290620803833, 1.7684497833251953] | Test Loss: [3.2010345458984375, 0.3130534589290619, 2.8879811763763428]\n",
            "248: Train Loss: [2.132052183151245, 0.2588093876838684, 1.873242735862732] | Test Loss: [3.201549768447876, 0.31077685952186584, 2.890772819519043]\n",
            "249: Train Loss: [2.2850701808929443, 0.24817395210266113, 2.036896228790283] | Test Loss: [3.200134038925171, 0.30518659949302673, 2.8949475288391113]\n",
            "250: Train Loss: [1.7081303596496582, 0.2801530659198761, 1.4279773235321045] | Test Loss: [3.19838285446167, 0.30235540866851807, 2.8960273265838623]\n",
            "251: Train Loss: [2.2721469402313232, 0.3227078318595886, 1.9494391679763794] | Test Loss: [3.2009172439575195, 0.30625471472740173, 2.894662618637085]\n",
            "252: Train Loss: [2.1737868785858154, 0.34308817982673645, 1.8306987285614014] | Test Loss: [3.205287456512451, 0.31226643919944763, 2.8930211067199707]\n",
            "253: Train Loss: [1.7828795909881592, 0.24728074669837952, 1.535598874092102] | Test Loss: [3.203317642211914, 0.3169233202934265, 2.8863942623138428]\n",
            "254: Train Loss: [2.097486734390259, 0.28468260169029236, 1.8128042221069336] | Test Loss: [3.1983797550201416, 0.3218200206756592, 2.8765597343444824]\n",
            "255: Train Loss: [1.8616702556610107, 0.3495236933231354, 1.5121465921401978] | Test Loss: [3.1981470584869385, 0.3350602984428406, 2.863086700439453]\n",
            "256: Train Loss: [2.129148006439209, 0.23830142617225647, 1.8908464908599854] | Test Loss: [3.191679000854492, 0.3419457972049713, 2.8497331142425537]\n",
            "257: Train Loss: [2.228790760040283, 0.2548850178718567, 1.9739058017730713] | Test Loss: [3.1783604621887207, 0.3443757891654968, 2.833984613418579]\n",
            "258: Train Loss: [2.004953384399414, 0.2393069565296173, 1.7656464576721191] | Test Loss: [3.1602911949157715, 0.33990105986595154, 2.820390224456787]\n",
            "259: Train Loss: [1.7723497152328491, 0.2801258862018585, 1.492223858833313] | Test Loss: [3.146716594696045, 0.33842357993125916, 2.808293104171753]\n",
            "260: Train Loss: [2.196636199951172, 0.31627053022384644, 1.8803657293319702] | Test Loss: [3.139892816543579, 0.3400133550167084, 2.799879550933838]\n",
            "261: Train Loss: [2.181278705596924, 0.2484743297100067, 1.9328043460845947] | Test Loss: [3.1311020851135254, 0.3366812467575073, 2.7944207191467285]\n",
            "262: Train Loss: [1.8276801109313965, 0.2566423714160919, 1.571037769317627] | Test Loss: [3.1210482120513916, 0.32927200198173523, 2.791776180267334]\n",
            "263: Train Loss: [1.9459190368652344, 0.29427123069763184, 1.6516478061676025] | Test Loss: [3.1158666610717773, 0.3282391130924225, 2.7876274585723877]\n",
            "264: Train Loss: [2.021313428878784, 0.30437371134757996, 1.7169398069381714] | Test Loss: [3.1102607250213623, 0.32961007952690125, 2.7806506156921387]\n",
            "265: Train Loss: [2.0568416118621826, 0.306253582239151, 1.750588059425354] | Test Loss: [3.1106414794921875, 0.33678194880485535, 2.7738595008850098]\n",
            "266: Train Loss: [2.0700602531433105, 0.28930848836898804, 1.7807518243789673] | Test Loss: [3.1133224964141846, 0.3430306017398834, 2.770291805267334]\n",
            "267: Train Loss: [1.820885181427002, 0.25250381231307983, 1.5683811902999878] | Test Loss: [3.1170806884765625, 0.3489914536476135, 2.7680892944335938]\n",
            "268: Train Loss: [1.7760467529296875, 0.3501209020614624, 1.425925850868225] | Test Loss: [3.126818895339966, 0.3628794252872467, 2.763939380645752]\n",
            "269: Train Loss: [2.134294033050537, 0.24930807948112488, 1.8849860429763794] | Test Loss: [3.1302309036254883, 0.36913031339645386, 2.7611005306243896]\n",
            "270: Train Loss: [1.8034250736236572, 0.3011581301689148, 1.5022668838500977] | Test Loss: [3.125072479248047, 0.3691611886024475, 2.755911350250244]\n",
            "271: Train Loss: [1.8629359006881714, 0.28105536103248596, 1.5818805694580078] | Test Loss: [3.109574794769287, 0.35776886343955994, 2.7518060207366943]\n",
            "272: Train Loss: [1.961986780166626, 0.2793876528739929, 1.6825991868972778] | Test Loss: [3.0862972736358643, 0.34141573309898376, 2.7448816299438477]\n",
            "273: Train Loss: [2.2986974716186523, 0.30544137954711914, 1.9932562112808228] | Test Loss: [3.067497968673706, 0.3248520493507385, 2.7426459789276123]\n",
            "274: Train Loss: [1.9132905006408691, 0.32805192470550537, 1.5852385759353638] | Test Loss: [3.049112319946289, 0.31078609824180603, 2.73832631111145]\n",
            "275: Train Loss: [2.088346481323242, 0.25132429599761963, 1.837022304534912] | Test Loss: [3.0323452949523926, 0.2963850498199463, 2.7359602451324463]\n",
            "276: Train Loss: [1.8316024541854858, 0.2694588899612427, 1.5621435642242432] | Test Loss: [3.0224785804748535, 0.2850540578365326, 2.737424612045288]\n",
            "277: Train Loss: [1.5859569311141968, 0.29959240555763245, 1.2863645553588867] | Test Loss: [3.0194194316864014, 0.2794083058834076, 2.740011215209961]\n",
            "278: Train Loss: [1.517439603805542, 0.33032935857772827, 1.187110185623169] | Test Loss: [3.0180203914642334, 0.2775225043296814, 2.7404978275299072]\n",
            "279: Train Loss: [1.8490872383117676, 0.3278220593929291, 1.5212651491165161] | Test Loss: [3.0199146270751953, 0.27649223804473877, 2.743422508239746]\n",
            "280: Train Loss: [1.1920303106307983, 0.2587417662143707, 0.93328857421875] | Test Loss: [3.0215351581573486, 0.2722298800945282, 2.749305248260498]\n",
            "281: Train Loss: [2.122687339782715, 0.2655027210712433, 1.8571845293045044] | Test Loss: [3.0223774909973145, 0.2658945918083191, 2.7564828395843506]\n",
            "282: Train Loss: [1.8545479774475098, 0.28301990032196045, 1.5715280771255493] | Test Loss: [3.0248918533325195, 0.2580186426639557, 2.7668731212615967]\n",
            "283: Train Loss: [2.5550217628479004, 0.3318271338939667, 2.2231945991516113] | Test Loss: [3.036618709564209, 0.25695955753326416, 2.7796592712402344]\n",
            "284: Train Loss: [1.9866708517074585, 0.3249913156032562, 1.6616795063018799] | Test Loss: [3.0565078258514404, 0.26368165016174316, 2.7928261756896973]\n",
            "285: Train Loss: [2.2446706295013428, 0.345236212015152, 1.8994344472885132] | Test Loss: [3.079444169998169, 0.27185556292533875, 2.807588577270508]\n",
            "286: Train Loss: [1.7295960187911987, 0.3257673680782318, 1.4038286209106445] | Test Loss: [3.1078941822052, 0.2836271822452545, 2.8242669105529785]\n",
            "287: Train Loss: [1.5662860870361328, 0.2881149649620056, 1.278171181678772] | Test Loss: [3.127891778945923, 0.2893211245536804, 2.8385705947875977]\n",
            "288: Train Loss: [1.0477979183197021, 0.2856055200099945, 0.76219242811203] | Test Loss: [3.1388652324676514, 0.28635305166244507, 2.8525121212005615]\n",
            "289: Train Loss: [1.9679367542266846, 0.3322277069091797, 1.6357090473175049] | Test Loss: [3.146312952041626, 0.2911365032196045, 2.8551764488220215]\n",
            "290: Train Loss: [2.179272174835205, 0.3054006099700928, 1.8738715648651123] | Test Loss: [3.150559902191162, 0.2959514260292053, 2.8546085357666016]\n",
            "291: Train Loss: [2.156607151031494, 0.30282002687454224, 1.8537871837615967] | Test Loss: [3.1487913131713867, 0.299052357673645, 2.8497390747070312]\n",
            "292: Train Loss: [1.8572876453399658, 0.26487722992897034, 1.5924104452133179] | Test Loss: [3.1445212364196777, 0.2953851819038391, 2.8491361141204834]\n",
            "293: Train Loss: [1.699953556060791, 0.3031269311904907, 1.3968266248703003] | Test Loss: [3.1463778018951416, 0.28901776671409607, 2.8573601245880127]\n",
            "294: Train Loss: [2.1734700202941895, 0.2589409053325653, 1.9145292043685913] | Test Loss: [3.146923542022705, 0.28060272336006165, 2.866320848464966]\n",
            "295: Train Loss: [2.17950439453125, 0.27550166845321655, 1.9040027856826782] | Test Loss: [3.1496341228485107, 0.27626150846481323, 2.8733725547790527]\n",
            "296: Train Loss: [2.352728843688965, 0.270427405834198, 2.082301378250122] | Test Loss: [3.1501758098602295, 0.27383899688720703, 2.8763368129730225]\n",
            "297: Train Loss: [1.3957098722457886, 0.2712480425834656, 1.1244620084762573] | Test Loss: [3.1483938694000244, 0.274181991815567, 2.8742117881774902]\n",
            "298: Train Loss: [2.0946295261383057, 0.2825683057308197, 1.8120613098144531] | Test Loss: [3.136495590209961, 0.2752366065979004, 2.8612589836120605]\n",
            "299: Train Loss: [1.6892271041870117, 0.2627747654914856, 1.4264522790908813] | Test Loss: [3.126741409301758, 0.27891188859939575, 2.847829580307007]\n",
            "300: Train Loss: [1.9158827066421509, 0.23854422569274902, 1.6773384809494019] | Test Loss: [3.111541986465454, 0.28329524397850037, 2.828246831893921]\n",
            "301: Train Loss: [1.8761106729507446, 0.37943634390830994, 1.4966742992401123] | Test Loss: [3.1087005138397217, 0.2951609790325165, 2.813539505004883]\n",
            "302: Train Loss: [2.0217442512512207, 0.26112738251686096, 1.7606167793273926] | Test Loss: [3.1041765213012695, 0.3010925054550171, 2.803083896636963]\n",
            "303: Train Loss: [1.9557993412017822, 0.35498425364494324, 1.6008150577545166] | Test Loss: [3.1062140464782715, 0.3112248480319977, 2.7949891090393066]\n",
            "304: Train Loss: [2.4677038192749023, 0.2940223813056946, 2.1736814975738525] | Test Loss: [3.110788345336914, 0.3197280466556549, 2.791060209274292]\n",
            "305: Train Loss: [1.896852731704712, 0.2649151682853699, 1.6319375038146973] | Test Loss: [3.1132285594940186, 0.32209086418151855, 2.7911376953125]\n",
            "306: Train Loss: [1.7058695554733276, 0.3253842294216156, 1.3804852962493896] | Test Loss: [3.1188762187957764, 0.32504191994667053, 2.7938342094421387]\n",
            "307: Train Loss: [2.140402317047119, 0.3262964189052582, 1.8141058683395386] | Test Loss: [3.128178119659424, 0.3261759877204895, 2.802002191543579]\n",
            "308: Train Loss: [1.8233928680419922, 0.290122926235199, 1.533270001411438] | Test Loss: [3.134153366088867, 0.3219268321990967, 2.8122265338897705]\n",
            "309: Train Loss: [2.0265300273895264, 0.279896080493927, 1.7466340065002441] | Test Loss: [3.136890411376953, 0.3134690523147583, 2.8234212398529053]\n",
            "310: Train Loss: [2.031826972961426, 0.2926602065563202, 1.7391667366027832] | Test Loss: [3.1386756896972656, 0.3050956428050995, 2.8335800170898438]\n",
            "311: Train Loss: [2.074617385864258, 0.2917105257511139, 1.7829068899154663] | Test Loss: [3.1316614151000977, 0.29589495062828064, 2.835766553878784]\n",
            "312: Train Loss: [2.271796703338623, 0.2564138174057007, 2.015383005142212] | Test Loss: [3.130249500274658, 0.2886618375778198, 2.841587781906128]\n",
            "313: Train Loss: [1.5071450471878052, 0.23312829434871674, 1.2740167379379272] | Test Loss: [3.125485420227051, 0.27977725863456726, 2.845708131790161]\n",
            "314: Train Loss: [2.4032680988311768, 0.23733815550804138, 2.1659300327301025] | Test Loss: [3.1216351985931396, 0.2699788212776184, 2.851656436920166]\n",
            "315: Train Loss: [1.9096128940582275, 0.2622604966163635, 1.6473524570465088] | Test Loss: [3.1171751022338867, 0.26153358817100525, 2.8556416034698486]\n",
            "316: Train Loss: [1.8815730810165405, 0.28270435333251953, 1.598868727684021] | Test Loss: [3.112609386444092, 0.2576434314250946, 2.854965925216675]\n",
            "317: Train Loss: [2.1135144233703613, 0.26061105728149414, 1.8529032468795776] | Test Loss: [3.1098642349243164, 0.25471699237823486, 2.855147123336792]\n",
            "318: Train Loss: [1.8897548913955688, 0.23305585980415344, 1.6566990613937378] | Test Loss: [3.1062111854553223, 0.2516980767250061, 2.854513168334961]\n",
            "319: Train Loss: [1.9157555103302002, 0.27369949221611023, 1.6420559883117676] | Test Loss: [3.107482671737671, 0.2537685036659241, 2.8537142276763916]\n",
            "320: Train Loss: [1.8635506629943848, 0.32586386799812317, 1.537686824798584] | Test Loss: [3.113943099975586, 0.2618641257286072, 2.852078914642334]\n",
            "321: Train Loss: [2.0430750846862793, 0.26672446727752686, 1.776350498199463] | Test Loss: [3.125117301940918, 0.2709176540374756, 2.8541996479034424]\n",
            "322: Train Loss: [2.083003520965576, 0.31246626377105713, 1.770537257194519] | Test Loss: [3.1391961574554443, 0.2832958400249481, 2.855900287628174]\n",
            "323: Train Loss: [1.616776704788208, 0.28075671195983887, 1.3360199928283691] | Test Loss: [3.1476027965545654, 0.2921488285064697, 2.8554539680480957]\n",
            "324: Train Loss: [2.011317014694214, 0.376321941614151, 1.6349951028823853] | Test Loss: [3.1764047145843506, 0.31276699900627136, 2.863637685775757]\n",
            "325: Train Loss: [1.8040742874145508, 0.35140129923820496, 1.4526729583740234] | Test Loss: [3.211123466491699, 0.3390708565711975, 2.8720526695251465]\n",
            "326: Train Loss: [1.0362249612808228, 0.3006958067417145, 0.7355291843414307] | Test Loss: [3.2364659309387207, 0.3582432270050049, 2.878222703933716]\n",
            "327: Train Loss: [1.988569736480713, 0.32821327447891235, 1.6603565216064453] | Test Loss: [3.2667994499206543, 0.3760303258895874, 2.8907692432403564]\n",
            "328: Train Loss: [1.7093244791030884, 0.2655649483203888, 1.443759560585022] | Test Loss: [3.2676281929016113, 0.3732537627220154, 2.894374370574951]\n",
            "329: Train Loss: [2.0466554164886475, 0.3708159923553467, 1.6758394241333008] | Test Loss: [3.268796443939209, 0.36904850602149963, 2.899747848510742]\n",
            "330: Train Loss: [2.1385116577148438, 0.28701478242874146, 1.8514968156814575] | Test Loss: [3.257204294204712, 0.3503311574459076, 2.9068732261657715]\n",
            "331: Train Loss: [2.429980516433716, 0.3118704557418823, 2.118110179901123] | Test Loss: [3.2310893535614014, 0.32080528140068054, 2.9102840423583984]\n",
            "332: Train Loss: [2.223085880279541, 0.3244880139827728, 1.8985979557037354] | Test Loss: [3.2097978591918945, 0.2989391088485718, 2.9108588695526123]\n",
            "333: Train Loss: [2.170976161956787, 0.2979775369167328, 1.8729987144470215] | Test Loss: [3.1890342235565186, 0.28367796540260315, 2.9053561687469482]\n",
            "334: Train Loss: [1.9963154792785645, 0.2995719611644745, 1.6967434883117676] | Test Loss: [3.1703197956085205, 0.2748539447784424, 2.895465850830078]\n",
            "335: Train Loss: [2.02461576461792, 0.41766393184661865, 1.6069519519805908] | Test Loss: [3.1525352001190186, 0.2741797864437103, 2.8783555030822754]\n",
            "336: Train Loss: [2.327972412109375, 0.31018614768981934, 2.0177862644195557] | Test Loss: [3.1454432010650635, 0.2790171205997467, 2.8664259910583496]\n",
            "337: Train Loss: [1.8936235904693604, 0.29278865456581116, 1.6008349657058716] | Test Loss: [3.1377036571502686, 0.28645071387290955, 2.851253032684326]\n",
            "338: Train Loss: [1.8818132877349854, 0.24008183181285858, 1.6417315006256104] | Test Loss: [3.128542900085449, 0.29436442255973816, 2.8341784477233887]\n",
            "339: Train Loss: [1.0855772495269775, 0.2588675022125244, 0.8267098069190979] | Test Loss: [3.117821455001831, 0.3004481792449951, 2.817373275756836]\n",
            "340: Train Loss: [2.216398000717163, 0.2517731189727783, 1.9646248817443848] | Test Loss: [3.097846269607544, 0.2983553111553192, 2.7994909286499023]\n",
            "341: Train Loss: [2.425537109375, 0.27284741401672363, 2.1526896953582764] | Test Loss: [3.0821280479431152, 0.2920839488506317, 2.790044069290161]\n",
            "342: Train Loss: [1.6917734146118164, 0.2959904074668884, 1.3957830667495728] | Test Loss: [3.070324420928955, 0.2883381247520447, 2.7819862365722656]\n",
            "343: Train Loss: [1.851165771484375, 0.28314217925071716, 1.5680235624313354] | Test Loss: [3.061422824859619, 0.28561216592788696, 2.775810718536377]\n",
            "344: Train Loss: [1.8455798625946045, 0.23597824573516846, 1.609601616859436] | Test Loss: [3.052135944366455, 0.281130850315094, 2.771005153656006]\n",
            "345: Train Loss: [2.2532639503479004, 0.3463113307952881, 1.9069526195526123] | Test Loss: [3.0482242107391357, 0.27973467111587524, 2.7684895992279053]\n",
            "346: Train Loss: [2.1710357666015625, 0.3162871301174164, 1.8547487258911133] | Test Loss: [3.0506467819213867, 0.2825413644313812, 2.7681055068969727]\n",
            "347: Train Loss: [2.1240718364715576, 0.2711199223995209, 1.8529518842697144] | Test Loss: [3.052236557006836, 0.2829013764858246, 2.7693352699279785]\n",
            "348: Train Loss: [1.800948143005371, 0.3199205994606018, 1.4810274839401245] | Test Loss: [3.0521395206451416, 0.2837889492511749, 2.768350601196289]\n",
            "349: Train Loss: [1.8274767398834229, 0.27023518085479736, 1.5572415590286255] | Test Loss: [3.0506749153137207, 0.2831907868385315, 2.767484188079834]\n",
            "350: Train Loss: [1.6890544891357422, 0.23435761034488678, 1.4546968936920166] | Test Loss: [3.0485572814941406, 0.27756267786026, 2.7709946632385254]\n",
            "351: Train Loss: [2.213552713394165, 0.22318606078624725, 1.9903666973114014] | Test Loss: [3.048835039138794, 0.26914075016975403, 2.7796943187713623]\n",
            "352: Train Loss: [1.9715700149536133, 0.2797999978065491, 1.6917699575424194] | Test Loss: [3.051361083984375, 0.2620626986026764, 2.7892982959747314]\n",
            "353: Train Loss: [1.9990545511245728, 0.26554396748542786, 1.7335106134414673] | Test Loss: [3.0573158264160156, 0.25701630115509033, 2.8002994060516357]\n",
            "354: Train Loss: [2.0121450424194336, 0.28354203701019287, 1.7286030054092407] | Test Loss: [3.0642073154449463, 0.2536301612854004, 2.810577154159546]\n",
            "355: Train Loss: [1.872978925704956, 0.3038612902164459, 1.5691176652908325] | Test Loss: [3.0759048461914062, 0.2547430396080017, 2.8211617469787598]\n",
            "356: Train Loss: [1.831972599029541, 0.26545414328575134, 1.5665184259414673] | Test Loss: [3.0904293060302734, 0.2585778534412384, 2.8318514823913574]\n",
            "357: Train Loss: [2.0373449325561523, 0.2559170722961426, 1.7814277410507202] | Test Loss: [3.1053645610809326, 0.26436153054237366, 2.841002941131592]\n",
            "358: Train Loss: [2.106276512145996, 0.32761964201927185, 1.7786569595336914] | Test Loss: [3.1227915287017822, 0.277506023645401, 2.845285415649414]\n",
            "359: Train Loss: [1.8837370872497559, 0.23499180376529694, 1.6487452983856201] | Test Loss: [3.136608362197876, 0.28737711906433105, 2.849231243133545]\n",
            "360: Train Loss: [2.088716983795166, 0.2566741406917572, 1.832042932510376] | Test Loss: [3.1483261585235596, 0.29366499185562134, 2.854661226272583]\n",
            "361: Train Loss: [1.9455243349075317, 0.28331419825553894, 1.6622101068496704] | Test Loss: [3.163480281829834, 0.3014336824417114, 2.862046718597412]\n",
            "362: Train Loss: [2.037935733795166, 0.2981303930282593, 1.7398052215576172] | Test Loss: [3.1701371669769287, 0.30435627698898315, 2.865780830383301]\n",
            "363: Train Loss: [1.929674506187439, 0.24998734891414642, 1.6796871423721313] | Test Loss: [3.1635680198669434, 0.29825788736343384, 2.8653101921081543]\n",
            "364: Train Loss: [1.8426048755645752, 0.3003894090652466, 1.5422154664993286] | Test Loss: [3.1568050384521484, 0.29232633113861084, 2.864478588104248]\n",
            "365: Train Loss: [2.2428627014160156, 0.34716928005218506, 1.895693302154541] | Test Loss: [3.152383804321289, 0.28718581795692444, 2.8651978969573975]\n",
            "366: Train Loss: [1.042040228843689, 0.28503161668777466, 0.7570086121559143] | Test Loss: [3.1411337852478027, 0.2781338095664978, 2.86299991607666]\n",
            "367: Train Loss: [1.8426647186279297, 0.2641708254814148, 1.5784939527511597] | Test Loss: [3.1301634311676025, 0.2698654234409332, 2.860297918319702]\n",
            "368: Train Loss: [2.046178102493286, 0.34728536009788513, 1.6988927125930786] | Test Loss: [3.1210134029388428, 0.2716696262359619, 2.849343776702881]\n",
            "369: Train Loss: [0.8804171681404114, 0.30981189012527466, 0.5706052780151367] | Test Loss: [3.116790771484375, 0.2787896990776062, 2.838001012802124]\n",
            "370: Train Loss: [1.9171485900878906, 0.25790780782699585, 1.6592408418655396] | Test Loss: [3.105964183807373, 0.28102388978004456, 2.8249402046203613]\n",
            "371: Train Loss: [1.862887978553772, 0.28171810507774353, 1.581169843673706] | Test Loss: [3.0972652435302734, 0.28134647011756897, 2.8159186840057373]\n",
            "372: Train Loss: [1.5204583406448364, 0.2782491445541382, 1.2422091960906982] | Test Loss: [3.0889172554016113, 0.2858825922012329, 2.803034543991089]\n",
            "373: Train Loss: [1.8496441841125488, 0.2376384735107422, 1.6120057106018066] | Test Loss: [3.0748345851898193, 0.28586751222610474, 2.7889671325683594]\n",
            "374: Train Loss: [1.943627119064331, 0.27239134907722473, 1.6712357997894287] | Test Loss: [3.0625758171081543, 0.28838199377059937, 2.77419376373291]\n",
            "375: Train Loss: [2.0988917350769043, 0.3360060155391693, 1.7628856897354126] | Test Loss: [3.0644898414611816, 0.30154889822006226, 2.7629408836364746]\n",
            "376: Train Loss: [1.8088163137435913, 0.3160174787044525, 1.4927988052368164] | Test Loss: [3.0677430629730225, 0.31702306866645813, 2.7507200241088867]\n",
            "377: Train Loss: [1.9467692375183105, 0.3250715434551239, 1.6216976642608643] | Test Loss: [3.0746352672576904, 0.33789676427841187, 2.736738443374634]\n",
            "378: Train Loss: [1.6005277633666992, 0.2693740725517273, 1.3311537504196167] | Test Loss: [3.0737969875335693, 0.3491908013820648, 2.7246062755584717]\n",
            "379: Train Loss: [1.868037223815918, 0.33911556005477905, 1.5289216041564941] | Test Loss: [3.07207989692688, 0.3580898642539978, 2.7139899730682373]\n",
            "380: Train Loss: [1.792778491973877, 0.31743401288986206, 1.4753445386886597] | Test Loss: [3.068124294281006, 0.3653293251991272, 2.7027950286865234]\n",
            "381: Train Loss: [2.013760566711426, 0.28774309158325195, 1.7260174751281738] | Test Loss: [3.068950653076172, 0.37378326058387756, 2.695167303085327]\n",
            "382: Train Loss: [2.200010061264038, 0.3169246017932892, 1.8830853700637817] | Test Loss: [3.071413993835449, 0.38352271914482117, 2.6878912448883057]\n",
            "383: Train Loss: [2.055781364440918, 0.23134548962116241, 1.8244359493255615] | Test Loss: [3.0641465187072754, 0.38432878255844116, 2.6798176765441895]\n",
            "384: Train Loss: [2.0742666721343994, 0.3184916079044342, 1.755774974822998] | Test Loss: [3.051422357559204, 0.3809730112552643, 2.6704492568969727]\n",
            "385: Train Loss: [2.1581313610076904, 0.2712077498435974, 1.8869235515594482] | Test Loss: [3.031240940093994, 0.3721849322319031, 2.6590559482574463]\n",
            "386: Train Loss: [1.5971119403839111, 0.27197444438934326, 1.3251374959945679] | Test Loss: [3.015944480895996, 0.3623488247394562, 2.6535956859588623]\n",
            "387: Train Loss: [2.188042402267456, 0.32181113958358765, 1.8662312030792236] | Test Loss: [3.0005617141723633, 0.3575489819049835, 2.643012762069702]\n",
            "388: Train Loss: [1.9571704864501953, 0.28744620084762573, 1.6697243452072144] | Test Loss: [2.9909074306488037, 0.3551257252693176, 2.635781764984131]\n",
            "389: Train Loss: [2.1626532077789307, 0.27472084760665894, 1.887932300567627] | Test Loss: [2.986846446990967, 0.35118499398231506, 2.6356613636016846]\n",
            "390: Train Loss: [2.0592458248138428, 0.3083924949169159, 1.750853419303894] | Test Loss: [3.0020532608032227, 0.3579196333885193, 2.6441335678100586]\n",
            "391: Train Loss: [2.2356646060943604, 0.25503453612327576, 1.9806300401687622] | Test Loss: [3.008910655975342, 0.3550141453742981, 2.6538965702056885]\n",
            "392: Train Loss: [2.4732067584991455, 0.2871951460838318, 2.186011552810669] | Test Loss: [3.018078565597534, 0.34997278451919556, 2.6681058406829834]\n",
            "393: Train Loss: [2.0402045249938965, 0.31198933720588684, 1.728215217590332] | Test Loss: [3.036945104598999, 0.3518802225589752, 2.6850647926330566]\n",
            "394: Train Loss: [1.540460467338562, 0.2356255203485489, 1.3048349618911743] | Test Loss: [3.051027297973633, 0.3501257300376892, 2.700901508331299]\n",
            "395: Train Loss: [1.8961355686187744, 0.28348833322525024, 1.612647294998169] | Test Loss: [3.0608441829681396, 0.34957271814346313, 2.7112715244293213]\n",
            "396: Train Loss: [1.8262932300567627, 0.3251796066761017, 1.5011136531829834] | Test Loss: [3.0778298377990723, 0.3547258675098419, 2.7231040000915527]\n",
            "397: Train Loss: [2.150369167327881, 0.26209205389022827, 1.8882770538330078] | Test Loss: [3.0799551010131836, 0.35372599959373474, 2.726229190826416]\n",
            "398: Train Loss: [2.015598773956299, 0.36488285660743713, 1.6507158279418945] | Test Loss: [3.083728551864624, 0.3600431978702545, 2.7236852645874023]\n",
            "399: Train Loss: [2.097806453704834, 0.2956506013870239, 1.80215585231781] | Test Loss: [3.0804531574249268, 0.36070722341537476, 2.7197458744049072]\n",
            "400: Train Loss: [2.236116409301758, 0.30720409750938416, 1.9289124011993408] | Test Loss: [3.0725655555725098, 0.3539921045303345, 2.718573570251465]\n",
            "401: Train Loss: [1.9544912576675415, 0.24203039705753326, 1.7124608755111694] | Test Loss: [3.0612614154815674, 0.34320729970932007, 2.7180540561676025]\n",
            "402: Train Loss: [2.1163811683654785, 0.31770867109298706, 1.7986725568771362] | Test Loss: [3.05781888961792, 0.33575356006622314, 2.7220654487609863]\n",
            "403: Train Loss: [1.9774069786071777, 0.3257511258125305, 1.651655912399292] | Test Loss: [3.058032751083374, 0.330239474773407, 2.7277932167053223]\n",
            "404: Train Loss: [1.5868974924087524, 0.37341466546058655, 1.2134828567504883] | Test Loss: [3.068898916244507, 0.3331778049468994, 2.7357211112976074]\n",
            "405: Train Loss: [2.0700464248657227, 0.2835999131202698, 1.7864465713500977] | Test Loss: [3.0760605335235596, 0.3313710689544678, 2.744689464569092]\n",
            "406: Train Loss: [1.7675104141235352, 0.3456168472766876, 1.42189359664917] | Test Loss: [3.0813653469085693, 0.3282395899295807, 2.7531256675720215]\n",
            "407: Train Loss: [2.0292463302612305, 0.31734633445739746, 1.711899995803833] | Test Loss: [3.0862767696380615, 0.3253892958164215, 2.760887384414673]\n",
            "408: Train Loss: [2.1845929622650146, 0.30689945816993713, 1.877693772315979] | Test Loss: [3.0861268043518066, 0.32051339745521545, 2.765613317489624]\n",
            "409: Train Loss: [2.103616714477539, 0.28853166103363037, 1.8150849342346191] | Test Loss: [3.0782880783081055, 0.30902549624443054, 2.7692625522613525]\n",
            "410: Train Loss: [1.8469419479370117, 0.28336286544799805, 1.5635790824890137] | Test Loss: [3.069653034210205, 0.2960662543773651, 2.7735867500305176]\n",
            "411: Train Loss: [1.907975196838379, 0.2528296411037445, 1.655145525932312] | Test Loss: [3.05314302444458, 0.28511345386505127, 2.7680296897888184]\n",
            "412: Train Loss: [2.0630669593811035, 0.26001060009002686, 1.803056240081787] | Test Loss: [3.0397391319274902, 0.27950865030288696, 2.760230541229248]\n",
            "413: Train Loss: [1.7684156894683838, 0.25421249866485596, 1.5142031908035278] | Test Loss: [3.0273895263671875, 0.27678048610687256, 2.7506089210510254]\n",
            "414: Train Loss: [1.9929999113082886, 0.27995410561561584, 1.7130458354949951] | Test Loss: [3.0153870582580566, 0.27723661065101624, 2.7381503582000732]\n",
            "415: Train Loss: [2.174504041671753, 0.29884588718414307, 1.8756581544876099] | Test Loss: [3.0146312713623047, 0.2835956811904907, 2.7310357093811035]\n",
            "416: Train Loss: [2.2982122898101807, 0.30701133608818054, 1.9912010431289673] | Test Loss: [3.023890972137451, 0.29618391394615173, 2.7277071475982666]\n",
            "417: Train Loss: [2.2328476905822754, 0.30115020275115967, 1.9316976070404053] | Test Loss: [3.03812837600708, 0.31318947672843933, 2.7249388694763184]\n",
            "418: Train Loss: [1.9212465286254883, 0.2374594658613205, 1.6837871074676514] | Test Loss: [3.045257329940796, 0.3261825144290924, 2.7190747261047363]\n",
            "419: Train Loss: [1.7736374139785767, 0.34124645590782166, 1.4323909282684326] | Test Loss: [3.0534396171569824, 0.34121882915496826, 2.7122209072113037]\n",
            "420: Train Loss: [1.7888234853744507, 0.2681659162044525, 1.5206575393676758] | Test Loss: [3.0582828521728516, 0.3508188724517822, 2.7074639797210693]\n",
            "421: Train Loss: [2.1340808868408203, 0.2925716042518616, 1.8415093421936035] | Test Loss: [3.0631625652313232, 0.35756975412368774, 2.7055928707122803]\n",
            "422: Train Loss: [1.8886919021606445, 0.2661137282848358, 1.6225781440734863] | Test Loss: [3.063485860824585, 0.35248395800590515, 2.7110018730163574]\n",
            "423: Train Loss: [1.9184726476669312, 0.22281594574451447, 1.695656657218933] | Test Loss: [3.057093620300293, 0.3402092456817627, 2.7168843746185303]\n",
            "424: Train Loss: [2.1563053131103516, 0.2761080265045166, 1.8801974058151245] | Test Loss: [3.0539391040802, 0.3272063732147217, 2.7267327308654785]\n",
            "425: Train Loss: [1.5284662246704102, 0.24321472644805908, 1.285251498222351] | Test Loss: [3.0467262268066406, 0.3127914071083069, 2.7339348793029785]\n",
            "426: Train Loss: [2.047217607498169, 0.2523248791694641, 1.7948927879333496] | Test Loss: [3.0382490158081055, 0.299609899520874, 2.7386391162872314]\n",
            "427: Train Loss: [2.2481493949890137, 0.2657390534877777, 1.9824103116989136] | Test Loss: [3.0300333499908447, 0.28937000036239624, 2.7406632900238037]\n",
            "428: Train Loss: [2.1878528594970703, 0.2713254988193512, 1.916527271270752] | Test Loss: [3.020754098892212, 0.2795322835445404, 2.7412219047546387]\n",
            "429: Train Loss: [2.2675580978393555, 0.30435511469841003, 1.9632030725479126] | Test Loss: [3.0185396671295166, 0.27637237310409546, 2.7421672344207764]\n",
            "430: Train Loss: [1.8216484785079956, 0.3470677137374878, 1.4745807647705078] | Test Loss: [3.0294227600097656, 0.2875053882598877, 2.741917371749878]\n",
            "431: Train Loss: [1.870495080947876, 0.2606813609600067, 1.6098136901855469] | Test Loss: [3.0359301567077637, 0.29816803336143494, 2.737762212753296]\n",
            "432: Train Loss: [2.1767070293426514, 0.33105966448783875, 1.8456474542617798] | Test Loss: [3.0514795780181885, 0.31532806158065796, 2.7361514568328857]\n",
            "433: Train Loss: [2.28185772895813, 0.30919694900512695, 1.972660779953003] | Test Loss: [3.0734341144561768, 0.3356871008872986, 2.7377469539642334]\n",
            "434: Train Loss: [1.9523147344589233, 0.3208000957965851, 1.6315146684646606] | Test Loss: [3.0955071449279785, 0.3596777021884918, 2.7358293533325195]\n",
            "435: Train Loss: [1.967752456665039, 0.28057315945625305, 1.6871793270111084] | Test Loss: [3.1215107440948486, 0.3830604553222656, 2.738450288772583]\n",
            "436: Train Loss: [2.2941741943359375, 0.32793375849723816, 1.966240406036377] | Test Loss: [3.148487091064453, 0.40452367067337036, 2.7439634799957275]\n",
            "437: Train Loss: [2.166862726211548, 0.28204670548439026, 1.88481605052948] | Test Loss: [3.163590669631958, 0.4129539132118225, 2.7506368160247803]\n",
            "438: Train Loss: [1.9467798471450806, 0.3025681674480438, 1.6442116498947144] | Test Loss: [3.1629786491394043, 0.40386199951171875, 2.7591166496276855]\n",
            "439: Train Loss: [2.3351821899414062, 0.3006441295146942, 2.0345380306243896] | Test Loss: [3.1501221656799316, 0.3850783407688141, 2.7650437355041504]\n",
            "440: Train Loss: [1.3278257846832275, 0.3220459818840027, 1.0057798624038696] | Test Loss: [3.14250111579895, 0.37187203764915466, 2.7706291675567627]\n",
            "441: Train Loss: [2.192959785461426, 0.2686064839363098, 1.9243532419204712] | Test Loss: [3.126638889312744, 0.3525208532810211, 2.774117946624756]\n",
            "442: Train Loss: [2.2725353240966797, 0.2856518626213074, 1.986883521080017] | Test Loss: [3.1109721660614014, 0.33440259099006653, 2.7765696048736572]\n",
            "443: Train Loss: [1.992533802986145, 0.3003522455692291, 1.6921815872192383] | Test Loss: [3.100283622741699, 0.3207746148109436, 2.7795090675354004]\n",
            "444: Train Loss: [2.1467483043670654, 0.3155420422554016, 1.831206202507019] | Test Loss: [3.0898568630218506, 0.3093019425868988, 2.780555009841919]\n",
            "445: Train Loss: [2.1667439937591553, 0.24767433106899261, 1.9190696477890015] | Test Loss: [3.081699848175049, 0.298825204372406, 2.782874584197998]\n",
            "446: Train Loss: [1.8797203302383423, 0.28336775302886963, 1.5963525772094727] | Test Loss: [3.0661916732788086, 0.2931905686855316, 2.773001194000244]\n",
            "447: Train Loss: [1.8721835613250732, 0.2987859547138214, 1.5733976364135742] | Test Loss: [3.044325828552246, 0.28735023736953735, 2.7569756507873535]\n",
            "448: Train Loss: [2.1079416275024414, 0.2327267825603485, 1.87521493434906] | Test Loss: [3.01096773147583, 0.2790283262729645, 2.7319393157958984]\n",
            "449: Train Loss: [1.8924586772918701, 0.2510806918144226, 1.6413779258728027] | Test Loss: [2.982532501220703, 0.27135634422302246, 2.7111761569976807]\n",
            "450: Train Loss: [1.7927523851394653, 0.2657506763935089, 1.5270017385482788] | Test Loss: [2.9598352909088135, 0.26511067152023315, 2.6947245597839355]\n",
            "451: Train Loss: [1.6154839992523193, 0.2554000914096832, 1.3600839376449585] | Test Loss: [2.938843011856079, 0.25975051522254944, 2.6790924072265625]\n",
            "452: Train Loss: [2.216681480407715, 0.33025360107421875, 1.8864279985427856] | Test Loss: [2.9250690937042236, 0.25907444953918457, 2.665994644165039]\n",
            "453: Train Loss: [2.108149528503418, 0.28140220046043396, 1.8267472982406616] | Test Loss: [2.9141697883605957, 0.2617996037006378, 2.6523702144622803]\n",
            "454: Train Loss: [2.0031378269195557, 0.2935178875923157, 1.7096198797225952] | Test Loss: [2.910656452178955, 0.26739412546157837, 2.6432623863220215]\n",
            "455: Train Loss: [2.1714279651641846, 0.27315422892570496, 1.8982737064361572] | Test Loss: [2.900886058807373, 0.27180805802345276, 2.629077911376953]\n",
            "456: Train Loss: [2.3096799850463867, 0.2940520644187927, 2.015627861022949] | Test Loss: [2.8942952156066895, 0.27663809061050415, 2.61765718460083]\n",
            "457: Train Loss: [1.9878418445587158, 0.3259628415107727, 1.661879062652588] | Test Loss: [2.8913302421569824, 0.28531068563461304, 2.6060194969177246]\n",
            "458: Train Loss: [2.1346933841705322, 0.26720309257507324, 1.867490291595459] | Test Loss: [2.8786981105804443, 0.2887117862701416, 2.5899863243103027]\n",
            "459: Train Loss: [1.6755928993225098, 0.2870388329029083, 1.3885540962219238] | Test Loss: [2.8676202297210693, 0.2866040766239166, 2.5810160636901855]\n",
            "460: Train Loss: [1.5303945541381836, 0.2684115767478943, 1.2619829177856445] | Test Loss: [2.8552112579345703, 0.28335630893707275, 2.571855068206787]\n",
            "461: Train Loss: [2.196626901626587, 0.29740801453590393, 1.8992189168930054] | Test Loss: [2.8431811332702637, 0.2773909568786621, 2.5657901763916016]\n",
            "462: Train Loss: [1.5079045295715332, 0.3084045350551605, 1.1994999647140503] | Test Loss: [2.835799217224121, 0.27495232224464417, 2.5608468055725098]\n",
            "463: Train Loss: [2.2184934616088867, 0.3093291223049164, 1.9091644287109375] | Test Loss: [2.830155849456787, 0.2766570448875427, 2.5534987449645996]\n",
            "464: Train Loss: [1.7653427124023438, 0.2941780984401703, 1.471164584159851] | Test Loss: [2.826399087905884, 0.27839407324790955, 2.5480051040649414]\n",
            "465: Train Loss: [1.6064209938049316, 0.3628523349761963, 1.2435686588287354] | Test Loss: [2.828667402267456, 0.2851802706718445, 2.543487071990967]\n",
            "466: Train Loss: [1.0870927572250366, 0.29173657298088074, 0.7953561544418335] | Test Loss: [2.8314781188964844, 0.29156315326690674, 2.539914846420288]\n",
            "467: Train Loss: [2.0308542251586914, 0.26439353823661804, 1.766460657119751] | Test Loss: [2.826744794845581, 0.2911837100982666, 2.5355610847473145]\n",
            "468: Train Loss: [1.8165156841278076, 0.27407538890838623, 1.5424402952194214] | Test Loss: [2.8196334838867188, 0.28793805837631226, 2.5316953659057617]\n",
            "469: Train Loss: [1.98389732837677, 0.27231499552726746, 1.7115823030471802] | Test Loss: [2.814866065979004, 0.28538331389427185, 2.529482841491699]\n",
            "470: Train Loss: [1.8202842473983765, 0.25097858905792236, 1.569305658340454] | Test Loss: [2.8080990314483643, 0.28051236271858215, 2.5275866985321045]\n",
            "471: Train Loss: [2.000154495239258, 0.2835061848163605, 1.7166482210159302] | Test Loss: [2.806049108505249, 0.280170202255249, 2.52587890625]\n",
            "472: Train Loss: [1.968557596206665, 0.3652389347553253, 1.603318691253662] | Test Loss: [2.8135666847229004, 0.28588858246803284, 2.5276780128479004]\n",
            "473: Train Loss: [2.058422327041626, 0.35406872630119324, 1.7043536901474] | Test Loss: [2.82832670211792, 0.2964520752429962, 2.531874656677246]\n",
            "474: Train Loss: [2.3232638835906982, 0.2706851363182068, 2.0525786876678467] | Test Loss: [2.8524365425109863, 0.3127564489841461, 2.539680004119873]\n",
            "475: Train Loss: [1.861995816230774, 0.2918509542942047, 1.5701448917388916] | Test Loss: [2.879197835922241, 0.32970815896987915, 2.549489736557007]\n",
            "476: Train Loss: [1.174385905265808, 0.3098698854446411, 0.864516019821167] | Test Loss: [2.905073881149292, 0.34640225768089294, 2.558671712875366]\n",
            "477: Train Loss: [1.5385198593139648, 0.23458008468151093, 1.3039398193359375] | Test Loss: [2.9237780570983887, 0.3590184450149536, 2.5647597312927246]\n",
            "478: Train Loss: [2.0081725120544434, 0.2792550325393677, 1.7289175987243652] | Test Loss: [2.9409632682800293, 0.3697548806667328, 2.5712084770202637]\n",
            "479: Train Loss: [1.9194939136505127, 0.25246161222457886, 1.6670323610305786] | Test Loss: [2.952949047088623, 0.3722272515296936, 2.580721855163574]\n",
            "480: Train Loss: [2.118133306503296, 0.3075908422470093, 1.8105424642562866] | Test Loss: [2.9529032707214355, 0.36655035614967346, 2.586352825164795]\n",
            "481: Train Loss: [2.180814743041992, 0.28537288308143616, 1.8954418897628784] | Test Loss: [2.9516611099243164, 0.35902103781700134, 2.5926401615142822]\n",
            "482: Train Loss: [2.0960614681243896, 0.30274343490600586, 1.7933180332183838] | Test Loss: [2.9612843990325928, 0.35968294739723206, 2.6016013622283936]\n",
            "483: Train Loss: [2.0592150688171387, 0.322352796792984, 1.7368621826171875] | Test Loss: [2.961364507675171, 0.35579895973205566, 2.6055655479431152]\n",
            "484: Train Loss: [1.5313962697982788, 0.29144492745399475, 1.2399513721466064] | Test Loss: [2.9641430377960205, 0.35174840688705444, 2.6123945713043213]\n",
            "485: Train Loss: [2.097621440887451, 0.2928534150123596, 1.8047680854797363] | Test Loss: [2.972630023956299, 0.3526420295238495, 2.619987964630127]\n",
            "486: Train Loss: [2.1851041316986084, 0.34566730260849, 1.8394371271133423] | Test Loss: [2.9960639476776123, 0.35842910408973694, 2.637634754180908]\n",
            "487: Train Loss: [2.2087368965148926, 0.3101811707019806, 1.8985556364059448] | Test Loss: [3.019721269607544, 0.3662613332271576, 2.6534600257873535]\n",
            "488: Train Loss: [2.0615668296813965, 0.2912788689136505, 1.7702879905700684] | Test Loss: [3.0328569412231445, 0.366601824760437, 2.666254997253418]\n",
            "489: Train Loss: [1.893862247467041, 0.24497462809085846, 1.6488877534866333] | Test Loss: [3.0407967567443848, 0.36168962717056274, 2.679107189178467]\n",
            "490: Train Loss: [1.8611658811569214, 0.3110927641391754, 1.5500731468200684] | Test Loss: [3.05051851272583, 0.3600710928440094, 2.6904473304748535]\n",
            "491: Train Loss: [1.7324645519256592, 0.2826278805732727, 1.4498368501663208] | Test Loss: [3.0559213161468506, 0.3525678217411041, 2.7033534049987793]\n",
            "492: Train Loss: [1.9871846437454224, 0.2632687985897064, 1.7239158153533936] | Test Loss: [3.053318738937378, 0.3423878252506256, 2.710930824279785]\n",
            "493: Train Loss: [2.1500980854034424, 0.31916067004203796, 1.830937385559082] | Test Loss: [3.0462212562561035, 0.33173173666000366, 2.714489459991455]\n",
            "494: Train Loss: [1.5685889720916748, 0.2690635025501251, 1.2995253801345825] | Test Loss: [3.0334646701812744, 0.3172033429145813, 2.716261386871338]\n",
            "495: Train Loss: [1.7602472305297852, 0.26013171672821045, 1.5001155138015747] | Test Loss: [3.0177643299102783, 0.30041712522506714, 2.7173471450805664]\n",
            "496: Train Loss: [2.1834475994110107, 0.2731228768825531, 1.9103246927261353] | Test Loss: [3.0056893825531006, 0.2870686650276184, 2.718620777130127]\n",
            "497: Train Loss: [2.0435566902160645, 0.38762184977531433, 1.6559349298477173] | Test Loss: [3.0095248222351074, 0.2887263298034668, 2.7207984924316406]\n",
            "498: Train Loss: [1.8404927253723145, 0.2822265326976776, 1.5582661628723145] | Test Loss: [3.0124268531799316, 0.28761205077171326, 2.7248148918151855]\n",
            "499: Train Loss: [1.9431180953979492, 0.30544573068618774, 1.6376723051071167] | Test Loss: [3.020106792449951, 0.2914744019508362, 2.7286324501037598]\n",
            "500: Train Loss: [1.9313791990280151, 0.2465248852968216, 1.68485426902771] | Test Loss: [3.028123140335083, 0.29732394218444824, 2.7307991981506348]\n",
            "501: Train Loss: [1.9186724424362183, 0.2571515142917633, 1.6615209579467773] | Test Loss: [3.0350899696350098, 0.2989100515842438, 2.736179828643799]\n",
            "502: Train Loss: [1.9335877895355225, 0.29119500517845154, 1.6423927545547485] | Test Loss: [3.0368406772613525, 0.30010107159614563, 2.7367396354675293]\n",
            "503: Train Loss: [1.8905794620513916, 0.23852083086967468, 1.6520586013793945] | Test Loss: [3.0411107540130615, 0.300303190946579, 2.74080753326416]\n",
            "504: Train Loss: [2.165837287902832, 0.30028778314590454, 1.8655495643615723] | Test Loss: [3.0409600734710693, 0.3002350330352783, 2.740725040435791]\n",
            "505: Train Loss: [1.5937411785125732, 0.3409198522567749, 1.252821445465088] | Test Loss: [3.053191661834717, 0.3058295249938965, 2.7473621368408203]\n",
            "506: Train Loss: [2.1257362365722656, 0.31047406792640686, 1.8152621984481812] | Test Loss: [3.0706214904785156, 0.3148656487464905, 2.75575590133667]\n",
            "507: Train Loss: [1.6925560235977173, 0.32018089294433594, 1.3723751306533813] | Test Loss: [3.0881478786468506, 0.3265727162361145, 2.761575222015381]\n",
            "508: Train Loss: [1.6098108291625977, 0.27056270837783813, 1.3392481803894043] | Test Loss: [3.1000781059265137, 0.33567434549331665, 2.764403820037842]\n",
            "509: Train Loss: [2.19688081741333, 0.2751205563545227, 1.9217603206634521] | Test Loss: [3.104572057723999, 0.34129026532173157, 2.76328182220459]\n",
            "510: Train Loss: [1.7813948392868042, 0.27079904079437256, 1.5105959177017212] | Test Loss: [3.0973434448242188, 0.33887267112731934, 2.7584707736968994]\n",
            "511: Train Loss: [2.051069736480713, 0.2660169303417206, 1.7850526571273804] | Test Loss: [3.080396890640259, 0.33119750022888184, 2.749199390411377]\n",
            "512: Train Loss: [1.9061429500579834, 0.27890270948410034, 1.6272403001785278] | Test Loss: [3.0551741123199463, 0.3194511830806732, 2.7357230186462402]\n",
            "513: Train Loss: [1.9829063415527344, 0.3952765166759491, 1.587629795074463] | Test Loss: [3.0368454456329346, 0.31362414360046387, 2.7232213020324707]\n",
            "514: Train Loss: [1.9173412322998047, 0.27235934138298035, 1.644981861114502] | Test Loss: [3.0119950771331787, 0.3108151853084564, 2.7011799812316895]\n",
            "515: Train Loss: [2.0638952255249023, 0.2699204683303833, 1.7939748764038086] | Test Loss: [2.991037130355835, 0.30557742714881897, 2.685459613800049]\n",
            "516: Train Loss: [1.8679125308990479, 0.29525068402290344, 1.5726618766784668] | Test Loss: [2.974722146987915, 0.3024018704891205, 2.6723203659057617]\n",
            "517: Train Loss: [1.7790513038635254, 0.29444560408592224, 1.4846056699752808] | Test Loss: [2.9659037590026855, 0.2999979257583618, 2.6659059524536133]\n",
            "518: Train Loss: [1.924144983291626, 0.319503515958786, 1.6046414375305176] | Test Loss: [2.967177152633667, 0.3008999228477478, 2.6662771701812744]\n",
            "519: Train Loss: [1.8734700679779053, 0.27620774507522583, 1.5972622632980347] | Test Loss: [2.972496271133423, 0.3037722110748291, 2.6687240600585938]\n",
            "520: Train Loss: [1.9246577024459839, 0.2534879744052887, 1.6711698770523071] | Test Loss: [2.978637218475342, 0.30295976996421814, 2.675677537918091]\n",
            "521: Train Loss: [2.1000280380249023, 0.29219940304756165, 1.807828664779663] | Test Loss: [2.980914354324341, 0.3020303249359131, 2.6788840293884277]\n",
            "522: Train Loss: [2.206178903579712, 0.26937323808670044, 1.9368057250976562] | Test Loss: [2.984529972076416, 0.30058446526527405, 2.683945417404175]\n",
            "523: Train Loss: [2.0356626510620117, 0.26176872849464417, 1.77389395236969] | Test Loss: [2.981447696685791, 0.2979191541671753, 2.683528423309326]\n",
            "Epoch 4\n",
            "0: Train Loss: [1.9106037616729736, 0.2532053291797638, 1.6573984622955322] | Test Loss: [2.9748382568359375, 0.29770511388778687, 2.677133083343506]\n",
            "1: Train Loss: [1.9175021648406982, 0.32058170437812805, 1.596920371055603] | Test Loss: [2.974604606628418, 0.3010108172893524, 2.673593759536743]\n",
            "2: Train Loss: [1.8858826160430908, 0.2597636580467224, 1.6261188983917236] | Test Loss: [2.9747867584228516, 0.30433154106140137, 2.67045521736145]\n",
            "3: Train Loss: [1.9400293827056885, 0.2889249920845032, 1.6511043310165405] | Test Loss: [2.971853494644165, 0.30990585684776306, 2.661947727203369]\n",
            "4: Train Loss: [1.1355293989181519, 0.28077930212020874, 0.8547500967979431] | Test Loss: [2.9738001823425293, 0.31810086965560913, 2.6556992530822754]\n",
            "5: Train Loss: [1.4962782859802246, 0.23861336708068848, 1.2576649188995361] | Test Loss: [2.972630739212036, 0.32120394706726074, 2.6514267921447754]\n",
            "6: Train Loss: [1.6779201030731201, 0.2527534067630768, 1.4251667261123657] | Test Loss: [2.9742937088012695, 0.3245391547679901, 2.649754524230957]\n",
            "7: Train Loss: [1.7418479919433594, 0.2344384789466858, 1.5074094533920288] | Test Loss: [2.973358631134033, 0.32096466422080994, 2.6523940563201904]\n",
            "8: Train Loss: [2.1185038089752197, 0.25822076201438904, 1.8602831363677979] | Test Loss: [2.9683468341827393, 0.3133767545223236, 2.654970169067383]\n",
            "9: Train Loss: [1.7926688194274902, 0.21771098673343658, 1.5749578475952148] | Test Loss: [2.9623332023620605, 0.3027728796005249, 2.659560441970825]\n",
            "10: Train Loss: [2.009133815765381, 0.29605528712272644, 1.713078498840332] | Test Loss: [2.956169366836548, 0.29330238699913025, 2.6628670692443848]\n",
            "11: Train Loss: [1.8426027297973633, 0.2411206066608429, 1.6014821529388428] | Test Loss: [2.949618101119995, 0.28286653757095337, 2.6667516231536865]\n",
            "12: Train Loss: [2.2370729446411133, 0.2979552745819092, 1.939117670059204] | Test Loss: [2.946777820587158, 0.2766173779964447, 2.6701605319976807]\n",
            "13: Train Loss: [1.9872459173202515, 0.28653958439826965, 1.7007063627243042] | Test Loss: [2.9511911869049072, 0.27871206402778625, 2.6724791526794434]\n",
            "14: Train Loss: [1.6209205389022827, 0.3232983648777008, 1.2976222038269043] | Test Loss: [2.9642539024353027, 0.2907463610172272, 2.6735074520111084]\n",
            "15: Train Loss: [1.8104441165924072, 0.2505730986595154, 1.5598710775375366] | Test Loss: [2.9774341583251953, 0.30194130539894104, 2.675492763519287]\n",
            "16: Train Loss: [1.8470921516418457, 0.2713530957698822, 1.5757390260696411] | Test Loss: [2.9815011024475098, 0.3124151825904846, 2.66908597946167]\n",
            "17: Train Loss: [1.9575525522232056, 0.28530433773994446, 1.6722482442855835] | Test Loss: [2.9820384979248047, 0.3251575231552124, 2.6568808555603027]\n",
            "18: Train Loss: [1.8952962160110474, 0.28019794821739197, 1.615098237991333] | Test Loss: [2.9765329360961914, 0.33488500118255615, 2.641648054122925]\n",
            "19: Train Loss: [2.093250036239624, 0.3001333177089691, 1.7931166887283325] | Test Loss: [2.970590829849243, 0.34110227227211, 2.629488468170166]\n",
            "20: Train Loss: [2.20582914352417, 0.25779208540916443, 1.948037028312683] | Test Loss: [2.949784517288208, 0.3328988254070282, 2.6168856620788574]\n",
            "21: Train Loss: [1.0181403160095215, 0.2564464211463928, 0.7616939544677734] | Test Loss: [2.9266107082366943, 0.3179751932621002, 2.608635425567627]\n",
            "22: Train Loss: [2.2004077434539795, 0.25357645750045776, 1.9468313455581665] | Test Loss: [2.902576208114624, 0.29978564381599426, 2.602790594100952]\n",
            "23: Train Loss: [1.733513593673706, 0.23179161548614502, 1.501721978187561] | Test Loss: [2.8843135833740234, 0.2830128073692322, 2.6013007164001465]\n",
            "24: Train Loss: [1.938053846359253, 0.2870613932609558, 1.650992512702942] | Test Loss: [2.8742823600769043, 0.27176061272621155, 2.6025216579437256]\n",
            "25: Train Loss: [1.8676960468292236, 0.2784299850463867, 1.589266061782837] | Test Loss: [2.8731608390808105, 0.26601871848106384, 2.607142210006714]\n",
            "26: Train Loss: [1.8310823440551758, 0.25861385464668274, 1.5724685192108154] | Test Loss: [2.8742523193359375, 0.2654295265674591, 2.608822822570801]\n",
            "27: Train Loss: [1.7257270812988281, 0.23875769972801208, 1.4869693517684937] | Test Loss: [2.869021415710449, 0.2673744857311249, 2.601646900177002]\n",
            "28: Train Loss: [1.6976726055145264, 0.2611187696456909, 1.4365538358688354] | Test Loss: [2.8693714141845703, 0.27153193950653076, 2.59783935546875]\n",
            "29: Train Loss: [1.5387316942214966, 0.3325558006763458, 1.2061759233474731] | Test Loss: [2.8793694972991943, 0.2829229235649109, 2.5964465141296387]\n",
            "30: Train Loss: [2.0607194900512695, 0.25086694955825806, 1.8098524808883667] | Test Loss: [2.892239570617676, 0.2925516664981842, 2.5996878147125244]\n",
            "31: Train Loss: [1.9592082500457764, 0.27937549352645874, 1.6798326969146729] | Test Loss: [2.9075450897216797, 0.3049735128879547, 2.602571487426758]\n",
            "32: Train Loss: [1.8065907955169678, 0.2974783778190613, 1.5091122388839722] | Test Loss: [2.9281044006347656, 0.31770646572113037, 2.610398054122925]\n",
            "33: Train Loss: [2.076331615447998, 0.29310542345046997, 1.7832262516021729] | Test Loss: [2.954022169113159, 0.3288783133029938, 2.6251437664031982]\n",
            "34: Train Loss: [1.2759439945220947, 0.308119535446167, 0.967824399471283] | Test Loss: [2.979633331298828, 0.3391701579093933, 2.64046311378479]\n",
            "35: Train Loss: [2.230945587158203, 0.2674432694911957, 1.963502287864685] | Test Loss: [3.000993490219116, 0.3436509668827057, 2.6573424339294434]\n",
            "36: Train Loss: [1.9910833835601807, 0.27357685565948486, 1.7175065279006958] | Test Loss: [3.019174337387085, 0.34018900990486145, 2.678985357284546]\n",
            "37: Train Loss: [2.0661041736602783, 0.2791387736797333, 1.7869654893875122] | Test Loss: [3.028521776199341, 0.3306529223918915, 2.697868824005127]\n",
            "38: Train Loss: [1.5346355438232422, 0.2687839865684509, 1.265851616859436] | Test Loss: [3.027031183242798, 0.3201845586299896, 2.7068467140197754]\n",
            "39: Train Loss: [1.5556087493896484, 0.27356332540512085, 1.2820453643798828] | Test Loss: [3.0276081562042236, 0.3136162757873535, 2.71399188041687]\n",
            "40: Train Loss: [1.7693077325820923, 0.3140815496444702, 1.455226182937622] | Test Loss: [3.0329654216766357, 0.31380516290664673, 2.719160318374634]\n",
            "41: Train Loss: [2.0838818550109863, 0.2704979181289673, 1.8133840560913086] | Test Loss: [3.0475268363952637, 0.31848442554473877, 2.7290425300598145]\n",
            "42: Train Loss: [1.8405848741531372, 0.2700079679489136, 1.5705769062042236] | Test Loss: [3.047654390335083, 0.32153505086898804, 2.72611927986145]\n",
            "43: Train Loss: [2.148859739303589, 0.2589119076728821, 1.8899478912353516] | Test Loss: [3.041093587875366, 0.32270076870918274, 2.718392848968506]\n",
            "44: Train Loss: [1.5551594495773315, 0.26234784722328186, 1.292811632156372] | Test Loss: [3.0302655696868896, 0.31917116045951843, 2.711094379425049]\n",
            "45: Train Loss: [2.0733554363250732, 0.2638760507106781, 1.8094793558120728] | Test Loss: [3.0134263038635254, 0.30990129709243774, 2.7035250663757324]\n",
            "46: Train Loss: [1.5373568534851074, 0.27640536427497864, 1.2609515190124512] | Test Loss: [3.0079574584960938, 0.3102739453315735, 2.697683572769165]\n",
            "47: Train Loss: [2.0780296325683594, 0.33579882979393005, 1.742230772972107] | Test Loss: [3.015232801437378, 0.3248121440410614, 2.690420627593994]\n",
            "48: Train Loss: [1.7082833051681519, 0.30359551310539246, 1.404687762260437] | Test Loss: [3.0209782123565674, 0.3373587429523468, 2.683619499206543]\n",
            "49: Train Loss: [1.899828314781189, 0.24172620475292206, 1.6581021547317505] | Test Loss: [3.024833917617798, 0.3471098840236664, 2.6777241230010986]\n",
            "50: Train Loss: [0.7732799053192139, 0.29389330744743347, 0.4793866276741028] | Test Loss: [3.0287070274353027, 0.35664045810699463, 2.6720664501190186]\n",
            "51: Train Loss: [2.077674388885498, 0.2605495750904083, 1.817124843597412] | Test Loss: [3.0335495471954346, 0.3619515895843506, 2.671597957611084]\n",
            "52: Train Loss: [1.6613225936889648, 0.2759113907814026, 1.385411262512207] | Test Loss: [3.0359854698181152, 0.36462968587875366, 2.671355724334717]\n",
            "53: Train Loss: [1.0282810926437378, 0.29800301790237427, 0.7302780747413635] | Test Loss: [3.043574333190918, 0.3692064881324768, 2.674367904663086]\n",
            "54: Train Loss: [0.5400393605232239, 0.3113560676574707, 0.22868330776691437] | Test Loss: [3.057077646255493, 0.3783266246318817, 2.678750991821289]\n",
            "55: Train Loss: [1.927736520767212, 0.2702668011188507, 1.6574697494506836] | Test Loss: [3.0577354431152344, 0.378883957862854, 2.67885160446167]\n",
            "56: Train Loss: [1.8602042198181152, 0.2690931260585785, 1.5911110639572144] | Test Loss: [3.0578372478485107, 0.3768211007118225, 2.681016206741333]\n",
            "57: Train Loss: [1.9839473962783813, 0.28079450130462646, 1.7031528949737549] | Test Loss: [3.0548713207244873, 0.3726804554462433, 2.6821908950805664]\n",
            "58: Train Loss: [1.8362329006195068, 0.27208295464515686, 1.5641499757766724] | Test Loss: [3.0549607276916504, 0.37386661767959595, 2.681094169616699]\n",
            "59: Train Loss: [1.8417623043060303, 0.28008192777633667, 1.5616804361343384] | Test Loss: [3.0652053356170654, 0.3803820312023163, 2.6848232746124268]\n",
            "60: Train Loss: [2.2379543781280518, 0.2770262062549591, 1.9609280824661255] | Test Loss: [3.082442045211792, 0.3870430886745453, 2.695399045944214]\n",
            "61: Train Loss: [2.169999599456787, 0.2839389145374298, 1.8860607147216797] | Test Loss: [3.1075844764709473, 0.3946591019630432, 2.712925434112549]\n",
            "62: Train Loss: [1.8011223077774048, 0.2829838693141937, 1.5181384086608887] | Test Loss: [3.126945972442627, 0.39745989441871643, 2.7294859886169434]\n",
            "63: Train Loss: [2.167269468307495, 0.3454263508319855, 1.8218430280685425] | Test Loss: [3.145948648452759, 0.40209630131721497, 2.743852376937866]\n",
            "64: Train Loss: [1.447730541229248, 0.28611186146736145, 1.161618709564209] | Test Loss: [3.1594836711883545, 0.40218526124954224, 2.757298469543457]\n",
            "65: Train Loss: [1.8458709716796875, 0.25355929136276245, 1.5923116207122803] | Test Loss: [3.1493282318115234, 0.3870675265789032, 2.762260675430298]\n",
            "66: Train Loss: [1.977311372756958, 0.3016354739665985, 1.675675868988037] | Test Loss: [3.1441287994384766, 0.3796754479408264, 2.764453411102295]\n",
            "67: Train Loss: [1.9882618188858032, 0.3447248041629791, 1.6435370445251465] | Test Loss: [3.141887903213501, 0.37805166840553284, 2.763836145401001]\n",
            "68: Train Loss: [2.2369086742401123, 0.28333911299705505, 1.9535696506500244] | Test Loss: [3.131028652191162, 0.3756791353225708, 2.755349636077881]\n",
            "69: Train Loss: [1.9285353422164917, 0.27996566891670227, 1.6485697031021118] | Test Loss: [3.1039226055145264, 0.3615933954715729, 2.7423291206359863]\n",
            "70: Train Loss: [1.8129277229309082, 0.2765589952468872, 1.536368727684021] | Test Loss: [3.0753657817840576, 0.34595802426338196, 2.729407787322998]\n",
            "71: Train Loss: [2.093858480453491, 0.2935577929019928, 1.8003004789352417] | Test Loss: [3.050354242324829, 0.3333519399166107, 2.7170023918151855]\n",
            "72: Train Loss: [1.9113740921020508, 0.28552329540252686, 1.625850796699524] | Test Loss: [3.0402560234069824, 0.32796987891197205, 2.7122862339019775]\n",
            "73: Train Loss: [1.762300968170166, 0.27004748582839966, 1.4922535419464111] | Test Loss: [3.028679132461548, 0.3224031925201416, 2.7062759399414062]\n",
            "74: Train Loss: [1.6297146081924438, 0.24885427951812744, 1.3808603286743164] | Test Loss: [3.0211827754974365, 0.31337422132492065, 2.707808494567871]\n",
            "75: Train Loss: [2.1768338680267334, 0.24860650300979614, 1.928227424621582] | Test Loss: [3.0214643478393555, 0.3079855442047119, 2.7134788036346436]\n",
            "76: Train Loss: [1.5821467638015747, 0.30000829696655273, 1.282138466835022] | Test Loss: [3.029629707336426, 0.30637413263320923, 2.7232556343078613]\n",
            "77: Train Loss: [1.5817710161209106, 0.2902103364467621, 1.2915606498718262] | Test Loss: [3.0443296432495117, 0.31020504236221313, 2.7341246604919434]\n",
            "78: Train Loss: [1.8638571500778198, 0.25242850184440613, 1.6114287376403809] | Test Loss: [3.0625388622283936, 0.31783947348594666, 2.744699478149414]\n",
            "79: Train Loss: [1.759289264678955, 0.2427258938550949, 1.5165634155273438] | Test Loss: [3.0746428966522217, 0.3259081542491913, 2.748734712600708]\n",
            "80: Train Loss: [1.791290521621704, 0.2730344831943512, 1.5182560682296753] | Test Loss: [3.0890743732452393, 0.33779722452163696, 2.751277208328247]\n",
            "81: Train Loss: [2.1380040645599365, 0.2565998435020447, 1.8814042806625366] | Test Loss: [3.104635715484619, 0.3466373085975647, 2.757998466491699]\n",
            "82: Train Loss: [1.9365124702453613, 0.30190396308898926, 1.634608507156372] | Test Loss: [3.1188149452209473, 0.3557780683040619, 2.7630369663238525]\n",
            "83: Train Loss: [1.9187090396881104, 0.2592087686061859, 1.659500241279602] | Test Loss: [3.1324591636657715, 0.36374226212501526, 2.768716812133789]\n",
            "84: Train Loss: [1.102075219154358, 0.2736864984035492, 0.8283887505531311] | Test Loss: [3.1462950706481934, 0.3692294657230377, 2.7770655155181885]\n",
            "85: Train Loss: [1.9116321802139282, 0.2985304892063141, 1.6131017208099365] | Test Loss: [3.1689579486846924, 0.38127607107162476, 2.787681818008423]\n",
            "86: Train Loss: [1.8974053859710693, 0.2766326367855072, 1.6207727193832397] | Test Loss: [3.1805202960968018, 0.38965877890586853, 2.7908616065979004]\n",
            "87: Train Loss: [1.626033067703247, 0.28089913725852966, 1.345133900642395] | Test Loss: [3.1838033199310303, 0.39326271414756775, 2.7905406951904297]\n",
            "88: Train Loss: [1.5465233325958252, 0.2685105502605438, 1.278012752532959] | Test Loss: [3.1809022426605225, 0.3905671536922455, 2.790335178375244]\n",
            "89: Train Loss: [1.8642441034317017, 0.27715662121772766, 1.5870875120162964] | Test Loss: [3.172354221343994, 0.3834265172481537, 2.7889277935028076]\n",
            "90: Train Loss: [1.9047667980194092, 0.24482020735740662, 1.6599465608596802] | Test Loss: [3.16387677192688, 0.373125821352005, 2.7907509803771973]\n",
            "91: Train Loss: [2.137434244155884, 0.30703097581863403, 1.830403208732605] | Test Loss: [3.1653878688812256, 0.371699720621109, 2.7936880588531494]\n",
            "92: Train Loss: [0.8923938274383545, 0.25976869463920593, 0.632625162601471] | Test Loss: [3.170522689819336, 0.37406057119369507, 2.796462059020996]\n",
            "93: Train Loss: [1.917694330215454, 0.24303649365901947, 1.6746578216552734] | Test Loss: [3.177008628845215, 0.3764911890029907, 2.8005175590515137]\n",
            "94: Train Loss: [2.0068347454071045, 0.2764081060886383, 1.730426549911499] | Test Loss: [3.1769485473632812, 0.38262513279914856, 2.794323444366455]\n",
            "95: Train Loss: [1.9888646602630615, 0.2932172417640686, 1.6956474781036377] | Test Loss: [3.185307741165161, 0.39484161138534546, 2.790466070175171]\n",
            "96: Train Loss: [1.7836495637893677, 0.2526423931121826, 1.5310070514678955] | Test Loss: [3.195140838623047, 0.40243712067604065, 2.792703628540039]\n",
            "97: Train Loss: [1.8992304801940918, 0.2427389919757843, 1.6564915180206299] | Test Loss: [3.1846888065338135, 0.3974018096923828, 2.7872869968414307]\n",
            "98: Train Loss: [1.654170036315918, 0.2579001486301422, 1.3962699174880981] | Test Loss: [3.1719415187835693, 0.3879837393760681, 2.7839577198028564]\n",
            "99: Train Loss: [1.4791702032089233, 0.2582038938999176, 1.2209663391113281] | Test Loss: [3.170536518096924, 0.3855842053890228, 2.784952402114868]\n",
            "100: Train Loss: [2.0939671993255615, 0.2559208273887634, 1.8380463123321533] | Test Loss: [3.1643247604370117, 0.3809429705142975, 2.783381700515747]\n",
            "101: Train Loss: [1.9865939617156982, 0.2759368121623993, 1.7106571197509766] | Test Loss: [3.1615777015686035, 0.3808338940143585, 2.7807438373565674]\n",
            "102: Train Loss: [1.7740718126296997, 0.25692620873451233, 1.5171456336975098] | Test Loss: [3.1596567630767822, 0.3788541257381439, 2.7808027267456055]\n",
            "103: Train Loss: [1.9291070699691772, 0.29529067873954773, 1.6338163614273071] | Test Loss: [3.157684803009033, 0.3790399134159088, 2.7786448001861572]\n",
            "104: Train Loss: [2.109999656677246, 0.2756846249103546, 1.8343150615692139] | Test Loss: [3.1453723907470703, 0.37496304512023926, 2.770409345626831]\n",
            "105: Train Loss: [2.119255781173706, 0.269513338804245, 1.8497424125671387] | Test Loss: [3.123518466949463, 0.3668394684791565, 2.756679058074951]\n",
            "106: Train Loss: [1.9063034057617188, 0.24059778451919556, 1.665705680847168] | Test Loss: [3.097838878631592, 0.3575723171234131, 2.7402665615081787]\n",
            "107: Train Loss: [1.8490649461746216, 0.24493928253650665, 1.6041256189346313] | Test Loss: [3.0815346240997314, 0.34638381004333496, 2.7351508140563965]\n",
            "108: Train Loss: [1.8539777994155884, 0.2675720453262329, 1.5864057540893555] | Test Loss: [3.0797836780548096, 0.34636759757995605, 2.7334160804748535]\n",
            "109: Train Loss: [2.2552387714385986, 0.26070287823677063, 1.9945358037948608] | Test Loss: [3.085662841796875, 0.3510377109050751, 2.7346251010894775]\n",
            "110: Train Loss: [1.9608776569366455, 0.26537778973579407, 1.6954998970031738] | Test Loss: [3.0959360599517822, 0.35827916860580444, 2.737656831741333]\n",
            "111: Train Loss: [1.578468680381775, 0.36317458748817444, 1.2152941226959229] | Test Loss: [3.121274709701538, 0.3808419704437256, 2.7404327392578125]\n",
            "112: Train Loss: [1.8656085729599, 0.265506774187088, 1.6001018285751343] | Test Loss: [3.151008367538452, 0.40625929832458496, 2.744749069213867]\n",
            "113: Train Loss: [1.5569252967834473, 0.2814198136329651, 1.2755054235458374] | Test Loss: [3.166393756866455, 0.4182652533054352, 2.7481284141540527]\n",
            "114: Train Loss: [1.928826093673706, 0.25530821084976196, 1.6735179424285889] | Test Loss: [3.159554958343506, 0.4105519652366638, 2.7490029335021973]\n",
            "115: Train Loss: [1.8566746711730957, 0.26896801590919495, 1.5877066850662231] | Test Loss: [3.1455609798431396, 0.39722102880477905, 2.748339891433716]\n",
            "116: Train Loss: [1.9139854907989502, 0.23813115060329437, 1.6758543252944946] | Test Loss: [3.131040096282959, 0.3824644088745117, 2.7485756874084473]\n",
            "117: Train Loss: [1.8265637159347534, 0.22565646469593048, 1.6009072065353394] | Test Loss: [3.0983712673187256, 0.36102715134620667, 2.7373440265655518]\n",
            "118: Train Loss: [2.0904269218444824, 0.2795804738998413, 1.8108465671539307] | Test Loss: [3.075857400894165, 0.3518216013908386, 2.7240357398986816]\n",
            "119: Train Loss: [1.924306035041809, 0.2660689353942871, 1.658237099647522] | Test Loss: [3.0697920322418213, 0.35335516929626465, 2.7164368629455566]\n",
            "120: Train Loss: [1.9260622262954712, 0.2900237739086151, 1.6360384225845337] | Test Loss: [3.0636632442474365, 0.36016321182250977, 2.7035000324249268]\n",
            "121: Train Loss: [1.9724969863891602, 0.3091347813606262, 1.6633621454238892] | Test Loss: [3.057882308959961, 0.36439192295074463, 2.6934902667999268]\n",
            "122: Train Loss: [1.9605871438980103, 0.24524463713169098, 1.7153425216674805] | Test Loss: [3.056558609008789, 0.366767942905426, 2.689790725708008]\n",
            "123: Train Loss: [1.4460828304290771, 0.2628370523452759, 1.1832457780838013] | Test Loss: [3.051567792892456, 0.36325860023498535, 2.6883091926574707]\n",
            "124: Train Loss: [1.9393391609191895, 0.29360005259513855, 1.6457390785217285] | Test Loss: [3.0333006381988525, 0.35764846205711365, 2.675652265548706]\n",
            "125: Train Loss: [2.010991334915161, 0.2949482798576355, 1.7160429954528809] | Test Loss: [3.014418840408325, 0.3540477752685547, 2.6603710651397705]\n",
            "126: Train Loss: [1.8396520614624023, 0.2731388211250305, 1.566513180732727] | Test Loss: [2.9863498210906982, 0.3458174765110016, 2.6405322551727295]\n",
            "127: Train Loss: [2.0685267448425293, 0.24022461473941803, 1.8283021450042725] | Test Loss: [2.9543638229370117, 0.3339405953884125, 2.6204233169555664]\n",
            "128: Train Loss: [1.7435708045959473, 0.3093966841697693, 1.4341741800308228] | Test Loss: [2.9304628372192383, 0.32644230127334595, 2.604020595550537]\n",
            "129: Train Loss: [1.9574477672576904, 0.2636110186576843, 1.6938368082046509] | Test Loss: [2.914750337600708, 0.32144513726234436, 2.5933051109313965]\n",
            "130: Train Loss: [2.021231174468994, 0.30541789531707764, 1.715813159942627] | Test Loss: [2.9062514305114746, 0.32158392667770386, 2.584667444229126]\n",
            "131: Train Loss: [2.056384563446045, 0.24373000860214233, 1.8126544952392578] | Test Loss: [2.895778179168701, 0.3205345869064331, 2.5752437114715576]\n",
            "132: Train Loss: [1.9429348707199097, 0.3463766872882843, 1.5965582132339478] | Test Loss: [2.9111931324005127, 0.34484878182411194, 2.5663442611694336]\n",
            "133: Train Loss: [1.8060975074768066, 0.2867209315299988, 1.5193766355514526] | Test Loss: [2.9241855144500732, 0.36715584993362427, 2.5570297241210938]\n",
            "134: Train Loss: [1.8138906955718994, 0.3484996557235718, 1.4653910398483276] | Test Loss: [2.945561170578003, 0.3919617831707001, 2.5535993576049805]\n",
            "135: Train Loss: [1.947197675704956, 0.2630724310874939, 1.684125304222107] | Test Loss: [2.944852352142334, 0.4014224112033844, 2.5434298515319824]\n",
            "136: Train Loss: [2.0411102771759033, 0.2907421588897705, 1.7503681182861328] | Test Loss: [2.9387574195861816, 0.40605735778808594, 2.5327000617980957]\n",
            "137: Train Loss: [1.839112639427185, 0.2793983221054077, 1.5597143173217773] | Test Loss: [2.9166784286499023, 0.39217525720596313, 2.524503231048584]\n",
            "138: Train Loss: [2.07037353515625, 0.2956126034259796, 1.7747609615325928] | Test Loss: [2.889345169067383, 0.3731788396835327, 2.5161662101745605]\n",
            "139: Train Loss: [1.9622631072998047, 0.26622098684310913, 1.6960421800613403] | Test Loss: [2.8604722023010254, 0.35189127922058105, 2.5085809230804443]\n",
            "140: Train Loss: [1.8025308847427368, 0.2597450315952301, 1.542785882949829] | Test Loss: [2.842440128326416, 0.3362564444541931, 2.506183624267578]\n",
            "141: Train Loss: [2.206972122192383, 0.29584479331970215, 1.9111272096633911] | Test Loss: [2.847066879272461, 0.3337365388870239, 2.5133302211761475]\n",
            "142: Train Loss: [1.9623677730560303, 0.2553335726261139, 1.7070342302322388] | Test Loss: [2.8545114994049072, 0.33081284165382385, 2.523698568344116]\n",
            "143: Train Loss: [2.3189215660095215, 0.27277323603630066, 2.0461483001708984] | Test Loss: [2.8716225624084473, 0.3315305709838867, 2.5400919914245605]\n",
            "144: Train Loss: [1.8706930875778198, 0.25057488679885864, 1.6201183795928955] | Test Loss: [2.884556770324707, 0.3364464044570923, 2.5481104850769043]\n",
            "145: Train Loss: [2.1767592430114746, 0.3073769509792328, 1.8693822622299194] | Test Loss: [2.8960771560668945, 0.3510783910751343, 2.54499888420105]\n",
            "146: Train Loss: [2.031581401824951, 0.2752434015274048, 1.756338119506836] | Test Loss: [2.908034563064575, 0.3711073100566864, 2.5369272232055664]\n",
            "147: Train Loss: [2.347956895828247, 0.2590225040912628, 2.0889344215393066] | Test Loss: [2.923152208328247, 0.39276573061943054, 2.530386447906494]\n",
            "148: Train Loss: [1.9990568161010742, 0.29803919792175293, 1.7010176181793213] | Test Loss: [2.946274757385254, 0.4205667972564697, 2.525707960128784]\n",
            "149: Train Loss: [1.9888534545898438, 0.24062508344650269, 1.7482283115386963] | Test Loss: [2.9624533653259277, 0.4370241165161133, 2.5254292488098145]\n",
            "150: Train Loss: [1.900913953781128, 0.2504793405532837, 1.6504346132278442] | Test Loss: [2.969048500061035, 0.4415089190006256, 2.5275394916534424]\n",
            "151: Train Loss: [1.8097151517868042, 0.2360071986913681, 1.573707938194275] | Test Loss: [2.966289758682251, 0.430372953414917, 2.535916805267334]\n",
            "152: Train Loss: [1.7797127962112427, 0.26236918568611145, 1.5173436403274536] | Test Loss: [2.956021308898926, 0.4137492775917053, 2.5422720909118652]\n",
            "153: Train Loss: [2.1683804988861084, 0.22874988615512848, 1.9396305084228516] | Test Loss: [2.9466400146484375, 0.393802285194397, 2.55283784866333]\n",
            "154: Train Loss: [1.728426218032837, 0.25436991453170776, 1.474056363105774] | Test Loss: [2.936917304992676, 0.37456345558166504, 2.5623538494110107]\n",
            "155: Train Loss: [2.1570565700531006, 0.29196897149086, 1.865087628364563] | Test Loss: [2.930795669555664, 0.36462900042533875, 2.566166639328003]\n",
            "156: Train Loss: [1.5668492317199707, 0.2841396629810333, 1.2827095985412598] | Test Loss: [2.9394028186798096, 0.36615443229675293, 2.5732483863830566]\n",
            "157: Train Loss: [1.8726977109909058, 0.2544647455215454, 1.6182329654693604] | Test Loss: [2.9539222717285156, 0.3733881711959839, 2.580533981323242]\n",
            "158: Train Loss: [1.7216037511825562, 0.27098405361175537, 1.4506196975708008] | Test Loss: [2.975322723388672, 0.38613831996917725, 2.589184284210205]\n",
            "159: Train Loss: [1.8440488576889038, 0.21818800270557404, 1.6258608102798462] | Test Loss: [2.9974989891052246, 0.39922210574150085, 2.5982768535614014]\n",
            "160: Train Loss: [1.7289040088653564, 0.24056249856948853, 1.4883415699005127] | Test Loss: [3.01640248298645, 0.4116203784942627, 2.6047821044921875]\n",
            "161: Train Loss: [1.864219069480896, 0.2545686662197113, 1.6096503734588623] | Test Loss: [3.0347814559936523, 0.42525532841682434, 2.6095261573791504]\n",
            "162: Train Loss: [2.049198865890503, 0.3122442364692688, 1.7369545698165894] | Test Loss: [3.058925151824951, 0.44572508335113525, 2.6132001876831055]\n",
            "163: Train Loss: [1.6635609865188599, 0.28365570306777954, 1.379905104637146] | Test Loss: [3.0824058055877686, 0.46344104409217834, 2.618964672088623]\n",
            "164: Train Loss: [1.8282536268234253, 0.25642791390419006, 1.5718257427215576] | Test Loss: [3.094785690307617, 0.4637758731842041, 2.631009817123413]\n",
            "165: Train Loss: [2.0631468296051025, 0.2995843291282654, 1.7635624408721924] | Test Loss: [3.1178743839263916, 0.4695582687854767, 2.6483161449432373]\n",
            "166: Train Loss: [1.8354843854904175, 0.24708180129528046, 1.5884026288986206] | Test Loss: [3.1187684535980225, 0.45057910680770874, 2.668189287185669]\n",
            "167: Train Loss: [1.9246480464935303, 0.28123739361763, 1.6434106826782227] | Test Loss: [3.1157031059265137, 0.425540566444397, 2.6901626586914062]\n",
            "168: Train Loss: [2.130642890930176, 0.23867952823638916, 1.891963243484497] | Test Loss: [3.1089115142822266, 0.4007163345813751, 2.708195209503174]\n",
            "169: Train Loss: [1.948148250579834, 0.300760954618454, 1.6473872661590576] | Test Loss: [3.100282907485962, 0.3762173056602478, 2.7240655422210693]\n",
            "170: Train Loss: [1.5080020427703857, 0.2716427445411682, 1.2363592386245728] | Test Loss: [3.0888357162475586, 0.3527434766292572, 2.7360923290252686]\n",
            "171: Train Loss: [2.1334333419799805, 0.25744542479515076, 1.8759878873825073] | Test Loss: [3.070096731185913, 0.32976993918418884, 2.7403268814086914]\n",
            "172: Train Loss: [1.7287781238555908, 0.29792025685310364, 1.4308578968048096] | Test Loss: [3.0579605102539062, 0.31319278478622437, 2.744767665863037]\n",
            "173: Train Loss: [2.010040760040283, 0.2922486960887909, 1.71779203414917] | Test Loss: [3.045962333679199, 0.30660808086395264, 2.739354372024536]\n",
            "174: Train Loss: [1.828537940979004, 0.2878049612045288, 1.540732979774475] | Test Loss: [3.0402262210845947, 0.3059728145599365, 2.734253406524658]\n",
            "175: Train Loss: [2.123046875, 0.28400474786758423, 1.839042067527771] | Test Loss: [3.0242385864257812, 0.30906176567077637, 2.715176820755005]\n",
            "176: Train Loss: [2.1067349910736084, 0.25689569115638733, 1.849839210510254] | Test Loss: [3.00553035736084, 0.3127611577510834, 2.6927692890167236]\n",
            "177: Train Loss: [2.190129518508911, 0.22370342910289764, 1.9664260149002075] | Test Loss: [2.9816102981567383, 0.3154779374599457, 2.6661324501037598]\n",
            "178: Train Loss: [1.7747230529785156, 0.24359136819839478, 1.5311317443847656] | Test Loss: [2.948824167251587, 0.31204429268836975, 2.63677978515625]\n",
            "179: Train Loss: [2.076786994934082, 0.2563464045524597, 1.8204405307769775] | Test Loss: [2.928872585296631, 0.3083820044994354, 2.620490550994873]\n",
            "180: Train Loss: [1.795301914215088, 0.2716605067253113, 1.5236414670944214] | Test Loss: [2.9177558422088623, 0.305438369512558, 2.6123175621032715]\n",
            "181: Train Loss: [1.9201098680496216, 0.2247629314661026, 1.6953469514846802] | Test Loss: [2.916311025619507, 0.3034062385559082, 2.6129047870635986]\n",
            "182: Train Loss: [1.7684428691864014, 0.3167871832847595, 1.451655626296997] | Test Loss: [2.912135362625122, 0.29804378747940063, 2.614091634750366]\n",
            "183: Train Loss: [1.620700716972351, 0.23008230328559875, 1.3906184434890747] | Test Loss: [2.9110867977142334, 0.2933451235294342, 2.617741584777832]\n",
            "184: Train Loss: [2.1764779090881348, 0.25093531608581543, 1.9255427122116089] | Test Loss: [2.911681652069092, 0.2893153131008148, 2.622366428375244]\n",
            "185: Train Loss: [1.9441766738891602, 0.290249764919281, 1.653926968574524] | Test Loss: [2.9243619441986084, 0.288472980260849, 2.6358890533447266]\n",
            "186: Train Loss: [1.5230165719985962, 0.2941878139972687, 1.22882878780365] | Test Loss: [2.948488712310791, 0.2930859923362732, 2.655402660369873]\n",
            "187: Train Loss: [2.371389865875244, 0.2751314342021942, 2.0962584018707275] | Test Loss: [2.976799488067627, 0.29848918318748474, 2.6783103942871094]\n",
            "188: Train Loss: [2.080390214920044, 0.24561843276023865, 1.8347718715667725] | Test Loss: [3.0032267570495605, 0.3076561689376831, 2.695570468902588]\n",
            "189: Train Loss: [1.9207255840301514, 0.30428311228752136, 1.6164424419403076] | Test Loss: [3.0308146476745605, 0.3245917856693268, 2.7062227725982666]\n",
            "190: Train Loss: [1.9576727151870728, 0.27731287479400635, 1.6803598403930664] | Test Loss: [3.059408664703369, 0.3452792763710022, 2.7141294479370117]\n",
            "191: Train Loss: [1.7374699115753174, 0.2949971556663513, 1.4424728155136108] | Test Loss: [3.0728237628936768, 0.3599188029766083, 2.712904930114746]\n",
            "192: Train Loss: [1.6324135065078735, 0.28500238060951233, 1.3474111557006836] | Test Loss: [3.0752782821655273, 0.3682785928249359, 2.7069997787475586]\n",
            "193: Train Loss: [1.665661334991455, 0.29280126094818115, 1.372860074043274] | Test Loss: [3.073683500289917, 0.372047483921051, 2.7016360759735107]\n",
            "194: Train Loss: [1.7353880405426025, 0.3089684844017029, 1.4264194965362549] | Test Loss: [3.0842597484588623, 0.3858201503753662, 2.698439598083496]\n",
            "195: Train Loss: [2.119680166244507, 0.25576475262641907, 1.8639153242111206] | Test Loss: [3.0809249877929688, 0.38585108518600464, 2.6950738430023193]\n",
            "196: Train Loss: [1.8448349237442017, 0.2590431272983551, 1.585791826248169] | Test Loss: [3.068000316619873, 0.38011443614959717, 2.6878857612609863]\n",
            "197: Train Loss: [0.885479211807251, 0.2366712987422943, 0.6488078832626343] | Test Loss: [3.0503106117248535, 0.36748480796813965, 2.682825803756714]\n",
            "198: Train Loss: [1.9214571714401245, 0.2148798704147339, 1.7065773010253906] | Test Loss: [3.0368666648864746, 0.35287752747535706, 2.6839890480041504]\n",
            "199: Train Loss: [1.799034595489502, 0.287830114364624, 1.511204481124878] | Test Loss: [3.0338234901428223, 0.3473500907421112, 2.6864733695983887]\n",
            "200: Train Loss: [2.040191411972046, 0.26542922854423523, 1.7747622728347778] | Test Loss: [3.040328025817871, 0.34591537714004517, 2.6944127082824707]\n",
            "201: Train Loss: [0.9211435914039612, 0.2811063528060913, 0.6400372385978699] | Test Loss: [3.0553619861602783, 0.35009893774986267, 2.705263137817383]\n",
            "202: Train Loss: [1.8716404438018799, 0.3172298073768616, 1.554410696029663] | Test Loss: [3.083458185195923, 0.3616725504398346, 2.721785545349121]\n",
            "203: Train Loss: [2.0172524452209473, 0.26351499557495117, 1.7537375688552856] | Test Loss: [3.1144514083862305, 0.3774978518486023, 2.7369534969329834]\n",
            "204: Train Loss: [1.8752646446228027, 0.2328556776046753, 1.6424089670181274] | Test Loss: [3.1492605209350586, 0.3944476544857025, 2.7548129558563232]\n",
            "205: Train Loss: [1.9782072305679321, 0.22066448628902435, 1.7575427293777466] | Test Loss: [3.174250364303589, 0.40484052896499634, 2.7694098949432373]\n",
            "206: Train Loss: [2.063828945159912, 0.30272188782691956, 1.761107087135315] | Test Loss: [3.2003273963928223, 0.4153386056423187, 2.7849888801574707]\n",
            "207: Train Loss: [1.6258251667022705, 0.26360973715782166, 1.3622153997421265] | Test Loss: [3.2189462184906006, 0.4218490719795227, 2.7970972061157227]\n",
            "208: Train Loss: [1.8240303993225098, 0.2887619435787201, 1.5352684259414673] | Test Loss: [3.2302730083465576, 0.4289882183074951, 2.8012847900390625]\n",
            "209: Train Loss: [1.797829270362854, 0.2507290840148926, 1.5471001863479614] | Test Loss: [3.2271859645843506, 0.42885223031044006, 2.7983336448669434]\n",
            "210: Train Loss: [1.8741313219070435, 0.2774362564086914, 1.596695065498352] | Test Loss: [3.2188994884490967, 0.4240000247955322, 2.7948994636535645]\n",
            "211: Train Loss: [1.8094978332519531, 0.2677346467971802, 1.541763186454773] | Test Loss: [3.2048659324645996, 0.41451677680015564, 2.790349245071411]\n",
            "212: Train Loss: [2.0153696537017822, 0.22422713041305542, 1.791142463684082] | Test Loss: [3.1883928775787354, 0.4027861952781677, 2.785606622695923]\n",
            "213: Train Loss: [1.5579962730407715, 0.25572314858436584, 1.302273154258728] | Test Loss: [3.1662001609802246, 0.3890407979488373, 2.7771594524383545]\n",
            "214: Train Loss: [2.235487461090088, 0.3093879818916321, 1.926099419593811] | Test Loss: [3.150423765182495, 0.382337749004364, 2.7680859565734863]\n",
            "215: Train Loss: [1.8531635999679565, 0.3048058748245239, 1.5483577251434326] | Test Loss: [3.1413657665252686, 0.3786138892173767, 2.762751817703247]\n",
            "216: Train Loss: [1.6967189311981201, 0.24444052577018738, 1.4522783756256104] | Test Loss: [3.1309685707092285, 0.37435632944107056, 2.7566123008728027]\n",
            "217: Train Loss: [1.5925036668777466, 0.2433343082666397, 1.349169373512268] | Test Loss: [3.114248037338257, 0.37091517448425293, 2.743332862854004]\n",
            "218: Train Loss: [1.913705587387085, 0.25588053464889526, 1.657824993133545] | Test Loss: [3.102745532989502, 0.3717498481273651, 2.7309956550598145]\n",
            "219: Train Loss: [2.4148190021514893, 0.2927524149417877, 2.1220664978027344] | Test Loss: [3.0980823040008545, 0.37263044714927673, 2.725451946258545]\n",
            "220: Train Loss: [1.7551665306091309, 0.2626243829727173, 1.4925421476364136] | Test Loss: [3.0926568508148193, 0.37613749504089355, 2.716519355773926]\n",
            "221: Train Loss: [2.2299726009368896, 0.3065679669380188, 1.9234046936035156] | Test Loss: [3.097672700881958, 0.3869410455226898, 2.7107317447662354]\n",
            "222: Train Loss: [2.000771999359131, 0.23938505351543427, 1.7613868713378906] | Test Loss: [3.0999040603637695, 0.3911460340023041, 2.7087581157684326]\n",
            "223: Train Loss: [2.0820531845092773, 0.3189636170864105, 1.763089656829834] | Test Loss: [3.1019959449768066, 0.39835435152053833, 2.703641653060913]\n",
            "224: Train Loss: [1.7986527681350708, 0.25802847743034363, 1.5406242609024048] | Test Loss: [3.101738452911377, 0.4021536111831665, 2.6995849609375]\n",
            "225: Train Loss: [2.207263231277466, 0.2631727159023285, 1.9440906047821045] | Test Loss: [3.0984253883361816, 0.39593130350112915, 2.7024941444396973]\n",
            "226: Train Loss: [1.4266661405563354, 0.2612021267414093, 1.1654640436172485] | Test Loss: [3.097125291824341, 0.39181163907051086, 2.7053136825561523]\n",
            "227: Train Loss: [1.9002721309661865, 0.28950196504592896, 1.6107702255249023] | Test Loss: [3.0859670639038086, 0.3837493658065796, 2.7022175788879395]\n",
            "228: Train Loss: [1.9332000017166138, 0.26374468207359314, 1.6694552898406982] | Test Loss: [3.0781986713409424, 0.379384309053421, 2.6988143920898438]\n",
            "229: Train Loss: [1.8538858890533447, 0.24888931214809418, 1.6049965620040894] | Test Loss: [3.0664663314819336, 0.3700573444366455, 2.696408987045288]\n",
            "230: Train Loss: [1.944119930267334, 0.271295964717865, 1.6728239059448242] | Test Loss: [3.0494232177734375, 0.3569867014884949, 2.692436456680298]\n",
            "231: Train Loss: [1.920715093612671, 0.26410800218582153, 1.6566071510314941] | Test Loss: [3.038209915161133, 0.3456743359565735, 2.692535638809204]\n",
            "232: Train Loss: [1.8288542032241821, 0.3147113025188446, 1.5141428709030151] | Test Loss: [3.036792039871216, 0.3453693687915802, 2.691422700881958]\n",
            "233: Train Loss: [1.695155143737793, 0.2434885948896408, 1.4516665935516357] | Test Loss: [3.039792776107788, 0.3464326560497284, 2.6933600902557373]\n",
            "234: Train Loss: [1.9410189390182495, 0.30094417929649353, 1.6400747299194336] | Test Loss: [3.0466599464416504, 0.34801194071769714, 2.698647975921631]\n",
            "235: Train Loss: [1.5486310720443726, 0.30338916182518005, 1.2452418804168701] | Test Loss: [3.061187982559204, 0.3579004406929016, 2.7032876014709473]\n",
            "236: Train Loss: [1.8840489387512207, 0.3319821059703827, 1.5520668029785156] | Test Loss: [3.0964388847351074, 0.3850933313369751, 2.7113454341888428]\n",
            "237: Train Loss: [1.893791913986206, 0.2734912037849426, 1.6203006505966187] | Test Loss: [3.130284309387207, 0.4111972451210022, 2.7190871238708496]\n",
            "238: Train Loss: [1.7606786489486694, 0.3062082827091217, 1.4544703960418701] | Test Loss: [3.1577742099761963, 0.4325619339942932, 2.725212335586548]\n",
            "239: Train Loss: [1.1147783994674683, 0.28763285279273987, 0.8271456360816956] | Test Loss: [3.1678595542907715, 0.43840718269348145, 2.72945237159729]\n",
            "240: Train Loss: [2.09000301361084, 0.26446831226348877, 1.8255345821380615] | Test Loss: [3.1636056900024414, 0.42860865592956543, 2.734997034072876]\n",
            "241: Train Loss: [1.7956135272979736, 0.2610119581222534, 1.5346015691757202] | Test Loss: [3.1519851684570312, 0.4115006625652313, 2.7404844760894775]\n",
            "242: Train Loss: [1.9542794227600098, 0.2709029018878937, 1.6833765506744385] | Test Loss: [3.1368751525878906, 0.39425045251846313, 2.7426247596740723]\n",
            "243: Train Loss: [1.8221765756607056, 0.24616144597530365, 1.5760151147842407] | Test Loss: [3.1090080738067627, 0.36661678552627563, 2.742391347885132]\n",
            "244: Train Loss: [1.2612924575805664, 0.2589518427848816, 1.00234055519104] | Test Loss: [3.0890238285064697, 0.345777690410614, 2.743246078491211]\n",
            "245: Train Loss: [1.5260488986968994, 0.3096731901168823, 1.216375708580017] | Test Loss: [3.0872788429260254, 0.3395509421825409, 2.747727870941162]\n",
            "246: Train Loss: [2.179124116897583, 0.2796494960784912, 1.8994746208190918] | Test Loss: [3.098433017730713, 0.34151491522789, 2.75691819190979]\n",
            "247: Train Loss: [1.929462194442749, 0.26919397711753845, 1.6602681875228882] | Test Loss: [3.113598346710205, 0.3479956090450287, 2.7656028270721436]\n",
            "248: Train Loss: [1.5336960554122925, 0.23601649701595306, 1.2976795434951782] | Test Loss: [3.120234251022339, 0.35260775685310364, 2.7676265239715576]\n",
            "249: Train Loss: [1.9704389572143555, 0.23012524843215942, 1.7403137683868408] | Test Loss: [3.122483015060425, 0.35407379269599915, 2.768409252166748]\n",
            "250: Train Loss: [1.7840790748596191, 0.2553907036781311, 1.5286884307861328] | Test Loss: [3.126685380935669, 0.3596195876598358, 2.7670657634735107]\n",
            "251: Train Loss: [1.7436778545379639, 0.31862667202949524, 1.425051212310791] | Test Loss: [3.1409616470336914, 0.37096723914146423, 2.7699944972991943]\n",
            "252: Train Loss: [1.4935722351074219, 0.2994888424873352, 1.194083333015442] | Test Loss: [3.159308433532715, 0.3848165273666382, 2.774492025375366]\n",
            "253: Train Loss: [1.6800005435943604, 0.2546832263469696, 1.4253172874450684] | Test Loss: [3.1721644401550293, 0.39819732308387756, 2.7739670276641846]\n",
            "254: Train Loss: [1.9661917686462402, 0.2883737087249756, 1.6778180599212646] | Test Loss: [3.1827852725982666, 0.4081878662109375, 2.774597406387329]\n",
            "255: Train Loss: [1.9826035499572754, 0.26137301325798035, 1.7212305068969727] | Test Loss: [3.1840500831604004, 0.4052204489707947, 2.778829574584961]\n",
            "256: Train Loss: [1.5382564067840576, 0.2899564504623413, 1.2482999563217163] | Test Loss: [3.1825616359710693, 0.3984747529029846, 2.7840869426727295]\n",
            "257: Train Loss: [2.011579751968384, 0.26249971985816956, 1.7490800619125366] | Test Loss: [3.178239107131958, 0.3843277394771576, 2.7939114570617676]\n",
            "258: Train Loss: [1.9407285451889038, 0.2762136161327362, 1.6645148992538452] | Test Loss: [3.1623663902282715, 0.3640773296356201, 2.7982890605926514]\n",
            "259: Train Loss: [2.0729165077209473, 0.22431018948554993, 1.8486063480377197] | Test Loss: [3.1382100582122803, 0.34021663665771484, 2.7979934215545654]\n",
            "260: Train Loss: [1.9617259502410889, 0.2760078012943268, 1.6857181787490845] | Test Loss: [3.11824893951416, 0.3228071928024292, 2.7954418659210205]\n",
            "261: Train Loss: [1.7572343349456787, 0.2829579710960388, 1.4742763042449951] | Test Loss: [3.095088481903076, 0.30848780274391174, 2.7866005897521973]\n",
            "262: Train Loss: [2.094905376434326, 0.24790389835834503, 1.847001552581787] | Test Loss: [3.0799331665039062, 0.29636549949645996, 2.7835676670074463]\n",
            "263: Train Loss: [1.7131468057632446, 0.2777968645095825, 1.435349941253662] | Test Loss: [3.0678586959838867, 0.28805601596832275, 2.7798027992248535]\n",
            "264: Train Loss: [1.819559931755066, 0.26401928067207336, 1.555540680885315] | Test Loss: [3.061117172241211, 0.2830662131309509, 2.7780508995056152]\n",
            "265: Train Loss: [2.1778087615966797, 0.22119516134262085, 1.956613540649414] | Test Loss: [3.064423084259033, 0.27973276376724243, 2.7846903800964355]\n",
            "266: Train Loss: [1.9383418560028076, 0.24576959013938904, 1.6925722360610962] | Test Loss: [3.0729942321777344, 0.28158482909202576, 2.791409492492676]\n",
            "267: Train Loss: [2.045644760131836, 0.26582348346710205, 1.7798212766647339] | Test Loss: [3.0901989936828613, 0.2910694181919098, 2.7991294860839844]\n",
            "268: Train Loss: [2.0687875747680664, 0.2816075086593628, 1.7871800661087036] | Test Loss: [3.1063411235809326, 0.3010156750679016, 2.805325508117676]\n",
            "269: Train Loss: [1.8496581315994263, 0.24643243849277496, 1.6032257080078125] | Test Loss: [3.114555835723877, 0.3100029528141022, 2.8045527935028076]\n",
            "270: Train Loss: [1.3736456632614136, 0.2906521260738373, 1.082993507385254] | Test Loss: [3.123084783554077, 0.3218897879123688, 2.801194906234741]\n",
            "271: Train Loss: [1.985245943069458, 0.265897274017334, 1.719348669052124] | Test Loss: [3.1166698932647705, 0.3313983380794525, 2.785271644592285]\n",
            "272: Train Loss: [2.1113359928131104, 0.2541975677013397, 1.8571385145187378] | Test Loss: [3.0970537662506104, 0.33759769797325134, 2.759456157684326]\n",
            "273: Train Loss: [2.0458288192749023, 0.27586299180984497, 1.7699657678604126] | Test Loss: [3.079576015472412, 0.3435908555984497, 2.735985040664673]\n",
            "274: Train Loss: [1.0407617092132568, 0.260933518409729, 0.7798282504081726] | Test Loss: [3.059438705444336, 0.3445495665073395, 2.7148890495300293]\n",
            "275: Train Loss: [1.8668761253356934, 0.31494641304016113, 1.5519297122955322] | Test Loss: [3.0414867401123047, 0.3387587368488312, 2.702728033065796]\n",
            "276: Train Loss: [1.9853086471557617, 0.30432945489883423, 1.6809791326522827] | Test Loss: [3.0293631553649902, 0.3369036018848419, 2.6924595832824707]\n",
            "277: Train Loss: [1.7780228853225708, 0.25725311040878296, 1.5207695960998535] | Test Loss: [3.012958526611328, 0.32619214057922363, 2.6867663860321045]\n",
            "278: Train Loss: [1.890353798866272, 0.27028146386146545, 1.620072364807129] | Test Loss: [3.0031399726867676, 0.31448933482170105, 2.688650608062744]\n",
            "279: Train Loss: [1.7455962896347046, 0.20594000816345215, 1.5396562814712524] | Test Loss: [2.9873785972595215, 0.30105674266815186, 2.68632173538208]\n",
            "280: Train Loss: [1.8359332084655762, 0.2843678593635559, 1.5515652894973755] | Test Loss: [2.9796595573425293, 0.29602715373039246, 2.6836323738098145]\n",
            "281: Train Loss: [0.7594311237335205, 0.2592434585094452, 0.5001876950263977] | Test Loss: [2.9754719734191895, 0.29301682114601135, 2.682455062866211]\n",
            "282: Train Loss: [1.894309163093567, 0.3014918863773346, 1.5928173065185547] | Test Loss: [2.9858500957489014, 0.2969319820404053, 2.688918113708496]\n",
            "283: Train Loss: [1.9493316411972046, 0.24196049571037292, 1.7073711156845093] | Test Loss: [2.9999611377716064, 0.3008478581905365, 2.699113368988037]\n",
            "284: Train Loss: [2.0359911918640137, 0.22100326418876648, 1.8149878978729248] | Test Loss: [3.0184051990509033, 0.308434396982193, 2.709970712661743]\n",
            "285: Train Loss: [1.7513203620910645, 0.3847379684448242, 1.3665823936462402] | Test Loss: [3.0554654598236084, 0.33496952056884766, 2.7204959392547607]\n",
            "286: Train Loss: [1.9822396039962769, 0.23161539435386658, 1.750624179840088] | Test Loss: [3.090864658355713, 0.358394593000412, 2.7324700355529785]\n",
            "287: Train Loss: [2.137388229370117, 0.25917738676071167, 1.8782107830047607] | Test Loss: [3.120866537094116, 0.3780820369720459, 2.7427845001220703]\n",
            "288: Train Loss: [2.011162281036377, 0.2807598412036896, 1.7304024696350098] | Test Loss: [3.1508688926696777, 0.40287527441978455, 2.7479937076568604]\n",
            "289: Train Loss: [1.8741576671600342, 0.2717532515525818, 1.6024043560028076] | Test Loss: [3.172546625137329, 0.42050623893737793, 2.752040386199951]\n",
            "290: Train Loss: [1.5645111799240112, 0.25530433654785156, 1.3092068433761597] | Test Loss: [3.184705972671509, 0.4316486716270447, 2.7530572414398193]\n",
            "291: Train Loss: [1.5907385349273682, 0.25359588861465454, 1.3371427059173584] | Test Loss: [3.1857428550720215, 0.43500301241874695, 2.750739812850952]\n",
            "292: Train Loss: [1.781142234802246, 0.26906439661979675, 1.512077808380127] | Test Loss: [3.1738269329071045, 0.43116670846939087, 2.7426602840423584]\n",
            "293: Train Loss: [2.207320213317871, 0.29514965415000916, 1.912170648574829] | Test Loss: [3.146855354309082, 0.4200856387615204, 2.7267696857452393]\n",
            "294: Train Loss: [1.7885905504226685, 0.26860901713371277, 1.5199815034866333] | Test Loss: [3.113348960876465, 0.4003368020057678, 2.713012218475342]\n",
            "295: Train Loss: [1.6951053142547607, 0.24783426523208618, 1.4472709894180298] | Test Loss: [3.081937789916992, 0.3835466802120209, 2.6983911991119385]\n",
            "296: Train Loss: [1.6927518844604492, 0.2344771921634674, 1.4582747220993042] | Test Loss: [3.052921772003174, 0.3710368871688843, 2.681884765625]\n",
            "297: Train Loss: [2.0296666622161865, 0.34690409898757935, 1.682762622833252] | Test Loss: [3.0426926612854004, 0.37159669399261475, 2.671095848083496]\n",
            "298: Train Loss: [1.5673692226409912, 0.2806171774864197, 1.2867519855499268] | Test Loss: [3.043165683746338, 0.37722083926200867, 2.665944814682007]\n",
            "299: Train Loss: [2.1043426990509033, 0.3094485104084015, 1.7948940992355347] | Test Loss: [3.049342393875122, 0.38544735312461853, 2.6638951301574707]\n",
            "300: Train Loss: [2.1889822483062744, 0.2979945242404938, 1.8909876346588135] | Test Loss: [3.0510222911834717, 0.3929579257965088, 2.658064365386963]\n",
            "301: Train Loss: [1.8286265134811401, 0.30347228050231934, 1.5251542329788208] | Test Loss: [3.050534248352051, 0.39812442660331726, 2.652409791946411]\n",
            "302: Train Loss: [1.8165106773376465, 0.28919318318367004, 1.5273175239562988] | Test Loss: [3.0515284538269043, 0.40362411737442017, 2.647904396057129]\n",
            "303: Train Loss: [1.5340641736984253, 0.2818395793437958, 1.2522245645523071] | Test Loss: [3.0514626502990723, 0.40307697653770447, 2.648385763168335]\n",
            "304: Train Loss: [1.7353752851486206, 0.28268226981163025, 1.452692985534668] | Test Loss: [3.042128086090088, 0.3960541784763336, 2.646073818206787]\n",
            "305: Train Loss: [1.7217326164245605, 0.25392359495162964, 1.4678089618682861] | Test Loss: [3.026093006134033, 0.3774065375328064, 2.648686408996582]\n",
            "306: Train Loss: [2.206038475036621, 0.2723964750766754, 1.9336419105529785] | Test Loss: [3.0053093433380127, 0.3533661961555481, 2.6519432067871094]\n",
            "307: Train Loss: [1.8519864082336426, 0.2764858603477478, 1.57550048828125] | Test Loss: [2.985856294631958, 0.3285580277442932, 2.6572983264923096]\n",
            "308: Train Loss: [2.0249743461608887, 0.2685621380805969, 1.756412148475647] | Test Loss: [2.971216917037964, 0.3107540011405945, 2.6604628562927246]\n",
            "309: Train Loss: [2.046231746673584, 0.2778552770614624, 1.7683764696121216] | Test Loss: [2.9593842029571533, 0.29836538434028625, 2.6610188484191895]\n",
            "310: Train Loss: [1.746712565422058, 0.28041109442710876, 1.466301441192627] | Test Loss: [2.9509668350219727, 0.2926211953163147, 2.6583456993103027]\n",
            "311: Train Loss: [1.5803017616271973, 0.3171617090702057, 1.2631402015686035] | Test Loss: [2.9448904991149902, 0.29393887519836426, 2.650951623916626]\n",
            "312: Train Loss: [1.7366666793823242, 0.28895220160484314, 1.4477144479751587] | Test Loss: [2.9447200298309326, 0.30180200934410095, 2.642918109893799]\n",
            "313: Train Loss: [1.002968430519104, 0.27519431710243225, 0.7277740836143494] | Test Loss: [2.947235345840454, 0.31165817379951477, 2.6355772018432617]\n",
            "314: Train Loss: [1.8069497346878052, 0.2264309972524643, 1.5805187225341797] | Test Loss: [2.9466116428375244, 0.3180913031101227, 2.6285202503204346]\n",
            "315: Train Loss: [2.1277763843536377, 0.21939365565776825, 1.908382773399353] | Test Loss: [2.935945987701416, 0.3186471164226532, 2.6172988414764404]\n",
            "316: Train Loss: [1.7447950839996338, 0.2717733383178711, 1.4730217456817627] | Test Loss: [2.9240097999572754, 0.3198774755001068, 2.6041324138641357]\n",
            "317: Train Loss: [1.7147200107574463, 0.2847614884376526, 1.429958462715149] | Test Loss: [2.9130523204803467, 0.32083144783973694, 2.5922207832336426]\n",
            "318: Train Loss: [1.5090968608856201, 0.2677711546421051, 1.2413257360458374] | Test Loss: [2.9077706336975098, 0.3257652521133423, 2.582005500793457]\n",
            "319: Train Loss: [1.9890375137329102, 0.28215354681015015, 1.7068839073181152] | Test Loss: [2.9088988304138184, 0.33768734335899353, 2.571211576461792]\n",
            "320: Train Loss: [1.9255398511886597, 0.29566606879234314, 1.6298737525939941] | Test Loss: [2.9119656085968018, 0.3487827479839325, 2.563182830810547]\n",
            "321: Train Loss: [2.06923246383667, 0.2714802920818329, 1.7977521419525146] | Test Loss: [2.9079809188842773, 0.3564373850822449, 2.5515434741973877]\n",
            "322: Train Loss: [1.8701186180114746, 0.31281837821006775, 1.5573002099990845] | Test Loss: [2.909221649169922, 0.3655795454978943, 2.543642044067383]\n",
            "323: Train Loss: [2.02791690826416, 0.2635161578655243, 1.7644007205963135] | Test Loss: [2.9087023735046387, 0.36741015315055847, 2.541292190551758]\n",
            "324: Train Loss: [1.687860369682312, 0.28917157649993896, 1.398688793182373] | Test Loss: [2.9094793796539307, 0.3640994429588318, 2.545379877090454]\n",
            "325: Train Loss: [2.3627257347106934, 0.3100566267967224, 2.052669048309326] | Test Loss: [2.9201622009277344, 0.36946073174476624, 2.550701379776001]\n",
            "326: Train Loss: [2.098254442214966, 0.2706669867038727, 1.8275874853134155] | Test Loss: [2.924114227294922, 0.3641212582588196, 2.559993028640747]\n",
            "327: Train Loss: [1.6673178672790527, 0.26781946420669556, 1.399498462677002] | Test Loss: [2.9193410873413086, 0.3552752733230591, 2.56406569480896]\n",
            "328: Train Loss: [1.5707347393035889, 0.2909366190433502, 1.279798150062561] | Test Loss: [2.9074411392211914, 0.34742027521133423, 2.560020923614502]\n",
            "329: Train Loss: [1.7973554134368896, 0.34027034044265747, 1.4570850133895874] | Test Loss: [2.897165060043335, 0.34168678522109985, 2.55547833442688]\n",
            "330: Train Loss: [2.1213366985321045, 0.31605225801467896, 1.8052843809127808] | Test Loss: [2.890876293182373, 0.33839571475982666, 2.552480697631836]\n",
            "331: Train Loss: [2.0342211723327637, 0.2752680480480194, 1.7589530944824219] | Test Loss: [2.879175901412964, 0.33291101455688477, 2.546264886856079]\n",
            "332: Train Loss: [1.6256937980651855, 0.29138535261154175, 1.334308385848999] | Test Loss: [2.8700265884399414, 0.3308337926864624, 2.5391929149627686]\n",
            "333: Train Loss: [1.8995177745819092, 0.23940075933933258, 1.6601170301437378] | Test Loss: [2.8685214519500732, 0.3288903832435608, 2.5396311283111572]\n",
            "334: Train Loss: [1.9778162240982056, 0.28151872754096985, 1.696297526359558] | Test Loss: [2.868438243865967, 0.32776984572410583, 2.540668487548828]\n",
            "335: Train Loss: [1.9553519487380981, 0.2762397229671478, 1.6791123151779175] | Test Loss: [2.87813663482666, 0.33157193660736084, 2.546564817428589]\n",
            "336: Train Loss: [2.0868070125579834, 0.3103305697441101, 1.776476502418518] | Test Loss: [2.9030416011810303, 0.3422291874885559, 2.560812473297119]\n",
            "337: Train Loss: [1.679244041442871, 0.2863331437110901, 1.3929108381271362] | Test Loss: [2.938621997833252, 0.3590169847011566, 2.5796051025390625]\n",
            "338: Train Loss: [1.8333138227462769, 0.2982184886932373, 1.5350953340530396] | Test Loss: [2.977006673812866, 0.3786965012550354, 2.5983102321624756]\n",
            "339: Train Loss: [1.9490206241607666, 0.2694060206413269, 1.679614543914795] | Test Loss: [3.0215818881988525, 0.4015290439128876, 2.6200528144836426]\n",
            "340: Train Loss: [1.8785855770111084, 0.23070961236953735, 1.6478760242462158] | Test Loss: [3.0560808181762695, 0.41566503047943115, 2.640415668487549]\n",
            "341: Train Loss: [1.6191728115081787, 0.22954857349395752, 1.3896242380142212] | Test Loss: [3.075704574584961, 0.4159407913684845, 2.659763813018799]\n",
            "342: Train Loss: [1.9926313161849976, 0.323340505361557, 1.6692907810211182] | Test Loss: [3.0933918952941895, 0.41511544585227966, 2.678276538848877]\n",
            "343: Train Loss: [2.1192479133605957, 0.2722848057746887, 1.8469630479812622] | Test Loss: [3.1007237434387207, 0.4096675217151642, 2.691056251525879]\n",
            "344: Train Loss: [2.004723072052002, 0.2557382583618164, 1.748984694480896] | Test Loss: [3.103466510772705, 0.40178245306015015, 2.70168399810791]\n",
            "345: Train Loss: [1.7555780410766602, 0.31190305948257446, 1.443674921989441] | Test Loss: [3.0948538780212402, 0.39396756887435913, 2.7008862495422363]\n",
            "346: Train Loss: [2.3378732204437256, 0.261168897151947, 2.076704263687134] | Test Loss: [3.081679344177246, 0.3848347067832947, 2.6968445777893066]\n",
            "347: Train Loss: [1.6438928842544556, 0.2659190893173218, 1.3779737949371338] | Test Loss: [3.054452657699585, 0.3728964328765869, 2.681556224822998]\n",
            "348: Train Loss: [2.0148215293884277, 0.3097962439060211, 1.705025315284729] | Test Loss: [3.0295965671539307, 0.3652420938014984, 2.6643545627593994]\n",
            "349: Train Loss: [1.6940152645111084, 0.2848874628543854, 1.4091278314590454] | Test Loss: [3.010730266571045, 0.3621748685836792, 2.648555278778076]\n",
            "350: Train Loss: [2.258760452270508, 0.22799257934093475, 2.0307679176330566] | Test Loss: [2.997305154800415, 0.3600918650627136, 2.6372132301330566]\n",
            "351: Train Loss: [1.6600795984268188, 0.2338382452726364, 1.426241397857666] | Test Loss: [2.98779034614563, 0.3578070104122162, 2.629983425140381]\n",
            "352: Train Loss: [1.803429126739502, 0.34200936555862427, 1.4614198207855225] | Test Loss: [2.989009380340576, 0.3604939579963684, 2.6285154819488525]\n",
            "353: Train Loss: [2.00533390045166, 0.2628650665283203, 1.7424688339233398] | Test Loss: [2.9814231395721436, 0.35887646675109863, 2.622546672821045]\n",
            "354: Train Loss: [1.7966361045837402, 0.2648208737373352, 1.5318151712417603] | Test Loss: [2.9734408855438232, 0.34855371713638306, 2.624887228012085]\n",
            "355: Train Loss: [2.0505902767181396, 0.2700215280056, 1.7805687189102173] | Test Loss: [2.970130443572998, 0.33556845784187317, 2.6345620155334473]\n",
            "356: Train Loss: [1.749723196029663, 0.2793055474758148, 1.4704176187515259] | Test Loss: [2.9663169384002686, 0.3222297728061676, 2.644087076187134]\n",
            "357: Train Loss: [2.0877485275268555, 0.3035712242126465, 1.784177303314209] | Test Loss: [2.961606740951538, 0.3105930984020233, 2.6510136127471924]\n",
            "358: Train Loss: [2.249298095703125, 0.28847774863243103, 1.9608203172683716] | Test Loss: [2.9636385440826416, 0.3070322573184967, 2.6566061973571777]\n",
            "359: Train Loss: [1.8229880332946777, 0.2593740224838257, 1.563614010810852] | Test Loss: [2.9676430225372314, 0.302745521068573, 2.6648974418640137]\n",
            "360: Train Loss: [1.5708434581756592, 0.3365482687950134, 1.234295129776001] | Test Loss: [2.975177764892578, 0.3045227527618408, 2.6706550121307373]\n",
            "361: Train Loss: [1.496796727180481, 0.22919927537441254, 1.2675974369049072] | Test Loss: [2.979656219482422, 0.30771324038505554, 2.671942949295044]\n",
            "362: Train Loss: [2.048793315887451, 0.25558924674987793, 1.7932041883468628] | Test Loss: [2.9812865257263184, 0.309994637966156, 2.6712918281555176]\n",
            "363: Train Loss: [1.8324459791183472, 0.2831818759441376, 1.5492640733718872] | Test Loss: [2.982409715652466, 0.3111569285392761, 2.671252727508545]\n",
            "364: Train Loss: [2.0222792625427246, 0.23561330139636993, 1.7866660356521606] | Test Loss: [2.974727153778076, 0.308601438999176, 2.666125774383545]\n",
            "365: Train Loss: [1.9990051984786987, 0.21150091290473938, 1.7875043153762817] | Test Loss: [2.9592463970184326, 0.302841454744339, 2.656404972076416]\n",
            "366: Train Loss: [1.8918298482894897, 0.22287830710411072, 1.6689515113830566] | Test Loss: [2.9556994438171387, 0.2984655201435089, 2.657233953475952]\n",
            "367: Train Loss: [1.9208437204360962, 0.2770439386367798, 1.6437997817993164] | Test Loss: [2.9625778198242188, 0.30228275060653687, 2.660295009613037]\n",
            "368: Train Loss: [1.9948283433914185, 0.27576375007629395, 1.7190645933151245] | Test Loss: [2.971087694168091, 0.3088098466396332, 2.662277936935425]\n",
            "369: Train Loss: [2.0449399948120117, 0.3048349618911743, 1.7401049137115479] | Test Loss: [2.978076696395874, 0.3172815442085266, 2.660795211791992]\n",
            "370: Train Loss: [1.9979181289672852, 0.25616759061813354, 1.7417505979537964] | Test Loss: [2.98522686958313, 0.32564330101013184, 2.659583568572998]\n",
            "371: Train Loss: [2.2261695861816406, 0.3113863468170166, 1.9147831201553345] | Test Loss: [2.981393814086914, 0.3338903784751892, 2.64750337600708]\n",
            "372: Train Loss: [1.792222023010254, 0.2769702970981598, 1.5152517557144165] | Test Loss: [2.98258113861084, 0.34397178888320923, 2.6386094093322754]\n",
            "373: Train Loss: [1.4737224578857422, 0.26461029052734375, 1.2091121673583984] | Test Loss: [2.9821529388427734, 0.3483734130859375, 2.633779525756836]\n",
            "374: Train Loss: [2.0035109519958496, 0.2822026014328003, 1.7213082313537598] | Test Loss: [2.9729838371276855, 0.3480645418167114, 2.6249194145202637]\n",
            "375: Train Loss: [1.8131446838378906, 0.2888280749320984, 1.524316668510437] | Test Loss: [2.958834648132324, 0.3410928249359131, 2.617741823196411]\n",
            "376: Train Loss: [2.276684522628784, 0.2616373896598816, 2.015047073364258] | Test Loss: [2.9417757987976074, 0.3312721848487854, 2.610503673553467]\n",
            "377: Train Loss: [1.6547598838806152, 0.26221904158592224, 1.3925408124923706] | Test Loss: [2.931701183319092, 0.32315793633461, 2.6085431575775146]\n",
            "378: Train Loss: [2.1302437782287598, 0.27604591846466064, 1.8541978597640991] | Test Loss: [2.9160358905792236, 0.31366991996765137, 2.6023659706115723]\n",
            "379: Train Loss: [2.127682685852051, 0.274791419506073, 1.8528913259506226] | Test Loss: [2.904461145401001, 0.30898067355155945, 2.595480442047119]\n",
            "380: Train Loss: [1.7899366617202759, 0.3174699544906616, 1.4724667072296143] | Test Loss: [2.8907387256622314, 0.3057286739349365, 2.585010051727295]\n",
            "381: Train Loss: [1.5787054300308228, 0.26988765597343445, 1.308817744255066] | Test Loss: [2.885627269744873, 0.30562490224838257, 2.5800023078918457]\n",
            "382: Train Loss: [1.9680495262145996, 0.2712671756744385, 1.6967823505401611] | Test Loss: [2.8912816047668457, 0.31217217445373535, 2.5791094303131104]\n",
            "383: Train Loss: [1.637300968170166, 0.3396473824977875, 1.2976535558700562] | Test Loss: [2.91003155708313, 0.3291900157928467, 2.580841541290283]\n",
            "384: Train Loss: [1.4336919784545898, 0.3141876459121704, 1.1195042133331299] | Test Loss: [2.932220220565796, 0.35016465187072754, 2.5820555686950684]\n",
            "385: Train Loss: [1.9527781009674072, 0.2594608962535858, 1.693317174911499] | Test Loss: [2.954472303390503, 0.3670169711112976, 2.5874552726745605]\n",
            "386: Train Loss: [1.9125347137451172, 0.30253854393959045, 1.6099961996078491] | Test Loss: [2.982837677001953, 0.3848832845687866, 2.597954273223877]\n",
            "387: Train Loss: [1.9109348058700562, 0.325641006231308, 1.5852937698364258] | Test Loss: [3.003910541534424, 0.39852625131607056, 2.605384349822998]\n",
            "388: Train Loss: [2.061042308807373, 0.2884816527366638, 1.772560715675354] | Test Loss: [3.0206003189086914, 0.40333521366119385, 2.617264986038208]\n",
            "389: Train Loss: [1.9636939764022827, 0.24980047345161438, 1.7138935327529907] | Test Loss: [3.032623052597046, 0.3954462707042694, 2.637176752090454]\n",
            "390: Train Loss: [2.252117156982422, 0.30947932600975037, 1.9426378011703491] | Test Loss: [3.0333194732666016, 0.38001179695129395, 2.6533076763153076]\n",
            "391: Train Loss: [1.9011350870132446, 0.24495412409305573, 1.65618097782135] | Test Loss: [3.02528977394104, 0.3616660237312317, 2.663623809814453]\n",
            "392: Train Loss: [1.0885875225067139, 0.24297671020030975, 0.8456107974052429] | Test Loss: [3.012589693069458, 0.3432953655719757, 2.6692943572998047]\n",
            "393: Train Loss: [1.8245317935943604, 0.24986059963703156, 1.5746711492538452] | Test Loss: [2.9843437671661377, 0.3287007808685303, 2.6556429862976074]\n",
            "394: Train Loss: [1.9875105619430542, 0.24733437597751617, 1.7401762008666992] | Test Loss: [2.967451333999634, 0.3196125626564026, 2.647838830947876]\n",
            "395: Train Loss: [1.9653615951538086, 0.23562926054000854, 1.7297322750091553] | Test Loss: [2.9467668533325195, 0.3099052608013153, 2.636861562728882]\n",
            "396: Train Loss: [1.9855339527130127, 0.28635287284851074, 1.699181079864502] | Test Loss: [2.941819667816162, 0.3070046305656433, 2.634814977645874]\n",
            "397: Train Loss: [1.7376692295074463, 0.2818472385406494, 1.4558219909667969] | Test Loss: [2.943725824356079, 0.30814051628112793, 2.635585308074951]\n",
            "398: Train Loss: [2.198413133621216, 0.35846906900405884, 1.8399440050125122] | Test Loss: [2.9590156078338623, 0.31867554783821106, 2.6403400897979736]\n",
            "399: Train Loss: [1.8404933214187622, 0.23040226101875305, 1.6100910902023315] | Test Loss: [2.9712321758270264, 0.3263213336467743, 2.6449108123779297]\n",
            "400: Train Loss: [1.0872256755828857, 0.34058862924575806, 0.7466370463371277] | Test Loss: [2.9851603507995605, 0.33539485931396484, 2.6497654914855957]\n",
            "401: Train Loss: [1.5543032884597778, 0.29618000984191895, 1.2581232786178589] | Test Loss: [2.989330768585205, 0.3400460183620453, 2.649284839630127]\n",
            "402: Train Loss: [1.9855750799179077, 0.27623042464256287, 1.7093446254730225] | Test Loss: [2.986034393310547, 0.3385186493396759, 2.6475157737731934]\n",
            "403: Train Loss: [1.639572024345398, 0.2599719762802124, 1.3796000480651855] | Test Loss: [2.9830994606018066, 0.33818238973617554, 2.6449170112609863]\n",
            "404: Train Loss: [2.1781294345855713, 0.3163948357105255, 1.8617345094680786] | Test Loss: [2.9782869815826416, 0.3388129770755768, 2.6394739151000977]\n",
            "405: Train Loss: [1.9593000411987305, 0.23960739374160767, 1.719692587852478] | Test Loss: [2.959961414337158, 0.331414133310318, 2.628547191619873]\n",
            "406: Train Loss: [1.7850518226623535, 0.29775139689445496, 1.4873003959655762] | Test Loss: [2.945866823196411, 0.3240882456302643, 2.6217784881591797]\n",
            "407: Train Loss: [1.9210271835327148, 0.26367074251174927, 1.6573563814163208] | Test Loss: [2.926630973815918, 0.31502577662467957, 2.611605167388916]\n",
            "408: Train Loss: [2.2133185863494873, 0.30843386054039, 1.9048848152160645] | Test Loss: [2.913961172103882, 0.31154075264930725, 2.6024203300476074]\n",
            "409: Train Loss: [2.27542781829834, 0.286018431186676, 1.989409327507019] | Test Loss: [2.902742624282837, 0.307335764169693, 2.5954067707061768]\n",
            "410: Train Loss: [1.7284307479858398, 0.27601462602615356, 1.452416181564331] | Test Loss: [2.8844332695007324, 0.3024795353412628, 2.581953763961792]\n",
            "411: Train Loss: [1.9052382707595825, 0.32444870471954346, 1.5807896852493286] | Test Loss: [2.870112657546997, 0.3006264269351959, 2.569486141204834]\n",
            "412: Train Loss: [1.3921115398406982, 0.28590601682662964, 1.1062055826187134] | Test Loss: [2.8577122688293457, 0.298211932182312, 2.559500217437744]\n",
            "413: Train Loss: [1.695150375366211, 0.2628629803657532, 1.4322874546051025] | Test Loss: [2.851010322570801, 0.29701387882232666, 2.5539965629577637]\n",
            "414: Train Loss: [2.151963949203491, 0.3161065876483917, 1.8358572721481323] | Test Loss: [2.845282793045044, 0.2927023470401764, 2.5525803565979004]\n",
            "415: Train Loss: [1.641211748123169, 0.3061826825141907, 1.335029125213623] | Test Loss: [2.847959280014038, 0.2936410903930664, 2.5543181896209717]\n",
            "416: Train Loss: [2.064549446105957, 0.31224924325942993, 1.7523001432418823] | Test Loss: [2.850473165512085, 0.2971239387989044, 2.553349256515503]\n",
            "417: Train Loss: [1.6260193586349487, 0.29479631781578064, 1.3312230110168457] | Test Loss: [2.857822895050049, 0.3023993670940399, 2.5554234981536865]\n",
            "418: Train Loss: [1.895633578300476, 0.2982354164123535, 1.597398281097412] | Test Loss: [2.862699508666992, 0.3021453619003296, 2.560554265975952]\n",
            "419: Train Loss: [1.705327033996582, 0.2686498165130615, 1.4366772174835205] | Test Loss: [2.8667564392089844, 0.29937395453453064, 2.567382574081421]\n",
            "420: Train Loss: [2.007002592086792, 0.28287774324417114, 1.724124789237976] | Test Loss: [2.867741107940674, 0.29132944345474243, 2.576411724090576]\n",
            "421: Train Loss: [2.181058168411255, 0.3173086643218994, 1.8637495040893555] | Test Loss: [2.878551721572876, 0.2905356287956238, 2.5880160331726074]\n",
            "422: Train Loss: [1.8323155641555786, 0.24179169535636902, 1.5905238389968872] | Test Loss: [2.882789134979248, 0.2858385443687439, 2.5969505310058594]\n",
            "423: Train Loss: [2.226670980453491, 0.31591668725013733, 1.9107543230056763] | Test Loss: [2.8848581314086914, 0.2822984755039215, 2.6025595664978027]\n",
            "424: Train Loss: [2.2544679641723633, 0.2922627925872803, 1.9622050523757935] | Test Loss: [2.890214204788208, 0.2833584249019623, 2.606855869293213]\n",
            "425: Train Loss: [1.8943126201629639, 0.24056346714496613, 1.6537491083145142] | Test Loss: [2.8885438442230225, 0.28061795234680176, 2.6079258918762207]\n",
            "426: Train Loss: [1.5345295667648315, 0.2537018358707428, 1.2808277606964111] | Test Loss: [2.8748018741607666, 0.2741753160953522, 2.6006264686584473]\n",
            "427: Train Loss: [1.6697431802749634, 0.25017526745796204, 1.4195679426193237] | Test Loss: [2.8587021827697754, 0.2683414816856384, 2.590360641479492]\n",
            "428: Train Loss: [1.911555528640747, 0.2849031388759613, 1.626652479171753] | Test Loss: [2.8465423583984375, 0.2687811851501465, 2.577761173248291]\n",
            "429: Train Loss: [1.4258999824523926, 0.3544726073741913, 1.071427345275879] | Test Loss: [2.845543384552002, 0.27906301617622375, 2.5664803981781006]\n",
            "430: Train Loss: [1.950096607208252, 0.26649418473243713, 1.6836024522781372] | Test Loss: [2.8432114124298096, 0.2856140434741974, 2.5575973987579346]\n",
            "431: Train Loss: [1.095047950744629, 0.2316562533378601, 0.8633917570114136] | Test Loss: [2.8417611122131348, 0.2898770272731781, 2.551884174346924]\n",
            "432: Train Loss: [1.6954576969146729, 0.2646831274032593, 1.4307745695114136] | Test Loss: [2.841848373413086, 0.2938727140426636, 2.547975540161133]\n",
            "433: Train Loss: [1.9499874114990234, 0.27436506748199463, 1.6756223440170288] | Test Loss: [2.845538377761841, 0.2971305251121521, 2.548407793045044]\n",
            "434: Train Loss: [1.9161100387573242, 0.24551483988761902, 1.6705952882766724] | Test Loss: [2.8481123447418213, 0.2969682812690735, 2.5511441230773926]\n",
            "435: Train Loss: [1.953078031539917, 0.25356778502464294, 1.6995102167129517] | Test Loss: [2.853105068206787, 0.2997743487358093, 2.553330659866333]\n",
            "436: Train Loss: [1.692152976989746, 0.24332112073898315, 1.4488317966461182] | Test Loss: [2.8575775623321533, 0.29834723472595215, 2.559230327606201]\n",
            "437: Train Loss: [1.7647125720977783, 0.23028038442134857, 1.5344321727752686] | Test Loss: [2.8632946014404297, 0.2948044538497925, 2.5684902667999268]\n",
            "438: Train Loss: [2.0684752464294434, 0.2898860573768616, 1.7785892486572266] | Test Loss: [2.8613595962524414, 0.2895641624927521, 2.5717954635620117]\n",
            "439: Train Loss: [1.846968173980713, 0.28382712602615356, 1.5631409883499146] | Test Loss: [2.862611770629883, 0.28936567902565, 2.5732460021972656]\n",
            "440: Train Loss: [2.0534567832946777, 0.2665119767189026, 1.78694486618042] | Test Loss: [2.8554863929748535, 0.28469595313072205, 2.5707905292510986]\n",
            "441: Train Loss: [1.8099985122680664, 0.2882761061191559, 1.521722435951233] | Test Loss: [2.8498528003692627, 0.2824661135673523, 2.5673866271972656]\n",
            "442: Train Loss: [1.942311406135559, 0.23147226870059967, 1.7108391523361206] | Test Loss: [2.8464035987854004, 0.28204894065856934, 2.564354658126831]\n",
            "443: Train Loss: [1.9640334844589233, 0.31951430439949036, 1.6445192098617554] | Test Loss: [2.853562355041504, 0.2873693108558655, 2.566193103790283]\n",
            "444: Train Loss: [2.100010395050049, 0.2904092073440552, 1.8096011877059937] | Test Loss: [2.8628711700439453, 0.2899596691131592, 2.572911500930786]\n",
            "445: Train Loss: [1.9422295093536377, 0.2912588119506836, 1.650970697402954] | Test Loss: [2.87127947807312, 0.29117196798324585, 2.5801074504852295]\n",
            "446: Train Loss: [1.9091025590896606, 0.3176170587539673, 1.5914855003356934] | Test Loss: [2.8891654014587402, 0.30701902508735657, 2.582146406173706]\n",
            "447: Train Loss: [1.7186119556427002, 0.25918200612068176, 1.4594299793243408] | Test Loss: [2.898425579071045, 0.31765779852867126, 2.580767869949341]\n",
            "448: Train Loss: [1.9247664213180542, 0.2581024169921875, 1.6666640043258667] | Test Loss: [2.9020779132843018, 0.31868329644203186, 2.5833945274353027]\n",
            "449: Train Loss: [2.030686616897583, 0.318737268447876, 1.711949348449707] | Test Loss: [2.9083251953125, 0.3220677077770233, 2.5862574577331543]\n",
            "450: Train Loss: [1.9265373945236206, 0.30426037311553955, 1.622277021408081] | Test Loss: [2.9086499214172363, 0.32152849435806274, 2.5871214866638184]\n",
            "451: Train Loss: [1.6346954107284546, 0.271207720041275, 1.363487720489502] | Test Loss: [2.8963844776153564, 0.3177815079689026, 2.5786030292510986]\n",
            "452: Train Loss: [2.109231472015381, 0.26004552841186523, 1.849185824394226] | Test Loss: [2.8750874996185303, 0.3073914349079132, 2.5676960945129395]\n",
            "453: Train Loss: [2.149228572845459, 0.2627337574958801, 1.8864948749542236] | Test Loss: [2.8569467067718506, 0.2936977744102478, 2.563248872756958]\n",
            "454: Train Loss: [2.0003764629364014, 0.24227598309516907, 1.7581005096435547] | Test Loss: [2.8370726108551025, 0.2804449498653412, 2.5566277503967285]\n",
            "455: Train Loss: [1.593808650970459, 0.24926862120628357, 1.344539999961853] | Test Loss: [2.8227133750915527, 0.27186691761016846, 2.5508463382720947]\n",
            "456: Train Loss: [1.8362987041473389, 0.2780160903930664, 1.5582826137542725] | Test Loss: [2.8161721229553223, 0.26704832911491394, 2.549123764038086]\n",
            "457: Train Loss: [1.8872220516204834, 0.3027079701423645, 1.5845140218734741] | Test Loss: [2.8172526359558105, 0.268525630235672, 2.548727035522461]\n",
            "458: Train Loss: [2.2325706481933594, 0.3211580216884613, 1.9114127159118652] | Test Loss: [2.8252439498901367, 0.27585265040397644, 2.549391269683838]\n",
            "459: Train Loss: [1.8992383480072021, 0.26351574063301086, 1.6357226371765137] | Test Loss: [2.8314642906188965, 0.2832988500595093, 2.5481655597686768]\n",
            "460: Train Loss: [2.0944619178771973, 0.24297237396240234, 1.8514896631240845] | Test Loss: [2.831996440887451, 0.2876049280166626, 2.544391393661499]\n",
            "461: Train Loss: [1.8140509128570557, 0.29725173115730286, 1.5167992115020752] | Test Loss: [2.8337883949279785, 0.2940983176231384, 2.5396900177001953]\n",
            "462: Train Loss: [2.284327983856201, 0.2935234606266022, 1.9908045530319214] | Test Loss: [2.836669445037842, 0.3032167851924896, 2.5334527492523193]\n",
            "463: Train Loss: [2.1457765102386475, 0.2747766375541687, 1.8709999322891235] | Test Loss: [2.8398873805999756, 0.31343644857406616, 2.5264508724212646]\n",
            "464: Train Loss: [1.8253240585327148, 0.2761382460594177, 1.549185872077942] | Test Loss: [2.84722900390625, 0.3270057141780853, 2.520223379135132]\n",
            "465: Train Loss: [1.620361089706421, 0.33211633563041687, 1.2882447242736816] | Test Loss: [2.8573813438415527, 0.33921781182289124, 2.5181634426116943]\n",
            "466: Train Loss: [1.982452392578125, 0.25373375415802, 1.728718638420105] | Test Loss: [2.873488664627075, 0.3492237627506256, 2.5242648124694824]\n",
            "467: Train Loss: [1.6109812259674072, 0.2740713059902191, 1.3369098901748657] | Test Loss: [2.886115312576294, 0.352907657623291, 2.533207654953003]\n",
            "468: Train Loss: [1.8451604843139648, 0.2687761187553406, 1.576384425163269] | Test Loss: [2.8868017196655273, 0.3487182855606079, 2.53808331489563]\n",
            "469: Train Loss: [2.057262659072876, 0.22678373754024506, 1.8304790258407593] | Test Loss: [2.8742027282714844, 0.33342763781547546, 2.5407750606536865]\n",
            "470: Train Loss: [2.028749704360962, 0.2658042907714844, 1.7629454135894775] | Test Loss: [2.858712673187256, 0.3171165883541107, 2.5415961742401123]\n",
            "471: Train Loss: [1.9331507682800293, 0.26196059584617615, 1.6711901426315308] | Test Loss: [2.844625473022461, 0.30243292450904846, 2.5421924591064453]\n",
            "472: Train Loss: [1.6885788440704346, 0.29793646931648254, 1.3906424045562744] | Test Loss: [2.8362326622009277, 0.29302290081977844, 2.5432097911834717]\n",
            "473: Train Loss: [2.0727367401123047, 0.3035247325897217, 1.769212007522583] | Test Loss: [2.826037645339966, 0.2882871925830841, 2.537750482559204]\n",
            "474: Train Loss: [1.8631724119186401, 0.2879249155521393, 1.5752475261688232] | Test Loss: [2.8129453659057617, 0.2860044240951538, 2.5269408226013184]\n",
            "475: Train Loss: [2.023423910140991, 0.24964402616024017, 1.7737799882888794] | Test Loss: [2.811906099319458, 0.29116517305374146, 2.5207409858703613]\n",
            "476: Train Loss: [2.001126766204834, 0.27843591570854187, 1.7226909399032593] | Test Loss: [2.817688465118408, 0.29784512519836426, 2.519843339920044]\n",
            "477: Train Loss: [1.7758086919784546, 0.30465495586395264, 1.471153736114502] | Test Loss: [2.8355095386505127, 0.30761417746543884, 2.527895450592041]\n",
            "478: Train Loss: [1.8973166942596436, 0.21504560112953186, 1.682271122932434] | Test Loss: [2.848529577255249, 0.3143329620361328, 2.534196615219116]\n",
            "479: Train Loss: [1.9704687595367432, 0.3539219796657562, 1.6165467500686646] | Test Loss: [2.8710923194885254, 0.33175283670425415, 2.539339542388916]\n",
            "480: Train Loss: [1.8627499341964722, 0.2845008373260498, 1.5782490968704224] | Test Loss: [2.886810302734375, 0.34628427028656006, 2.5405261516571045]\n",
            "481: Train Loss: [1.843223214149475, 0.290048211812973, 1.5531748533248901] | Test Loss: [2.897644281387329, 0.35396745800971985, 2.5436768531799316]\n",
            "482: Train Loss: [1.914451003074646, 0.29916998744010925, 1.6152809858322144] | Test Loss: [2.9003114700317383, 0.34938377141952515, 2.5509276390075684]\n",
            "483: Train Loss: [1.462904453277588, 0.2549434006214142, 1.207961082458496] | Test Loss: [2.891477346420288, 0.3349273204803467, 2.5565500259399414]\n",
            "484: Train Loss: [2.0038013458251953, 0.2679433226585388, 1.7358579635620117] | Test Loss: [2.8823728561401367, 0.3210478723049164, 2.5613250732421875]\n",
            "485: Train Loss: [1.9900496006011963, 0.30327755212783813, 1.686772108078003] | Test Loss: [2.876321315765381, 0.3093305826187134, 2.566990852355957]\n",
            "486: Train Loss: [1.8177322149276733, 0.31429481506347656, 1.5034373998641968] | Test Loss: [2.8749067783355713, 0.3049486577510834, 2.569958209991455]\n",
            "487: Train Loss: [1.8241660594940186, 0.23433080315589905, 1.589835286140442] | Test Loss: [2.872337579727173, 0.3019436299800873, 2.5703940391540527]\n",
            "488: Train Loss: [2.0809574127197266, 0.3295729160308838, 1.7513843774795532] | Test Loss: [2.872227907180786, 0.3030119836330414, 2.569216012954712]\n",
            "489: Train Loss: [2.1715569496154785, 0.3639502227306366, 1.807606816291809] | Test Loss: [2.88360857963562, 0.315867155790329, 2.5677413940429688]\n",
            "490: Train Loss: [1.7146025896072388, 0.27067553997039795, 1.4439270496368408] | Test Loss: [2.8928232192993164, 0.33006250858306885, 2.562760829925537]\n",
            "491: Train Loss: [1.5875906944274902, 0.2534550726413727, 1.33413565158844] | Test Loss: [2.903815984725952, 0.3437369465827942, 2.5600790977478027]\n",
            "492: Train Loss: [1.7484322786331177, 0.2966206967830658, 1.4518115520477295] | Test Loss: [2.906477451324463, 0.35744035243988037, 2.549036979675293]\n",
            "493: Train Loss: [1.8688656091690063, 0.2938823401927948, 1.5749832391738892] | Test Loss: [2.911560535430908, 0.3690548837184906, 2.5425057411193848]\n",
            "494: Train Loss: [2.0104727745056152, 0.29495739936828613, 1.7155154943466187] | Test Loss: [2.922440767288208, 0.3799549639225006, 2.5424857139587402]\n",
            "495: Train Loss: [1.5436261892318726, 0.23333092033863068, 1.3102952241897583] | Test Loss: [2.9280662536621094, 0.3831459581851959, 2.5449202060699463]\n",
            "496: Train Loss: [2.215585708618164, 0.3355167806148529, 1.8800688982009888] | Test Loss: [2.933633804321289, 0.38477665185928345, 2.5488572120666504]\n",
            "497: Train Loss: [1.6083967685699463, 0.23394817113876343, 1.374448537826538] | Test Loss: [2.93430233001709, 0.378116250038147, 2.5561859607696533]\n",
            "498: Train Loss: [2.0359113216400146, 0.28123554587364197, 1.7546757459640503] | Test Loss: [2.9319911003112793, 0.3690798282623291, 2.56291127204895]\n",
            "499: Train Loss: [1.960754632949829, 0.255865216255188, 1.7048894166946411] | Test Loss: [2.917433738708496, 0.3538621664047241, 2.5635716915130615]\n",
            "500: Train Loss: [1.9600640535354614, 0.23405881226062775, 1.7260050773620605] | Test Loss: [2.902547597885132, 0.3364957273006439, 2.566051959991455]\n",
            "501: Train Loss: [1.5738950967788696, 0.27614447474479675, 1.2977505922317505] | Test Loss: [2.8896901607513428, 0.3214559257030487, 2.5682342052459717]\n",
            "502: Train Loss: [1.993591070175171, 0.2573607861995697, 1.7362302541732788] | Test Loss: [2.879394054412842, 0.3097012937068939, 2.569692850112915]\n",
            "503: Train Loss: [1.7163796424865723, 0.28124383091926575, 1.435135841369629] | Test Loss: [2.8715426921844482, 0.30256563425064087, 2.568977117538452]\n",
            "504: Train Loss: [2.105210781097412, 0.35218244791030884, 1.7530282735824585] | Test Loss: [2.867373466491699, 0.30895698070526123, 2.5584166049957275]\n",
            "505: Train Loss: [1.9948172569274902, 0.27576780319213867, 1.7190494537353516] | Test Loss: [2.8669755458831787, 0.31809553503990173, 2.548880100250244]\n",
            "506: Train Loss: [1.9722659587860107, 0.26179102063179016, 1.710474967956543] | Test Loss: [2.8718509674072266, 0.32457399368286133, 2.5472769737243652]\n",
            "507: Train Loss: [1.874194860458374, 0.30722635984420776, 1.566968560218811] | Test Loss: [2.8752148151397705, 0.332286536693573, 2.5429282188415527]\n",
            "508: Train Loss: [2.0707523822784424, 0.2390267699956894, 1.8317255973815918] | Test Loss: [2.8770978450775146, 0.3337148129940033, 2.5433831214904785]\n",
            "509: Train Loss: [1.7762418985366821, 0.25327375531196594, 1.5229681730270386] | Test Loss: [2.8773460388183594, 0.32836389541625977, 2.5489821434020996]\n",
            "510: Train Loss: [1.3301924467086792, 0.2448885440826416, 1.0853039026260376] | Test Loss: [2.867922782897949, 0.31663477420806885, 2.55128812789917]\n",
            "511: Train Loss: [1.9542769193649292, 0.26306042075157166, 1.6912165880203247] | Test Loss: [2.8584818840026855, 0.3007470667362213, 2.557734727859497]\n",
            "512: Train Loss: [1.6338307857513428, 0.28117039799690247, 1.3526604175567627] | Test Loss: [2.855618953704834, 0.2920227348804474, 2.563596248626709]\n",
            "513: Train Loss: [2.0040483474731445, 0.2603967487812042, 1.7436516284942627] | Test Loss: [2.8549203872680664, 0.28738507628440857, 2.567535400390625]\n",
            "514: Train Loss: [2.1875016689300537, 0.25446027517318726, 1.9330413341522217] | Test Loss: [2.859919786453247, 0.2827407717704773, 2.577178955078125]\n",
            "515: Train Loss: [1.7453397512435913, 0.2451692372560501, 1.5001704692840576] | Test Loss: [2.868023633956909, 0.282116562128067, 2.585906982421875]\n",
            "516: Train Loss: [1.962085247039795, 0.25822657346725464, 1.703858733177185] | Test Loss: [2.872020721435547, 0.27796828746795654, 2.594052314758301]\n",
            "517: Train Loss: [1.6547926664352417, 0.2972109019756317, 1.3575817346572876] | Test Loss: [2.8793301582336426, 0.27924662828445435, 2.600083589553833]\n",
            "518: Train Loss: [1.750436782836914, 0.22464007139205933, 1.52579665184021] | Test Loss: [2.8882322311401367, 0.281707227230072, 2.60652494430542]\n",
            "519: Train Loss: [1.7377479076385498, 0.2712267339229584, 1.466521143913269] | Test Loss: [2.8947253227233887, 0.28379833698272705, 2.610927104949951]\n",
            "520: Train Loss: [2.370964527130127, 0.3303733170032501, 2.040591239929199] | Test Loss: [2.903742551803589, 0.29171445965766907, 2.612028121948242]\n",
            "521: Train Loss: [1.7040400505065918, 0.38087183237075806, 1.3231682777404785] | Test Loss: [2.924372673034668, 0.3121418356895447, 2.6122307777404785]\n",
            "522: Train Loss: [1.7447984218597412, 0.26697593927383423, 1.4778224229812622] | Test Loss: [2.944662094116211, 0.3299536108970642, 2.614708423614502]\n",
            "523: Train Loss: [1.747860074043274, 0.24459208548069, 1.5032680034637451] | Test Loss: [2.9568755626678467, 0.3435359597206116, 2.61333966255188]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "TowH48RuBJql",
        "outputId": "1d10101a-aeb7-4722-fb51-1638c393c727"
      },
      "source": [
        "#### SHOW LOSSES ####\n",
        "\n",
        "plt.plot(np.array(training_loss_history)[:,0])\n",
        "plt.plot(np.array(test_loss_history)[:,0])\n",
        "plt.show()\n",
        "        \n",
        "pkl.dump([training_loss_history, test_loss_history], open(os.path.join(RUN_FOLDER, 'weights/histories.pkl'), 'wb'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RcYoqO7QBJql"
      },
      "source": [
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}